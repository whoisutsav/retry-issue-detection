org/apache/hadoop/hbase/CoordinatedStateManager.class;;;public interface org.apache.hadoop.hbase.CoordinatedStateManager {\n  public abstract org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination getSplitLogWorkerCoordination();\n  public abstract org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination getSplitLogManagerCoordination();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ExecutorStatusChore.class;;;public class org.apache.hadoop.hbase.ExecutorStatusChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public static final java.lang.String WAKE_FREQ;\n  public static final int DEFAULT_WAKE_FREQ;\n  public org.apache.hadoop.hbase.ExecutorStatusChore(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.hbase.executor.ExecutorService, org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource);\n  public org.apache.hadoop.hbase.util.Pair<java.lang.Long, java.lang.Long> getExecutorStatus(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/HBaseRpcServicesBase.class;;;public abstract class org.apache.hadoop.hbase.HBaseRpcServicesBase<S extends org.apache.hadoop.hbase.HBaseServerBase<?>> implements org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$ClientMetaService$BlockingInterface, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$AdminService$BlockingInterface, org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public static final java.lang.String CLIENT_BOOTSTRAP_NODE_LIMIT;\n  public static final int DEFAULT_CLIENT_BOOTSTRAP_NODE_LIMIT;\n  public org.apache.hadoop.hbase.security.access.AccessChecker getAccessChecker();\n  public org.apache.hadoop.hbase.security.access.ZKPermissionWatcher getZkPermissionWatcher();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public S getServer();\n  public java.net.InetSocketAddress getSocketAddress();\n  public org.apache.hadoop.hbase.ipc.RpcServerInterface getRpcServer();\n  public org.apache.hadoop.hbase.ipc.RpcScheduler getRpcScheduler();\n  public int getPriority(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.security.User);\n  public long getDeadline(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message);\n  public boolean checkOOME(java.lang.Throwable);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetClusterIdResponse getClusterId(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetClusterIdRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetActiveMasterResponse getActiveMaster(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetActiveMasterRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetMastersResponse getMasters(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetMastersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetMetaRegionLocationsResponse getMetaRegionLocations(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetMetaRegionLocationsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public final org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetBootstrapNodesResponse getBootstrapNodes(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetBootstrapNodesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateConfigurationResponse updateConfiguration(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateConfigurationRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearSlowLogResponses clearSlowLogsResponses(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearSlowLogResponseRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$LogEntry getLogEntries(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$LogRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/HBaseServerBase.class;;;public abstract class org.apache.hadoop.hbase.HBaseServerBase<R extends org.apache.hadoop.hbase.HBaseRpcServicesBase<?>> extends java.lang.Thread implements org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.conf.ConfigurationObserver, org.apache.hadoop.hbase.client.ConnectionRegistryEndpoint {\n  public org.apache.hadoop.hbase.HBaseServerBase(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public boolean isStopped();\n  public boolean isAborted();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.client.AsyncClusterConnection getAsyncClusterConnection();\n  public org.apache.hadoop.hbase.zookeeper.ZKWatcher getZooKeeper();\n  public boolean isShutdownHookInstalled();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public org.apache.hadoop.hbase.ChoreService getChoreService();\n  public org.apache.hadoop.hbase.TableDescriptors getTableDescriptors();\n  public org.apache.hadoop.hbase.executor.ExecutorService getExecutorService();\n  public org.apache.hadoop.hbase.security.access.AccessChecker getAccessChecker();\n  public org.apache.hadoop.hbase.security.access.ZKPermissionWatcher getZKPermissionWatcher();\n  public org.apache.hadoop.hbase.CoordinatedStateManager getCoordinatedStateManager();\n  public org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getDataRootDir();\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n  public org.apache.hadoop.fs.Path getWALRootDir();\n  public org.apache.hadoop.fs.FileSystem getWALFileSystem();\n  public boolean isClusterUp();\n  public long getStartcode();\n  public org.apache.hadoop.hbase.http.InfoServer getInfoServer();\n  public int getMsgInterval();\n  public org.apache.hadoop.hbase.namequeues.NamedQueueRecorder getNamedQueueRecorder();\n  public org.apache.hadoop.hbase.ipc.RpcServerInterface getRpcServer();\n  public org.apache.hadoop.hbase.util.NettyEventLoopGroupConfig getEventLoopGroupConfig();\n  public R getRpcServices();\n  public org.apache.hadoop.hbase.MetaRegionLocationCache getMetaRegionLocationCache();\n  public void updateConfiguration();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/HealthCheckChore.class;;;public class org.apache.hadoop.hbase.HealthCheckChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.HealthCheckChore(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/HealthChecker$1.class;;;class org.apache.hadoop.hbase.HealthChecker$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue as it does not contain any information or instructions about a task to be performed.;;;N
org/apache/hadoop/hbase/HealthChecker$HealthCheckerExitStatus.class;;;final class org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus extends java.lang.Enum<org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus> {\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus SUCCESS;\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus TIMED_OUT;\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus FAILED_WITH_EXIT_CODE;\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus FAILED_WITH_EXCEPTION;\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus FAILED;\n  public static org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus[] values();\n  public static org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus valueOf(java.lang.String);\n}\n;;;No, this class definition is not a message definition that can be put on a message queue. It is an enum definition in Java, used to define a set of constant values.;;;N;;;No, it is not a task definition that might be put on a task queue, but rather a class definition for an enumeration.;;;N
org/apache/hadoop/hbase/HealthChecker.class;;;class org.apache.hadoop.hbase.HealthChecker {\n  public void init(java.lang.String, long);\n  public org.apache.hadoop.hbase.HealthReport checkHealth();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/HealthReport.class;;;class org.apache.hadoop.hbase.HealthReport {\n  public java.lang.String toString();\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/JMXListener.class;;;public class org.apache.hadoop.hbase.JMXListener implements org.apache.hadoop.hbase.coprocessor.MasterCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor {\n  public static final java.lang.String RMI_REGISTRY_PORT_CONF_KEY;\n  public static final java.lang.String RMI_CONNECTOR_PORT_CONF_KEY;\n  public static final int defMasterRMIRegistryPort;\n  public static final int defRegionserverRMIRegistryPort;\n  public org.apache.hadoop.hbase.JMXListener();\n  public static javax.management.remote.JMXServiceURL buildJMXServiceURL(int, int) throws java.io.IOException;\n  public void startConnectorServer(int, int) throws java.io.IOException;\n  public void stopConnectorServer() throws java.io.IOException;\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n}\n;;;No, the class does not define any message or message format. It defines a listener and related methods for managing JMX connections in Apache HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/LocalHBaseCluster$1.class;;;class org.apache.hadoop.hbase.LocalHBaseCluster$1 implements java.security.PrivilegedExceptionAction<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread> {\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/LocalHBaseCluster$2.class;;;class org.apache.hadoop.hbase.LocalHBaseCluster$2 implements java.security.PrivilegedExceptionAction<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread> {\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/LocalHBaseCluster.class;;;public class org.apache.hadoop.hbase.LocalHBaseCluster {\n  public static final java.lang.String LOCAL;\n  public static final java.lang.String LOCAL_COLON;\n  public static final java.lang.String ASSIGN_RANDOM_PORTS;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int) throws java.io.IOException;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int, java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int, int, java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread addRegionServer() throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread addRegionServer(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread addRegionServer(org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread addMaster() throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread addMaster(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread addMaster(org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException;\n  public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer(int);\n  public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread> getRegionServers();\n  public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread> getLiveRegionServers();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public java.lang.String waitOnRegionServer(int);\n  public java.lang.String waitOnRegionServer(org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread);\n  public org.apache.hadoop.hbase.master.HMaster getMaster(int);\n  public org.apache.hadoop.hbase.master.HMaster getActiveMaster();\n  public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread> getMasters();\n  public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread> getLiveMasters();\n  public java.lang.String waitOnMaster(int);\n  public java.lang.String waitOnMaster(org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread);\n  public void join();\n  public void startup() throws java.io.IOException;\n  public void shutdown();\n  public static boolean isLocal(org.apache.hadoop.conf.Configuration);\n  public static void main(java.lang.String[]) throws java.io.IOException;\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a class that defines methods and variables related to HBase cluster management.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a local HBase cluster.;;;N
org/apache/hadoop/hbase/MetaRegionLocationCache$ZNodeOpType.class;;;final class org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType extends java.lang.Enum<org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType> {\n  public static final org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType INIT;\n  public static final org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType CREATED;\n  public static final org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType CHANGED;\n  public static final org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType DELETED;\n  public static org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType[] values();\n  public static org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType valueOf(java.lang.String);\n}\n;;;No. The given class is an enumeration that represents different types of operations on a znode. It is not a message definition that contains any message payload or metadata that can be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It defines a set of constant values and methods for operating on those values.;;;N
org/apache/hadoop/hbase/MetaRegionLocationCache.class;;;public class org.apache.hadoop.hbase.MetaRegionLocationCache extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public org.apache.hadoop.hbase.MetaRegionLocationCache(org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n  public java.util.List<org.apache.hadoop.hbase.HRegionLocation> getMetaRegionLocations();\n  public void nodeCreated(java.lang.String);\n  public void nodeDeleted(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/MetaTableAccessor.class;;;public final class org.apache.hadoop.hbase.MetaTableAccessor {\n  public static void fullScanRegions(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.Result> fullScanRegions(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public static void fullScanTables(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Table getMetaHTable(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName> getRegion(org.apache.hadoop.hbase.client.Connection, byte[]) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.HRegionLocation getRegionLocation(org.apache.hadoop.hbase.client.Connection, byte[]) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.HRegionLocation getRegionLocation(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Result getCatalogFamilyRow(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Result getRegionResult(org.apache.hadoop.hbase.client.Connection, byte[]) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Result scanByRegionEncodedName(org.apache.hadoop.hbase.client.Connection, java.lang.String) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getAllRegions(org.apache.hadoop.hbase.client.Connection, boolean) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getTableRegions(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getTableRegions(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Scan getScanForTableName(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName);\n  public static java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName>> getTableRegionsAndLocations(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName>> getTableRegionsAndLocations(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException;\n  public static void fullScanMetaAndPrint(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public static void scanMetaForTableRegions(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static void scanMeta(org.apache.hadoop.hbase.client.Connection, byte[], byte[], org.apache.hadoop.hbase.ClientMetaTableAccessor$QueryType, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static void scanMeta(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor, org.apache.hadoop.hbase.TableName, byte[], int) throws java.io.IOException;\n  public static void scanMeta(org.apache.hadoop.hbase.client.Connection, byte[], byte[], org.apache.hadoop.hbase.ClientMetaTableAccessor$QueryType, int, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static void scanMeta(org.apache.hadoop.hbase.client.Connection, byte[], byte[], org.apache.hadoop.hbase.ClientMetaTableAccessor$QueryType, org.apache.hadoop.hbase.filter.Filter, int, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.ServerName getTargetServerName(org.apache.hadoop.hbase.client.Result, int);\n  public static org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.client.RegionInfo> getDaughterRegions(org.apache.hadoop.hbase.client.Result);\n  public static org.apache.hadoop.hbase.client.TableState getTableState(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableState> getTableStates(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public static void updateTableState(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableState$State) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put makePutFromRegionInfo(org.apache.hadoop.hbase.client.RegionInfo, long) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Delete makeDeleteFromRegionInfo(org.apache.hadoop.hbase.client.RegionInfo, long);\n  public static org.apache.hadoop.hbase.client.Put addDaughtersToPut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static void putsToMetaTable(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Put>) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put addRegionStateToPut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.master.RegionState$State) throws java.io.IOException;\n  public static void updateRegionState(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.master.RegionState$State) throws java.io.IOException;\n  public static void addSplitsToParent(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static void addRegionsToMeta(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, int) throws java.io.IOException;\n  public static void addRegionsToMeta(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, int, long) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put makePutFromTableState(org.apache.hadoop.hbase.client.TableState, long);\n  public static void deleteTableState(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static void updateRegionLocation(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, long, long) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put addRegionInfo(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put addLocation(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.ServerName, long, int) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put addEmptyLocation(org.apache.hadoop.hbase.client.Put, int) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/RegionStateListener.class;;;public interface org.apache.hadoop.hbase.RegionStateListener {\n  public abstract void onRegionSplit(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public abstract void onRegionSplitReverted(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public abstract void onRegionMerged(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No. This is an interface definition that defines several methods for a RegionStateListener, but it is not a message definition.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/Server.class;;;public interface org.apache.hadoop.hbase.Server extends org.apache.hadoop.hbase.Abortable,org.apache.hadoop.hbase.Stoppable {\n  public abstract org.apache.hadoop.conf.Configuration getConfiguration();\n  public abstract org.apache.hadoop.hbase.zookeeper.ZKWatcher getZooKeeper();\n  public default org.apache.hadoop.hbase.client.Connection getConnection();\n  public abstract org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.AsyncConnection getAsyncConnection();\n  public abstract org.apache.hadoop.hbase.client.AsyncClusterConnection getAsyncClusterConnection();\n  public abstract org.apache.hadoop.hbase.ServerName getServerName();\n  public abstract org.apache.hadoop.hbase.CoordinatedStateManager getCoordinatedStateManager();\n  public abstract org.apache.hadoop.hbase.ChoreService getChoreService();\n  public default org.apache.hadoop.fs.FileSystem getFileSystem();\n  public default boolean isStopping();\n}\n;;;No. This is an interface for defining methods and their return types. It is not a specific message that would be put on a message queue.;;;N;;;No, it is not a task definition. It is an interface defining methods and properties of a server in the Hadoop HBase framework.;;;N
org/apache/hadoop/hbase/SplitLogCounters.class;;;public class org.apache.hadoop.hbase.SplitLogCounters {\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_batch_start;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_batch_success;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_batch_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_success;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_create_queued;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_create_result;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_already_exists;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_create_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_create_retry;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_queued;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_result;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_nonode;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_retry;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_delete_queued;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_delete_result;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_delete_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_failed;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_null_data;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_orphan_task_acquired;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_wait_for_zk_delete;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_unacquired_orphan_done;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_threshold_reached;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_missing_state_in_delete;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_heartbeat;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_rescan;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_rescan_deleted;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_task_deleted;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_unassigned;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_dead_server_task;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_force;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_failed_to_grab_task_no_data;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_failed_to_grab_task_exception;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_failed_to_grab_task_owned;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_failed_to_grab_task_lost_race;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_acquired;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_resigned;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_done;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_heartbeat;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_acquired_rescan;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_get_data_queued;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_get_data_result;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_get_data_retry;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_preempt_task;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_heartbeat_failed;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_final_transition_failed;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_grabing;\n  public static void resetCounters() throws java.lang.Exception;\n}\n;;;No. This is a class defining a set of static atomic counters used for monitoring and logging. It is not a message definition that is typically put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/SplitLogTask$Done.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Done extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Done(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask$Err.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Err extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Err(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask$Owned.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Owned extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Owned(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask$Resigned.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Resigned extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Resigned(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask$Unassigned.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Unassigned extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Unassigned(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask.class;;;public class org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public boolean isUnassigned(org.apache.hadoop.hbase.ServerName);\n  public boolean isUnassigned();\n  public boolean isOwned(org.apache.hadoop.hbase.ServerName);\n  public boolean isOwned();\n  public boolean isResigned(org.apache.hadoop.hbase.ServerName);\n  public boolean isResigned();\n  public boolean isDone(org.apache.hadoop.hbase.ServerName);\n  public boolean isDone();\n  public boolean isErr(org.apache.hadoop.hbase.ServerName);\n  public boolean isErr();\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public static org.apache.hadoop.hbase.SplitLogTask parseFrom(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public byte[] toByteArray();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SslRMIClientSocketFactorySecure.class;;;public class org.apache.hadoop.hbase.SslRMIClientSocketFactorySecure extends javax.rmi.ssl.SslRMIClientSocketFactory {\n  public org.apache.hadoop.hbase.SslRMIClientSocketFactorySecure();\n  public java.net.Socket createSocket(java.lang.String, int) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SslRMIServerSocketFactorySecure$1.class;;;class org.apache.hadoop.hbase.SslRMIServerSocketFactorySecure$1 extends java.net.ServerSocket {\n  public java.net.Socket accept() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/SslRMIServerSocketFactorySecure.class;;;public class org.apache.hadoop.hbase.SslRMIServerSocketFactorySecure extends javax.rmi.ssl.SslRMIServerSocketFactory {\n  public org.apache.hadoop.hbase.SslRMIServerSocketFactorySecure();\n  public java.net.ServerSocket createServerSocket(int) throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/TableDescriptors.class;;;public interface org.apache.hadoop.hbase.TableDescriptors extends java.io.Closeable {\n  public default boolean exists(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void close() throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.TableDescriptor get(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public abstract java.util.Map<java.lang.String, org.apache.hadoop.hbase.client.TableDescriptor> getByNamespace(java.lang.String) throws java.io.IOException;\n  public abstract java.util.Map<java.lang.String, org.apache.hadoop.hbase.client.TableDescriptor> getAll() throws java.io.IOException;\n  public default void update(org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public abstract void update(org.apache.hadoop.hbase.client.TableDescriptor, boolean) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.TableDescriptor remove(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n}\n;;;No. This is an interface that defines methods for working with table descriptors in Hadoop HBase. It is not a message definition that can be put on a message queue.;;;N;;;No, it is not a task definition. It is an interface defining methods related to working with table descriptors in Apache HBase.;;;N
org/apache/hadoop/hbase/YouAreDeadException.class;;;public class org.apache.hadoop.hbase.YouAreDeadException extends java.io.IOException {\n  public org.apache.hadoop.hbase.YouAreDeadException(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/ZNodeClearer$1.class;;;final class org.apache.hadoop.hbase.ZNodeClearer$1 implements org.apache.hadoop.hbase.Abortable {\n  public void abort(java.lang.String, java.lang.Throwable);\n  public boolean isAborted();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/ZNodeClearer.class;;;public final class org.apache.hadoop.hbase.ZNodeClearer {\n  public static void writeMyEphemeralNodeOnDisk(java.lang.String);\n  public static java.lang.String readMyEphemeralNodeOnDisk() throws java.io.IOException;\n  public static java.lang.String getMyEphemeralNodeFileName();\n  public static void deleteMyEphemeralNodeOnDisk();\n  public static java.lang.String parseMasterServerName(java.lang.String);\n  public static boolean clear(org.apache.hadoop.conf.Configuration);\n}\n;;;No, this class does not define any message or message format. It contains static methods for working with ephemeral nodes, parsing server names, and clearing configurations in Hadoop.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/FailedArchiveException.class;;;public class org.apache.hadoop.hbase.backup.FailedArchiveException extends java.io.IOException {\n  public org.apache.hadoop.hbase.backup.FailedArchiveException(java.lang.String, java.util.Collection<org.apache.hadoop.fs.Path>);\n  public java.util.Collection<org.apache.hadoop.fs.Path> getFailedFiles();\n  public java.lang.String getMessage();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$1.class;;;final class org.apache.hadoop.hbase.backup.HFileArchiver$1 implements java.util.function.Function<org.apache.hadoop.hbase.backup.HFileArchiver$File, org.apache.hadoop.fs.Path> {\n  public org.apache.hadoop.fs.Path apply(org.apache.hadoop.hbase.backup.HFileArchiver$File);\n  public java.lang.Object apply(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$2.class;;;final class org.apache.hadoop.hbase.backup.HFileArchiver$2 implements org.apache.hadoop.fs.PathFilter {\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class definition for a PathFilter interface implementation in the Apache HBase backup module.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$3.class;;;final class org.apache.hadoop.hbase.backup.HFileArchiver$3 implements java.util.concurrent.ThreadFactory {\n  public java.lang.Thread newThread(java.lang.Runnable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$File.class;;;abstract class org.apache.hadoop.hbase.backup.HFileArchiver$File {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$File(org.apache.hadoop.fs.FileSystem);\n  public boolean moveAndClose(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$FileConverter.class;;;abstract class org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter<T> implements java.util.function.Function<T, org.apache.hadoop.hbase.backup.HFileArchiver$File> {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$FileStatusConverter.class;;;class org.apache.hadoop.hbase.backup.HFileArchiver$FileStatusConverter extends org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter<org.apache.hadoop.fs.FileStatus> {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$FileStatusConverter(org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.hbase.backup.HFileArchiver$File apply(org.apache.hadoop.fs.FileStatus);\n  public java.lang.Object apply(java.lang.Object);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition in Java programming language for file conversion, not a task that can be executed in a distributed computing environment.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$FileablePath.class;;;class org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath extends org.apache.hadoop.hbase.backup.HFileArchiver$File {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public void delete() throws java.io.IOException;\n  public java.lang.String getName();\n  public java.util.Collection<org.apache.hadoop.hbase.backup.HFileArchiver$File> getChildren() throws java.io.IOException;\n  public boolean isFile() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n;;;No. This is a class definition, but not a message definition. It defines a type of object that can be used in a program, but it does not define a message that can be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$FileableStoreFile.class;;;class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile extends org.apache.hadoop.hbase.backup.HFileArchiver$File {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public void delete() throws java.io.IOException;\n  public java.lang.String getName();\n  public boolean isFile();\n  public java.util.Collection<org.apache.hadoop.hbase.backup.HFileArchiver$File> getChildren() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n;;;No. This class provides methods to interact with a file system and is not a message definition.;;;N;;;No, it is not a task definition, but a class representing a file in the HBase backup system.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$StoreToFile.class;;;class org.apache.hadoop.hbase.backup.HFileArchiver$StoreToFile extends org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter<org.apache.hadoop.hbase.regionserver.HStoreFile> {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$StoreToFile(org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.hbase.backup.HFileArchiver$File apply(org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public java.lang.Object apply(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver.class;;;public class org.apache.hadoop.hbase.backup.HFileArchiver {\n  public static boolean exists(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static void archiveRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static boolean archiveRegion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void archiveRegions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n  public static void archiveFamily(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, byte[]) throws java.io.IOException;\n  public static void archiveFamilyByFamilyDir(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, byte[]) throws java.io.IOException;\n  public static void archiveStoreFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public static void archiveRecoveredEdits(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo, byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public static void archiveStoreFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, byte[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a utility class with static methods for archiving HBase data files.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/example/HFileArchiveManager.class;;;class org.apache.hadoop.hbase.backup.example.HFileArchiveManager {\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveManager(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.ZooKeeperConnectionException, java.io.IOException;\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveManager enableHFileBackup(byte[]) throws org.apache.zookeeper.KeeperException;\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveManager disableHFileBackup(byte[]) throws org.apache.zookeeper.KeeperException;\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveManager disableHFileBackup() throws java.io.IOException;\n  public void stop();\n  public boolean isArchivingEnabled(byte[]) throws org.apache.zookeeper.KeeperException;\n}\n;;;No, this class does not define any message or data structure that could be put on a message queue. It defines methods for managing HFile backups in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/example/HFileArchiveTableMonitor.class;;;public class org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor {\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor();\n  public synchronized void setArchiveTables(java.util.List<java.lang.String>);\n  public synchronized void addTable(java.lang.String);\n  public synchronized void removeTable(java.lang.String);\n  public synchronized void clearArchive();\n  public synchronized boolean shouldArchiveTable(java.lang.String);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.class;;;public class org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate {\n  public org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner();\n  public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void stop(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.class;;;public final class org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public static final java.lang.String HFILE_ARCHIVE_ZNODE_PARENT;\n  public void start() throws org.apache.zookeeper.KeeperException;\n  public void nodeCreated(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n  public void nodeDeleted(java.lang.String);\n  public boolean keepHFiles(java.lang.String);\n  public final org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor getMonitor();\n  public static org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker create(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.ZooKeeperConnectionException, java.io.IOException;\n  public org.apache.hadoop.hbase.zookeeper.ZKWatcher getZooKeeperWatcher();\n  public void stop();\n}\n;;;No, it is a class definition for a tool used in Hadoop Distributed File System, not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/example/ZKTableArchiveClient.class;;;public class org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient extends org.apache.hadoop.conf.Configured {\n  public org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.Connection);\n  public void enableHFileBackupAsync(byte[]) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public void disableHFileBackup(java.lang.String) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public void disableHFileBackup(byte[]) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public void disableHFileBackup() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public boolean getArchivingEnabled(byte[]) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public boolean getArchivingEnabled(java.lang.String) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public static java.lang.String getArchiveZNode(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n}\n;;;No. It is a class implementation for a HBase backup client and does not define any message format for message passing or queuing.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/AsyncClusterConnection.class;;;public interface org.apache.hadoop.hbase.client.AsyncClusterConnection extends org.apache.hadoop.hbase.client.AsyncConnection {\n  public abstract org.apache.hadoop.hbase.client.AsyncRegionServerAdmin getRegionServerAdmin(org.apache.hadoop.hbase.ServerName);\n  public abstract org.apache.hadoop.hbase.client.NonceGenerator getNonceGenerator();\n  public abstract org.apache.hadoop.hbase.ipc.RpcClient getRpcClient();\n  public abstract java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionResponse> flush(byte[], boolean);\n  public abstract java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.RegionLocations> getRegionLocations(org.apache.hadoop.hbase.TableName, byte[], boolean);\n  public abstract java.util.concurrent.CompletableFuture<java.lang.String> prepareBulkLoad(org.apache.hadoop.hbase.TableName);\n  public abstract java.util.concurrent.CompletableFuture<java.lang.Boolean> bulkLoad(org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, byte[], boolean, org.apache.hadoop.security.token.Token<?>, java.lang.String, boolean, java.util.List<java.lang.String>, boolean);\n  public abstract java.util.concurrent.CompletableFuture<java.lang.Void> cleanupBulkLoad(org.apache.hadoop.hbase.TableName, java.lang.String);\n  public abstract java.util.concurrent.CompletableFuture<java.util.List<org.apache.hadoop.hbase.ServerName>> getLiveRegionServers(org.apache.hadoop.hbase.zookeeper.MasterAddressTracker, int);\n  public abstract java.util.concurrent.CompletableFuture<java.util.List<org.apache.hadoop.hbase.ServerName>> getAllBootstrapNodes(org.apache.hadoop.hbase.ServerName);\n  public abstract java.util.concurrent.CompletableFuture<java.lang.Void> replicate(org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>, int, long, long);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/AsyncClusterConnectionImpl.class;;;class org.apache.hadoop.hbase.client.AsyncClusterConnectionImpl extends org.apache.hadoop.hbase.client.AsyncConnectionImpl implements org.apache.hadoop.hbase.client.AsyncClusterConnection {\n  public org.apache.hadoop.hbase.client.AsyncClusterConnectionImpl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.ConnectionRegistry, java.lang.String, java.net.SocketAddress, org.apache.hadoop.hbase.security.User);\n  public org.apache.hadoop.hbase.client.NonceGenerator getNonceGenerator();\n  public org.apache.hadoop.hbase.ipc.RpcClient getRpcClient();\n  public org.apache.hadoop.hbase.client.AsyncRegionServerAdmin getRegionServerAdmin(org.apache.hadoop.hbase.ServerName);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionResponse> flush(byte[], boolean);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.RegionLocations> getRegionLocations(org.apache.hadoop.hbase.TableName, byte[], boolean);\n  public java.util.concurrent.CompletableFuture<java.lang.String> prepareBulkLoad(org.apache.hadoop.hbase.TableName);\n  public java.util.concurrent.CompletableFuture<java.lang.Boolean> bulkLoad(org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, byte[], boolean, org.apache.hadoop.security.token.Token<?>, java.lang.String, boolean, java.util.List<java.lang.String>, boolean);\n  public java.util.concurrent.CompletableFuture<java.lang.Void> cleanupBulkLoad(org.apache.hadoop.hbase.TableName, java.lang.String);\n  public java.util.concurrent.CompletableFuture<java.util.List<org.apache.hadoop.hbase.ServerName>> getLiveRegionServers(org.apache.hadoop.hbase.zookeeper.MasterAddressTracker, int);\n  public java.util.concurrent.CompletableFuture<java.util.List<org.apache.hadoop.hbase.ServerName>> getAllBootstrapNodes(org.apache.hadoop.hbase.ServerName);\n  public java.util.concurrent.CompletableFuture<java.lang.Void> replicate(org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>, int, long, long);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/AsyncRegionReplicationRetryingCaller.class;;;public class org.apache.hadoop.hbase.client.AsyncRegionReplicationRetryingCaller extends org.apache.hadoop.hbase.client.AsyncRpcRetryingCaller<java.lang.Void> {\n  public org.apache.hadoop.hbase.client.AsyncRegionReplicationRetryingCaller(org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer, org.apache.hadoop.hbase.client.AsyncClusterConnectionImpl, int, long, long, org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/AsyncRegionServerAdmin$1.class;;;class org.apache.hadoop.hbase.client.AsyncRegionServerAdmin$1 implements org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<RESP> {\n  public void run(RESP);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/AsyncRegionServerAdmin$RpcCall.class;;;interface org.apache.hadoop.hbase.client.AsyncRegionServerAdmin$RpcCall<RESP> {\n  public abstract void call(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$AdminService$Interface, org.apache.hadoop.hbase.ipc.HBaseRpcController, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<RESP>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/AsyncRegionServerAdmin.class;;;public class org.apache.hadoop.hbase.client.AsyncRegionServerAdmin {\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionInfoResponse> getRegionInfo(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionInfoRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetStoreFileResponse> getStoreFile(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetStoreFileRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetOnlineRegionResponse> getOnlineRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetOnlineRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$OpenRegionResponse> openRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$OpenRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WarmupRegionResponse> warmupRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WarmupRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CloseRegionResponse> closeRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CloseRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionResponse> flushRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactionSwitchResponse> compactionSwitch(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactionSwitchRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactRegionResponse> compactRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse> replicateWALEntry(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner, int);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse> replay(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$RollWALWriterResponse> rollWALWriter(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$RollWALWriterRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetServerInfoResponse> getServerInfo(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetServerInfoRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$StopServerResponse> stopServer(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$StopServerRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateFavoredNodesResponse> updateFavoredNodes(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateFavoredNodesRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateConfigurationResponse> updateConfiguration(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateConfigurationRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionLoadResponse> getRegionLoad(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionLoadRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearCompactionQueuesResponse> clearCompactionQueues(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearCompactionQueuesRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearRegionBlockCacheResponse> clearRegionBlockCache(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearRegionBlockCacheRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaSnapshotsResponse> getSpaceQuotaSnapshots(org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaSnapshotsRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ExecuteProceduresResponse> executeProcedures(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ExecuteProceduresRequest);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/ClientSideRegionScanner.class;;;public class org.apache.hadoop.hbase.client.ClientSideRegionScanner extends org.apache.hadoop.hbase.client.AbstractClientScanner {\n  public org.apache.hadoop.hbase.client.ClientSideRegionScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.client.metrics.ScanMetrics) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result next() throws java.io.IOException;\n  public void close();\n  public boolean renewLease();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/ClusterConnectionFactory.class;;;public final class org.apache.hadoop.hbase.client.ClusterConnectionFactory {\n  public static final java.lang.String HBASE_SERVER_CLUSTER_CONNECTION_IMPL;\n  public static org.apache.hadoop.hbase.client.AsyncClusterConnection createAsyncClusterConnection(org.apache.hadoop.conf.Configuration, java.net.SocketAddress, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.AsyncClusterConnection createAsyncClusterConnection(org.apache.hadoop.hbase.client.ConnectionRegistryEndpoint, org.apache.hadoop.conf.Configuration, java.net.SocketAddress, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, this is not a task definition that might be put on a task queue as it does not describe a specific unit of work or task to be executed. It is a class that defines methods for creating connections to a HBase cluster, which is not a task.;;;N
org/apache/hadoop/hbase/client/ConnectionRegistryEndpoint.class;;;public interface org.apache.hadoop.hbase.client.ConnectionRegistryEndpoint {\n  public abstract java.lang.String getClusterId();\n  public abstract java.util.Optional<org.apache.hadoop.hbase.ServerName> getActiveMaster();\n  public abstract java.util.List<org.apache.hadoop.hbase.ServerName> getBackupMasters();\n  public abstract java.util.Iterator<org.apache.hadoop.hbase.ServerName> getBootstrapNodes();\n  public abstract java.util.List<org.apache.hadoop.hbase.HRegionLocation> getMetaLocations();\n}\n;;;No. It is an interface that define methods for a connection registry endpoint, but it is not a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/SharedAsyncConnection.class;;;public class org.apache.hadoop.hbase.client.SharedAsyncConnection implements org.apache.hadoop.hbase.client.AsyncConnection {\n  public org.apache.hadoop.hbase.client.SharedAsyncConnection(org.apache.hadoop.hbase.client.AsyncConnection);\n  public boolean isClosed();\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.client.AsyncTableRegionLocator getRegionLocator(org.apache.hadoop.hbase.TableName);\n  public void clearRegionLocationCache();\n  public org.apache.hadoop.hbase.client.AsyncTableBuilder<org.apache.hadoop.hbase.client.AdvancedScanResultConsumer> getTableBuilder(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.client.AsyncTableBuilder<org.apache.hadoop.hbase.client.ScanResultConsumer> getTableBuilder(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService);\n  public org.apache.hadoop.hbase.client.AsyncAdminBuilder getAdminBuilder();\n  public org.apache.hadoop.hbase.client.AsyncAdminBuilder getAdminBuilder(java.util.concurrent.ExecutorService);\n  public org.apache.hadoop.hbase.client.AsyncBufferedMutatorBuilder getBufferedMutatorBuilder(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.client.AsyncBufferedMutatorBuilder getBufferedMutatorBuilder(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.client.Hbck> getHbck();\n  public org.apache.hadoop.hbase.client.Hbck getHbck(org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Connection toConnection();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It defines a class that implements the AsyncConnection interface and provides methods to interact with HBase.;;;N
org/apache/hadoop/hbase/client/SharedConnection.class;;;public class org.apache.hadoop.hbase.client.SharedConnection implements org.apache.hadoop.hbase.client.Connection {\n  public org.apache.hadoop.hbase.client.SharedConnection(org.apache.hadoop.hbase.client.Connection);\n  public void close() throws java.io.IOException;\n  public boolean isClosed();\n  public void abort(java.lang.String, java.lang.Throwable);\n  public boolean isAborted();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.client.BufferedMutator getBufferedMutator(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.BufferedMutator getBufferedMutator(org.apache.hadoop.hbase.client.BufferedMutatorParams) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.RegionLocator getRegionLocator(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Admin getAdmin() throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableBuilder getTableBuilder(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService);\n  public void clearRegionLocationCache();\n  public org.apache.hadoop.hbase.client.Hbck getHbck() throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Hbck getHbck(org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.AsyncConnection toAsyncConnection();\n  public java.lang.String getClusterId();\n}\n;;;No. This class is an implementation of the Connection interface in HBase and is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/ShortCircuitConnectionRegistry.class;;;class org.apache.hadoop.hbase.client.ShortCircuitConnectionRegistry implements org.apache.hadoop.hbase.client.ConnectionRegistry {\n  public org.apache.hadoop.hbase.client.ShortCircuitConnectionRegistry(org.apache.hadoop.hbase.client.ConnectionRegistryEndpoint);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.RegionLocations> getMetaRegionLocations();\n  public java.util.concurrent.CompletableFuture<java.lang.String> getClusterId();\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.ServerName> getActiveMaster();\n  public java.lang.String getConnectionString();\n  public void close();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/TableSnapshotScanner.class;;;public class org.apache.hadoop.hbase.client.TableSnapshotScanner extends org.apache.hadoop.hbase.client.AbstractClientScanner {\n  public org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result next() throws java.io.IOException;\n  public void close();\n  public boolean renewLease();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It represents a scanner for reading data from a HBase table snapshot and provides methods for manipulating the scanner and reading the results.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/VersionInfoUtil$ServiceCallFunction.class;;;public interface org.apache.hadoop.hbase.client.VersionInfoUtil$ServiceCallFunction<T1, T2, R, E extends java.lang.Throwable> {\n  public abstract R apply(T1, T2) throws E;\n}\n;;;No.;;;N;;;No. It is an interface defining a function with a generic return and two parameters. It is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/client/VersionInfoUtil.class;;;public final class org.apache.hadoop.hbase.client.VersionInfoUtil {\n  public static boolean currentClientHasMinimumVersion(int, int);\n  public static boolean hasMinimumVersion(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$VersionInfo, int, int);\n  public static <T1, T2, R, E extends java.lang.Throwable> R callWithVersion(org.apache.hadoop.hbase.client.VersionInfoUtil$ServiceCallFunction<T1, T2, R, E>, T1, T2) throws E;\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$VersionInfo getCurrentClientVersionInfo();\n  public static java.lang.String versionNumberToString(int);\n  public static int getVersionNumber(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$VersionInfo);\n}\n;;;No, this class does not define a message that can be put on a message queue. It provides utility methods related to version information in the HBase client.;;;N;;;No, it is not a task definition, but rather a utility class with static methods.;;;N
org/apache/hadoop/hbase/client/locking/EntityLock$LockHeartbeatWorker.class;;;public class org.apache.hadoop.hbase.client.locking.EntityLock$LockHeartbeatWorker extends java.lang.Thread {\n  public org.apache.hadoop.hbase.client.locking.EntityLock$LockHeartbeatWorker(org.apache.hadoop.hbase.client.locking.EntityLock, java.lang.String);\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/client/locking/EntityLock.class;;;public class org.apache.hadoop.hbase.client.locking.EntityLock {\n  public static final java.lang.String HEARTBEAT_TIME_BUFFER;\n  public java.lang.String toString();\n  public boolean isLocked();\n  public void requestLock() throws java.io.IOException;\n  public boolean await(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public void await() throws java.lang.InterruptedException;\n  public void unlock() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/locking/LockServiceClient.class;;;public class org.apache.hadoop.hbase.client.locking.LockServiceClient {\n  public org.apache.hadoop.hbase.client.locking.LockServiceClient(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockService$BlockingInterface, org.apache.hadoop.hbase.client.NonceGenerator);\n  public org.apache.hadoop.hbase.client.locking.EntityLock tableLock(org.apache.hadoop.hbase.TableName, boolean, java.lang.String, org.apache.hadoop.hbase.Abortable);\n  public org.apache.hadoop.hbase.client.locking.EntityLock namespaceLock(java.lang.String, java.lang.String, org.apache.hadoop.hbase.Abortable);\n  public org.apache.hadoop.hbase.client.locking.EntityLock regionLock(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, java.lang.String, org.apache.hadoop.hbase.Abortable);\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockRequest buildLockRequest(org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockType, java.lang.String, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, java.lang.String, long, long);\n}\n;;;No. While this is a class that provides methods for obtaining locks on HBase entities, it is not a message definition. A message definition would describe the structure of a message that is sent across a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/codec/MessageCodec$MessageDecoder.class;;;class org.apache.hadoop.hbase.codec.MessageCodec$MessageDecoder extends org.apache.hadoop.hbase.codec.BaseDecoder {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/codec/MessageCodec$MessageEncoder.class;;;class org.apache.hadoop.hbase.codec.MessageCodec$MessageEncoder extends org.apache.hadoop.hbase.codec.BaseEncoder {\n  public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/codec/MessageCodec.class;;;public class org.apache.hadoop.hbase.codec.MessageCodec implements org.apache.hadoop.hbase.codec.Codec {\n  public org.apache.hadoop.hbase.codec.MessageCodec();\n  public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(java.io.InputStream);\n  public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(org.apache.hadoop.hbase.nio.ByteBuff);\n  public org.apache.hadoop.hbase.codec.Codec$Encoder getEncoder(java.io.OutputStream);\n}\n;;;No. This class is a codec implementation for HBase that can be used to compress and decompress data, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/constraint/BaseConstraint.class;;;public abstract class org.apache.hadoop.hbase.constraint.BaseConstraint extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.hbase.constraint.Constraint {\n  public org.apache.hadoop.hbase.constraint.BaseConstraint();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/constraint/Constraint.class;;;public interface org.apache.hadoop.hbase.constraint.Constraint extends org.apache.hadoop.conf.Configurable {\n  public abstract void check(org.apache.hadoop.hbase.client.Put) throws org.apache.hadoop.hbase.constraint.ConstraintException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/constraint/ConstraintException.class;;;public class org.apache.hadoop.hbase.constraint.ConstraintException extends org.apache.hadoop.hbase.DoNotRetryIOException {\n  public org.apache.hadoop.hbase.constraint.ConstraintException();\n  public org.apache.hadoop.hbase.constraint.ConstraintException(java.lang.String);\n  public org.apache.hadoop.hbase.constraint.ConstraintException(java.lang.String, java.lang.Throwable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/constraint/ConstraintProcessor.class;;;public class org.apache.hadoop.hbase.constraint.ConstraintProcessor implements org.apache.hadoop.hbase.coprocessor.RegionCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionObserver {\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionObserver> getRegionObserver();\n  public org.apache.hadoop.hbase.constraint.ConstraintProcessor();\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment);\n  public void prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n}\n;;;No. It is a class that implements two interfaces and has some methods defined within it. However, it is not a message definition that would typically be put on a message queue.;;;N;;;No;;;N
org/apache/hadoop/hbase/constraint/Constraints$1.class;;;final class org.apache.hadoop.hbase.constraint.Constraints$1 implements java.util.Comparator<org.apache.hadoop.hbase.constraint.Constraint> {\n  public int compare(org.apache.hadoop.hbase.constraint.Constraint, org.apache.hadoop.hbase.constraint.Constraint);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/constraint/Constraints.class;;;public final class org.apache.hadoop.hbase.constraint.Constraints {\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder enable(org.apache.hadoop.hbase.client.TableDescriptorBuilder) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder disable(org.apache.hadoop.hbase.client.TableDescriptorBuilder) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder remove(org.apache.hadoop.hbase.client.TableDescriptorBuilder) throws java.io.IOException;\n  public static boolean has(org.apache.hadoop.hbase.client.TableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>);\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder add(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>...) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder add(org.apache.hadoop.hbase.client.TableDescriptorBuilder, org.apache.hadoop.hbase.util.Pair<java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration>...) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder add(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder setConfiguration(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.IllegalArgumentException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder remove(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>);\n  public static void enableConstraint(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>) throws java.io.IOException;\n  public static void disableConstraint(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>) throws java.io.IOException;\n  public static boolean enabled(org.apache.hadoop.hbase.client.TableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class containing static methods related to manipulating constraints in Apache HBase.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a set of static utility methods for working with table constraints in Apache HBase.;;;N
org/apache/hadoop/hbase/coordination/SplitLogManagerCoordination$SplitLogManagerDetails.class;;;public class org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails {\n  public org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails(java.util.concurrent.ConcurrentMap<java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task>, org.apache.hadoop.hbase.master.MasterServices, java.util.Set<java.lang.String>);\n  public org.apache.hadoop.hbase.master.MasterServices getMaster();\n  public java.util.concurrent.ConcurrentMap<java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task> getTasks();\n  public java.util.Set<java.lang.String> getFailedDeletions();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n}\n;;;No, this is not a message definition that might be put on a message queue. It appears to be a Java class definition that contains methods and constructors.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/SplitLogManagerCoordination.class;;;public interface org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination {\n  public abstract void setDetails(org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails);\n  public abstract org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails getDetails();\n  public abstract java.lang.String prepareTask(java.lang.String);\n  public abstract void checkTasks();\n  public abstract int remainingTasksInCoordination();\n  public abstract void checkTaskStillAvailable(java.lang.String);\n  public abstract boolean resubmitTask(java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task, org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective);\n  public abstract void submitTask(java.lang.String);\n  public abstract void deleteTask(java.lang.String);\n  public abstract void init() throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition, but an interface defining methods related to managing the coordination of a SplitLogManager.;;;N
org/apache/hadoop/hbase/coordination/SplitLogWorkerCoordination$SplitTaskDetails.class;;;public interface org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails {\n  public abstract java.lang.String getWALFile();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/SplitLogWorkerCoordination.class;;;public interface org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination {\n  public abstract void init(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor, org.apache.hadoop.hbase.regionserver.SplitLogWorker);\n  public abstract void stopProcessingTasks();\n  public abstract boolean isStop();\n  public abstract void taskLoop() throws java.lang.InterruptedException;\n  public abstract void markCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem);\n  public abstract boolean isReady() throws java.lang.InterruptedException;\n  public abstract int getTaskReadySeq();\n  public abstract void registerListener();\n  public abstract void removeListener();\n  public abstract void endTask(org.apache.hadoop.hbase.SplitLogTask, java.util.concurrent.atomic.LongAdder, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails);\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an interface that defines a set of methods for coordinating tasks related to Hadoop HBase region servers.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$1.class;;;class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$1 implements org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status finish(org.apache.hadoop.hbase.ServerName, java.lang.String);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$CreateAsyncCallback.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateAsyncCallback implements org.apache.zookeeper.AsyncCallback$StringCallback {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination);\n  public void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$CreateRescanAsyncCallback.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateRescanAsyncCallback implements org.apache.zookeeper.AsyncCallback$StringCallback {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateRescanAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination);\n  public void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$DeleteAsyncCallback.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$DeleteAsyncCallback implements org.apache.zookeeper.AsyncCallback$VoidCallback {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$DeleteAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination);\n  public void processResult(int, java.lang.String, java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$GetDataAsyncCallback.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$GetDataAsyncCallback implements org.apache.zookeeper.AsyncCallback$DataCallback {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$GetDataAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination);\n  public void processResult(int, java.lang.String, java.lang.Object, byte[], org.apache.zookeeper.data.Stat);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$TaskFinisher$Status.class;;;public final class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status extends java.lang.Enum<org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status> {\n  public static final org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status DONE;\n  public static final org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status ERR;\n  public static org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status[] values();\n  public static org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status valueOf(java.lang.String);\n}\n;;;No. This class is not a message definition, it is an enum definition. It defines two possible values for the type org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status: DONE and ERR. However, it lacks any message-related fields or methods.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$TaskFinisher.class;;;public interface org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher {\n  public abstract org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status finish(org.apache.hadoop.hbase.ServerName, java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination extends org.apache.hadoop.hbase.zookeeper.ZKListener implements org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination {\n  public static final int DEFAULT_TIMEOUT;\n  public static final int DEFAULT_ZK_RETRIES;\n  public static final int DEFAULT_MAX_RESUBMIT;\n  public boolean ignoreZKDeleteForTesting;\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n  public void init() throws java.io.IOException;\n  public java.lang.String prepareTask(java.lang.String);\n  public int remainingTasksInCoordination();\n  public void deleteTask(java.lang.String);\n  public boolean resubmitTask(java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task, org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective);\n  public void checkTasks();\n  public void submitTask(java.lang.String);\n  public void checkTaskStillAvailable(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public void setDetails(org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails);\n  public org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails getDetails();\n  public void setIgnoreDeleteForTesting(boolean);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/ZkCoordinatedStateManager.class;;;public class org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager implements org.apache.hadoop.hbase.CoordinatedStateManager {\n  public org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager(org.apache.hadoop.hbase.Server);\n  public org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination getSplitLogWorkerCoordination();\n  public org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination getSplitLogManagerCoordination();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination$1.class;;;class org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$1 implements org.apache.hadoop.hbase.util.CancelableProgressable {\n  public boolean progress();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination$GetDataAsyncCallback.class;;;class org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$GetDataAsyncCallback implements org.apache.zookeeper.AsyncCallback$DataCallback {\n  public void processResult(int, java.lang.String, java.lang.Object, byte[], org.apache.zookeeper.data.Stat);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination$ZkSplitTaskDetails.class;;;public class org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails implements org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails {\n  public org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails();\n  public org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails(java.lang.String, org.apache.commons.lang3.mutable.MutableInt);\n  public java.lang.String getTaskNode();\n  public void setTaskNode(java.lang.String);\n  public org.apache.commons.lang3.mutable.MutableInt getCurTaskZKVersion();\n  public void setCurTaskZKVersion(org.apache.commons.lang3.mutable.MutableInt);\n  public java.lang.String getWALFile();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.class;;;public class org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination extends org.apache.hadoop.hbase.zookeeper.ZKListener implements org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination {\n  public org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n  public void nodeChildrenChanged(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public void init(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor, org.apache.hadoop.hbase.regionserver.SplitLogWorker);\n  public void getDataSetWatchAsync();\n  public void taskLoop() throws java.lang.InterruptedException;\n  public void markCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem);\n  public boolean isReady() throws java.lang.InterruptedException;\n  public int getTaskReadySeq();\n  public void registerListener();\n  public void removeListener();\n  public void stopProcessingTasks();\n  public boolean isStop();\n  public void endTask(org.apache.hadoop.hbase.SplitLogTask, java.util.concurrent.atomic.LongAdder, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/BaseEnvironment.class;;;public class org.apache.hadoop.hbase.coprocessor.BaseEnvironment<C extends org.apache.hadoop.hbase.Coprocessor> implements org.apache.hadoop.hbase.CoprocessorEnvironment<C> {\n  public C impl;\n  public org.apache.hadoop.hbase.coprocessor.BaseEnvironment(C, int, int, org.apache.hadoop.conf.Configuration);\n  public void startup() throws java.io.IOException;\n  public void shutdown();\n  public C getInstance();\n  public java.lang.ClassLoader getClassLoader();\n  public int getPriority();\n  public int getLoadSequence();\n  public int getVersion();\n  public java.lang.String getHBaseVersion();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/BulkLoadObserver.class;;;public interface org.apache.hadoop.hbase.coprocessor.BulkLoadObserver {\n  public default void prePrepareBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preCleanupBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$1.class;;;final class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$1 implements java.util.Comparator<java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>> {\n  public int compare(java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>, java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$EnvironmentPriorityComparator.class;;;class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$EnvironmentPriorityComparator implements java.util.Comparator<org.apache.hadoop.hbase.CoprocessorEnvironment> {\n  public int compare(org.apache.hadoop.hbase.CoprocessorEnvironment, org.apache.hadoop.hbase.CoprocessorEnvironment);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$ObserverGetter.class;;;public interface org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O> extends java.util.function.Function<C, java.util.Optional<O>> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$ObserverOperation.class;;;abstract class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperation<O> extends org.apache.hadoop.hbase.coprocessor.ObserverContextImpl<E> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$ObserverOperationWithResult.class;;;public abstract class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithResult<O, R> extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<C, E>.ObserverOperation<O> {\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, R);\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, R, boolean);\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, R, org.apache.hadoop.hbase.security.User);\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$ObserverOperationWithoutResult.class;;;public abstract class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult<O> extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<C, E>.ObserverOperation<O> {\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>);\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, org.apache.hadoop.hbase.security.User);\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, org.apache.hadoop.hbase.security.User, boolean);\n}\n;;;No, it is a class definition for an abstract class that extends another class. It may be used in defining a message, but it is not a message definition itself.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost.class;;;public abstract class org.apache.hadoop.hbase.coprocessor.CoprocessorHost<C extends org.apache.hadoop.hbase.Coprocessor, E extends org.apache.hadoop.hbase.CoprocessorEnvironment<C>> {\n  public static final java.lang.String REGION_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String REGIONSERVER_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String USER_REGION_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String MASTER_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String WAL_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String ABORT_ON_ERROR_KEY;\n  public static final boolean DEFAULT_ABORT_ON_ERROR;\n  public static final java.lang.String COPROCESSORS_ENABLED_CONF_KEY;\n  public static final boolean DEFAULT_COPROCESSORS_ENABLED;\n  public static final java.lang.String USER_COPROCESSORS_ENABLED_CONF_KEY;\n  public static final boolean DEFAULT_USER_COPROCESSORS_ENABLED;\n  public static final java.lang.String SKIP_LOAD_DUPLICATE_TABLE_COPROCESSOR;\n  public static final boolean DEFAULT_SKIP_LOAD_DUPLICATE_TABLE_COPROCESSOR;\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost(org.apache.hadoop.hbase.Abortable);\n  public static java.util.Set<java.lang.String> getLoadedCoprocessors();\n  public java.util.Set<java.lang.String> getCoprocessors();\n  public E load(org.apache.hadoop.fs.Path, java.lang.String, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public E load(org.apache.hadoop.fs.Path, java.lang.String, int, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public void load(java.lang.Class<? extends C>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public E checkAndLoadInstance(java.lang.Class<?>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract E createEnvironment(C, int, int, org.apache.hadoop.conf.Configuration);\n  public abstract C checkAndGetInstance(java.lang.Class<?>) throws java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public void shutdown(E);\n  public C findCoprocessor(java.lang.String);\n  public <T extends C> T findCoprocessor(java.lang.Class<T>);\n  public <T extends C> java.util.List<T> findCoprocessors(java.lang.Class<T>);\n  public E findCoprocessorEnvironment(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorService.class;;;public interface org.apache.hadoop.hbase.coprocessor.CoprocessorService {\n  public abstract org.apache.hbase.thirdparty.com.google.protobuf.Service getService();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/CoreCoprocessor.class;;;public interface org.apache.hadoop.hbase.coprocessor.CoreCoprocessor extends java.lang.annotation.Annotation {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/EndpointObserver.class;;;public interface org.apache.hadoop.hbase.coprocessor.EndpointObserver {\n  public default org.apache.hbase.thirdparty.com.google.protobuf.Message preEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hbase.thirdparty.com.google.protobuf.Service, java.lang.String, org.apache.hbase.thirdparty.com.google.protobuf.Message) throws java.io.IOException;\n  public default void postEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hbase.thirdparty.com.google.protobuf.Service, java.lang.String, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hbase.thirdparty.com.google.protobuf.Message$Builder) throws java.io.IOException;\n}\n;;;No. This is an interface definition for a class that implements an observer for HBase coprocessor endpoints. It does not define the structure or content of a message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/HasMasterServices.class;;;public interface org.apache.hadoop.hbase.coprocessor.HasMasterServices {\n  public abstract org.apache.hadoop.hbase.master.MasterServices getMasterServices();\n}\n;;;no;;;N;;;No. This is an interface definition for a method to get the master services in HBase, and it is not a task definition that can be executed on a task queue.;;;N
org/apache/hadoop/hbase/coprocessor/HasRegionServerServices.class;;;public interface org.apache.hadoop.hbase.coprocessor.HasRegionServerServices {\n  public abstract org.apache.hadoop.hbase.regionserver.RegionServerServices getRegionServerServices();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/MasterCoprocessor.class;;;public interface org.apache.hadoop.hbase.coprocessor.MasterCoprocessor extends org.apache.hadoop.hbase.Coprocessor {\n  public default java.util.Optional<org.apache.hadoop.hbase.coprocessor.MasterObserver> getMasterObserver();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/MasterCoprocessorEnvironment.class;;;public interface org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment extends org.apache.hadoop.hbase.CoprocessorEnvironment<org.apache.hadoop.hbase.coprocessor.MasterCoprocessor> {\n  public abstract org.apache.hadoop.hbase.ServerName getServerName();\n  public abstract org.apache.hadoop.hbase.client.Connection getConnection();\n  public abstract org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.metrics.MetricRegistry getMetricRegistryForMaster();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/coprocessor/MasterObserver.class;;;public interface org.apache.hadoop.hbase.coprocessor.MasterObserver {\n  public default org.apache.hadoop.hbase.client.TableDescriptor preCreateTableRegionsInfos(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void preCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public default void postCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public default void preCreateTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public default void postCompletedCreateTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public default void preDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void preDeleteTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postCompletedDeleteTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void preTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void preTruncateTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postCompletedTruncateTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.TableDescriptor preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void postModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default java.lang.String preModifyTableStoreFileTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, java.lang.String) throws java.io.IOException;\n  public default void postModifyTableStoreFileTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, java.lang.String) throws java.io.IOException;\n  public default java.lang.String preModifyColumnFamilyStoreFileTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[], java.lang.String) throws java.io.IOException;\n  public default void postModifyColumnFamilyStoreFileTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[], java.lang.String) throws java.io.IOException;\n  public default void preModifyTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void postCompletedModifyTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void preEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void preEnableTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postCompletedEnableTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void preDisableTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postCompletedDisableTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void preAbortProcedure(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, long) throws java.io.IOException;\n  public default void postAbortProcedure(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preGetProcedures(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postGetProcedures(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preGetLocks(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postGetLocks(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public default void postMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public default void preAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public default void postAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public default void preUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public default void postUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public default void preRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public default void postRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public default void preBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public default void postBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.BalanceRequest, java.util.List<org.apache.hadoop.hbase.master.RegionPlan>) throws java.io.IOException;\n  public default void preSetSplitOrMergeEnabled(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, org.apache.hadoop.hbase.client.MasterSwitchType) throws java.io.IOException;\n  public default void postSetSplitOrMergeEnabled(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, org.apache.hadoop.hbase.client.MasterSwitchType) throws java.io.IOException;\n  public default void preSplitRegion(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException;\n  public default void preSplitRegionAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException;\n  public default void postCompletedSplitRegionAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public default void preSplitRegionBeforeMETAAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, byte[], java.util.List<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public default void preSplitRegionAfterMETAAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postRollBackSplitRegionAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preMergeRegionsAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public default void postCompletedMergeRegionsAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public default void preMergeRegionsCommitAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo[], java.util.List<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public default void postMergeRegionsCommitAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public default void postRollBackMergeRegionsAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public default void preBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException;\n  public default void postBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, boolean) throws java.io.IOException;\n  public default void preShutdown(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preStopMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preMasterInitialization(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void postSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void postCompletedSnapshotAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void preListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public default void postListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public default void preCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void postCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void preRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void postRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public default void preDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public default void postDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public default void preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public default void postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public default void preGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public default void postGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public default void preCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public default void postCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public default void preDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public default void postModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public default void preGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public default void preListNamespaces(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<java.lang.String>) throws java.io.IOException;\n  public default void postListNamespaces(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<java.lang.String>) throws java.io.IOException;\n  public default void preListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException;\n  public default void postListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException;\n  public default void preTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void preMasterStoreFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postMasterStoreFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void postSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void preSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void postSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void preSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void postSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void preSetRegionServerQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void postSetRegionServerQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public default void preMergeRegions(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public default void postMergeRegions(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public default void preMoveServersAndTables(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.net.Address>, java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public default void postMoveServersAndTables(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.net.Address>, java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public default void preMoveServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String) throws java.io.IOException;\n  public default void postMoveServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String) throws java.io.IOException;\n  public default void preMoveTables(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public default void postMoveTables(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public default void preAddRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postAddRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preRemoveRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postRemoveRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preBalanceRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public default void postBalanceRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.client.BalanceRequest, org.apache.hadoop.hbase.client.BalanceResponse) throws java.io.IOException;\n  public default void preRemoveServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.net.Address>) throws java.io.IOException;\n  public default void postRemoveServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.net.Address>) throws java.io.IOException;\n  public default void preGetRSGroupInfo(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postGetRSGroupInfo(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preGetRSGroupInfoOfTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void postGetRSGroupInfoOfTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void preListRSGroups(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postListRSGroups(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preListTablesInRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postListTablesInRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preRenameRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String) throws java.io.IOException;\n  public default void postRenameRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String) throws java.io.IOException;\n  public default void preUpdateRSGroupConfig(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public default void postUpdateRSGroupConfig(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public default void preGetConfiguredNamespacesAndTablesInRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postGetConfiguredNamespacesAndTablesInRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preGetRSGroupInfoOfServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.net.Address) throws java.io.IOException;\n  public default void postGetRSGroupInfoOfServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.net.Address) throws java.io.IOException;\n  public default void preAddReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public default void postAddReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public default void preRemoveReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postRemoveReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preEnableReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postEnableReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preDisableReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postDisableReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preGetReplicationPeerConfig(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postGetReplicationPeerConfig(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preUpdateReplicationPeerConfig(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public default void postUpdateReplicationPeerConfig(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public default void preListReplicationPeers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void postListReplicationPeers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public default void preTransitReplicationPeerSyncReplicationState(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState) throws java.io.IOException;\n  public default void postTransitReplicationPeerSyncReplicationState(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState) throws java.io.IOException;\n  public default void preRequestLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo[], java.lang.String) throws java.io.IOException;\n  public default void postRequestLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo[], java.lang.String) throws java.io.IOException;\n  public default void preLockHeartbeat(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, java.lang.String) throws java.io.IOException;\n  public default void postLockHeartbeat(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preGetClusterMetrics(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postGetClusterMetrics(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.ClusterMetrics) throws java.io.IOException;\n  public default void preClearDeadServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postClearDeadServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.ServerName>, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException;\n  public default void preDecommissionRegionServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.ServerName>, boolean) throws java.io.IOException;\n  public default void postDecommissionRegionServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.ServerName>, boolean) throws java.io.IOException;\n  public default void preListDecommissionedRegionServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postListDecommissionedRegionServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preRecommissionRegionServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.ServerName, java.util.List<byte[]>) throws java.io.IOException;\n  public default void postRecommissionRegionServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.ServerName, java.util.List<byte[]>) throws java.io.IOException;\n  public default void preSwitchRpcThrottle(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException;\n  public default void postSwitchRpcThrottle(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, boolean) throws java.io.IOException;\n  public default void preIsRpcThrottleEnabled(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postIsRpcThrottleEnabled(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException;\n  public default void preSwitchExceedThrottleQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException;\n  public default void postSwitchExceedThrottleQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, boolean) throws java.io.IOException;\n  public default void preGrant(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.security.access.UserPermission, boolean) throws java.io.IOException;\n  public default void postGrant(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.security.access.UserPermission, boolean) throws java.io.IOException;\n  public default void preRevoke(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.security.access.UserPermission) throws java.io.IOException;\n  public default void postRevoke(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.security.access.UserPermission) throws java.io.IOException;\n  public default void preGetUserPermissions(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[]) throws java.io.IOException;\n  public default void postGetUserPermissions(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[]) throws java.io.IOException;\n  public default void preHasUserPermissions(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.util.List<org.apache.hadoop.hbase.security.access.Permission>) throws java.io.IOException;\n  public default void postHasUserPermissions(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.util.List<org.apache.hadoop.hbase.security.access.Permission>) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/coprocessor/MetaTableMetrics$1.class;;;class org.apache.hadoop.hbase.coprocessor.MetaTableMetrics$1 {\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/coprocessor/MetaTableMetrics$ExampleRegionObserverMeta.class;;;class org.apache.hadoop.hbase.coprocessor.MetaTableMetrics$ExampleRegionObserverMeta implements org.apache.hadoop.hbase.coprocessor.RegionCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionObserver {\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionObserver> getRegionObserver();\n  public void preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public void prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n  public void preDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability);\n}\n;;;Yes, it is a message definition that might be put on a message queue in the context of Apache Hadoop HBase coprocessors.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/MetaTableMetrics$MetaTableOps.class;;;final class org.apache.hadoop.hbase.coprocessor.MetaTableMetrics$MetaTableOps extends java.lang.Enum<org.apache.hadoop.hbase.coprocessor.MetaTableMetrics$MetaTableOps> {\n  public static final org.apache.hadoop.hbase.coprocessor.MetaTableMetrics$MetaTableOps GET;\n  public static final org.apache.hadoop.hbase.coprocessor.MetaTableMetrics$MetaTableOps PUT;\n  public static final org.apache.hadoop.hbase.coprocessor.MetaTableMetrics$MetaTableOps DELETE;\n  public static org.apache.hadoop.hbase.coprocessor.MetaTableMetrics$MetaTableOps[] values();\n  public static org.apache.hadoop.hbase.coprocessor.MetaTableMetrics$MetaTableOps valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/MetaTableMetrics.class;;;public class org.apache.hadoop.hbase.coprocessor.MetaTableMetrics implements org.apache.hadoop.hbase.coprocessor.RegionCoprocessor {\n  public org.apache.hadoop.hbase.coprocessor.MetaTableMetrics();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionObserver> getRegionObserver();\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/MetricsCoprocessor.class;;;public class org.apache.hadoop.hbase.coprocessor.MetricsCoprocessor {\n  public org.apache.hadoop.hbase.coprocessor.MetricsCoprocessor();\n  public static org.apache.hadoop.hbase.metrics.MetricRegistry createRegistryForMasterCoprocessor(java.lang.String);\n  public static org.apache.hadoop.hbase.metrics.MetricRegistry createRegistryForRSCoprocessor(java.lang.String);\n  public static org.apache.hadoop.hbase.metrics.MetricRegistryInfo createRegistryInfoForRegionCoprocessor(java.lang.String);\n  public static org.apache.hadoop.hbase.metrics.MetricRegistry createRegistryForRegionCoprocessor(java.lang.String);\n  public static org.apache.hadoop.hbase.metrics.MetricRegistryInfo createRegistryInfoForWALCoprocessor(java.lang.String);\n  public static org.apache.hadoop.hbase.metrics.MetricRegistry createRegistryForWALCoprocessor(java.lang.String);\n  public static void removeRegistry(org.apache.hadoop.hbase.metrics.MetricRegistry);\n}\n;;;No. This class provides methods to create and manage metric registries in HBase coprocessors, but it is not a message definition that can be put on a message queue.;;;N;;;No, it is not a task definition. It is a class containing static methods and a constructor.;;;N
org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint$1.class;;;class org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint$1 {\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/coprocessor/MultiRowMutationEndpoint.class;;;public class org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint extends org.apache.hadoop.hbase.shaded.protobuf.generated.MultiRowMutationProtos$MultiRowMutationService implements org.apache.hadoop.hbase.coprocessor.RegionCoprocessor {\n  public org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint();\n  public void mutateRows(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MultiRowMutationProtos$MutateRowsRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.MultiRowMutationProtos$MutateRowsResponse>);\n  public java.lang.Iterable<org.apache.hbase.thirdparty.com.google.protobuf.Service> getServices();\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/ObserverContext.class;;;public interface org.apache.hadoop.hbase.coprocessor.ObserverContext<E extends org.apache.hadoop.hbase.CoprocessorEnvironment> {\n  public abstract E getEnvironment();\n  public abstract void bypass();\n  public abstract java.util.Optional<org.apache.hadoop.hbase.security.User> getCaller();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/ObserverContextImpl.class;;;public class org.apache.hadoop.hbase.coprocessor.ObserverContextImpl<E extends org.apache.hadoop.hbase.CoprocessorEnvironment> implements org.apache.hadoop.hbase.coprocessor.ObserverContext<E> {\n  public org.apache.hadoop.hbase.coprocessor.ObserverContextImpl(org.apache.hadoop.hbase.security.User);\n  public org.apache.hadoop.hbase.coprocessor.ObserverContextImpl(org.apache.hadoop.hbase.security.User, boolean);\n  public E getEnvironment();\n  public void prepare(E);\n  public boolean isBypassable();\n  public void bypass();\n  public boolean shouldBypass();\n  public java.util.Optional<org.apache.hadoop.hbase.security.User> getCaller();\n  public static <E extends org.apache.hadoop.hbase.CoprocessorEnvironment> org.apache.hadoop.hbase.coprocessor.ObserverContext<E> createAndPrepare(E);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class that defines the implementation of an Observer Context used in the Hadoop HBase Coprocessor framework.;;;N
org/apache/hadoop/hbase/coprocessor/ReadOnlyConfiguration.class;;;public class org.apache.hadoop.hbase.coprocessor.ReadOnlyConfiguration extends org.apache.hadoop.conf.Configuration {\n  public org.apache.hadoop.hbase.coprocessor.ReadOnlyConfiguration(org.apache.hadoop.conf.Configuration);\n  public void setDeprecatedProperties();\n  public void addResource(java.lang.String);\n  public void addResource(java.net.URL);\n  public void addResource(org.apache.hadoop.fs.Path);\n  public void addResource(java.io.InputStream);\n  public void addResource(java.io.InputStream, java.lang.String);\n  public void addResource(org.apache.hadoop.conf.Configuration);\n  public synchronized void reloadConfiguration();\n  public java.lang.String get(java.lang.String);\n  public void setAllowNullValueProperties(boolean);\n  public java.lang.String getTrimmed(java.lang.String);\n  public java.lang.String getTrimmed(java.lang.String, java.lang.String);\n  public java.lang.String getRaw(java.lang.String);\n  public void set(java.lang.String, java.lang.String);\n  public void set(java.lang.String, java.lang.String, java.lang.String);\n  public synchronized void unset(java.lang.String);\n  public synchronized void setIfUnset(java.lang.String, java.lang.String);\n  public java.lang.String get(java.lang.String, java.lang.String);\n  public int getInt(java.lang.String, int);\n  public int[] getInts(java.lang.String);\n  public void setInt(java.lang.String, int);\n  public long getLong(java.lang.String, long);\n  public long getLongBytes(java.lang.String, long);\n  public void setLong(java.lang.String, long);\n  public float getFloat(java.lang.String, float);\n  public void setFloat(java.lang.String, float);\n  public double getDouble(java.lang.String, double);\n  public void setDouble(java.lang.String, double);\n  public boolean getBoolean(java.lang.String, boolean);\n  public void setBoolean(java.lang.String, boolean);\n  public void setBooleanIfUnset(java.lang.String, boolean);\n  public <T extends java.lang.Enum<T>> void setEnum(java.lang.String, T);\n  public <T extends java.lang.Enum<T>> T getEnum(java.lang.String, T);\n  public void setTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public long getTimeDuration(java.lang.String, long, java.util.concurrent.TimeUnit);\n  public java.util.regex.Pattern getPattern(java.lang.String, java.util.regex.Pattern);\n  public void setPattern(java.lang.String, java.util.regex.Pattern);\n  public synchronized java.lang.String[] getPropertySources(java.lang.String);\n  public org.apache.hadoop.conf.Configuration$IntegerRanges getRange(java.lang.String, java.lang.String);\n  public java.util.Collection<java.lang.String> getStringCollection(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String);\n  public java.lang.String[] getStrings(java.lang.String, java.lang.String...);\n  public java.util.Collection<java.lang.String> getTrimmedStringCollection(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String);\n  public java.lang.String[] getTrimmedStrings(java.lang.String, java.lang.String...);\n  public void setStrings(java.lang.String, java.lang.String...);\n  public char[] getPassword(java.lang.String) throws java.io.IOException;\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, java.lang.String, int);\n  public java.net.InetSocketAddress getSocketAddr(java.lang.String, java.lang.String, int);\n  public void setSocketAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.lang.String, java.lang.String, java.net.InetSocketAddress);\n  public java.net.InetSocketAddress updateConnectAddr(java.lang.String, java.net.InetSocketAddress);\n  public java.lang.Class<?> getClassByName(java.lang.String) throws java.lang.ClassNotFoundException;\n  public java.lang.Class<?> getClassByNameOrNull(java.lang.String);\n  public java.lang.Class<?>[] getClasses(java.lang.String, java.lang.Class<?>...);\n  public java.lang.Class<?> getClass(java.lang.String, java.lang.Class<?>);\n  public <U> java.lang.Class<? extends U> getClass(java.lang.String, java.lang.Class<? extends U>, java.lang.Class<U>);\n  public <U> java.util.List<U> getInstances(java.lang.String, java.lang.Class<U>);\n  public void setClass(java.lang.String, java.lang.Class<?>, java.lang.Class<?>);\n  public org.apache.hadoop.fs.Path getLocalPath(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.io.File getFile(java.lang.String, java.lang.String) throws java.io.IOException;\n  public java.net.URL getResource(java.lang.String);\n  public java.io.InputStream getConfResourceAsInputStream(java.lang.String);\n  public java.io.Reader getConfResourceAsReader(java.lang.String);\n  public java.util.Set<java.lang.String> getFinalParameters();\n  public int size();\n  public void clear();\n  public java.util.Iterator<java.util.Map$Entry<java.lang.String, java.lang.String>> iterator();\n  public void writeXml(java.io.OutputStream) throws java.io.IOException;\n  public void writeXml(java.io.Writer) throws java.io.IOException;\n  public java.lang.ClassLoader getClassLoader();\n  public void setClassLoader(java.lang.ClassLoader);\n  public java.lang.String toString();\n  public synchronized void setQuietMode(boolean);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.String> getValByRegex(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/RegionCoprocessor.class;;;public interface org.apache.hadoop.hbase.coprocessor.RegionCoprocessor extends org.apache.hadoop.hbase.Coprocessor {\n  public default java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionObserver> getRegionObserver();\n  public default java.util.Optional<org.apache.hadoop.hbase.coprocessor.EndpointObserver> getEndpointObserver();\n  public default java.util.Optional<org.apache.hadoop.hbase.coprocessor.BulkLoadObserver> getBulkLoadObserver();\n}\n;;;No, it is an interface definition for a HBase region coprocessor with getter methods for optional observer instances. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/RegionCoprocessorEnvironment.class;;;public interface org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment extends org.apache.hadoop.hbase.CoprocessorEnvironment<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor> {\n  public abstract org.apache.hadoop.hbase.regionserver.Region getRegion();\n  public abstract org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public abstract org.apache.hadoop.hbase.regionserver.OnlineRegions getOnlineRegions();\n  public abstract java.util.concurrent.ConcurrentMap<java.lang.String, java.lang.Object> getSharedData();\n  public abstract org.apache.hadoop.hbase.ServerName getServerName();\n  public abstract org.apache.hadoop.hbase.client.Connection getConnection();\n  public abstract org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.metrics.MetricRegistry getMetricRegistryForRegionServer();\n  public abstract org.apache.hadoop.hbase.RawCellBuilder getCellBuilder();\n}\n;;;No. This is an interface definition for a HBase RegionCoprocessorEnvironment, which provides access to various resources such as the region, connection, and metrics registry. It is not a message definition that can be put on a message queue.;;;N;;;No, it is not a task definition, but rather an interface for defining the environment of a HBase region coprocessor.;;;N
org/apache/hadoop/hbase/coprocessor/RegionObserver$MutationType.class;;;public final class org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType extends java.lang.Enum<org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType> {\n  public static final org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType APPEND;\n  public static final org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType INCREMENT;\n  public static org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType[] values();\n  public static org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType valueOf(java.lang.String);\n}\n;;;No, this class is not a message definition. It is an enum type that defines possible values for a specific attribute in the HBase coprocessor RegionObserver class. It is not designed to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/RegionObserver.class;;;public interface org.apache.hadoop.hbase.coprocessor.RegionObserver {\n  public default void preOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>);\n  public default void preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public default void preFlushScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanOptions, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.regionserver.InternalScanner preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public default void postFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public default void postFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public default void preMemStoreCompaction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store) throws java.io.IOException;\n  public default void preMemStoreCompactionCompactScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanOptions) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.regionserver.InternalScanner preMemStoreCompactionCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public default void postMemStoreCompaction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store) throws java.io.IOException;\n  public default void preCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<? extends org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker) throws java.io.IOException;\n  public default void postCompactSelection(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, java.util.List<? extends org.apache.hadoop.hbase.regionserver.StoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest);\n  public default void preCompactScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.ScanOptions, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.regionserver.InternalScanner preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException;\n  public default void postCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.StoreFile, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException;\n  public default void preClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, boolean) throws java.io.IOException;\n  public default void postClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, boolean);\n  public default void preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public default void postGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public default boolean preExists(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException;\n  public default boolean postExists(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException;\n  public default void prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n  public default void prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void postPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n  public default void postPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void preDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n  public default void preDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void prePrepareTimeStampForDeleteVersion(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.client.Get) throws java.io.IOException;\n  public default void postDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n  public default void postDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public default void postBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public default void postStartRegionOperation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException;\n  public default void postCloseRegionOperation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException;\n  public default void postBatchMutateIndispensably(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, boolean) throws java.io.IOException;\n  public default boolean preCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException;\n  public default boolean preCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException;\n  public default boolean preCheckAndPutAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException;\n  public default boolean preCheckAndPutAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException;\n  public default boolean postCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException;\n  public default boolean postCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException;\n  public default boolean preCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException;\n  public default boolean preCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException;\n  public default boolean preCheckAndDeleteAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException;\n  public default boolean preCheckAndDeleteAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException;\n  public default boolean postCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException;\n  public default boolean postCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.CheckAndMutateResult preCheckAndMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.CheckAndMutate, org.apache.hadoop.hbase.client.CheckAndMutateResult) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.CheckAndMutateResult preCheckAndMutateAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.CheckAndMutate, org.apache.hadoop.hbase.client.CheckAndMutateResult) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.CheckAndMutateResult postCheckAndMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.CheckAndMutate, org.apache.hadoop.hbase.client.CheckAndMutateResult) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result preAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result preAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result preAppendAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result postAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append, org.apache.hadoop.hbase.client.Result) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result postAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result preIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result preIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result preIncrementAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result postIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment, org.apache.hadoop.hbase.client.Result) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.Result postIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void preScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.regionserver.RegionScanner postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException;\n  public default boolean preScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException;\n  public default boolean postScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException;\n  public default boolean postScannerFilterRow(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.Cell, boolean) throws java.io.IOException;\n  public default void preScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public default void postScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public default void preStoreScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.ScanOptions) throws java.io.IOException;\n  public default void preReplayWALs(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public default void postReplayWALs(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public default void preWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void postWALRestore(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void preBulkLoadHFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>) throws java.io.IOException;\n  public default void preCommitStoreFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path>>) throws java.io.IOException;\n  public default void postCommitStoreFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public default void postBulkLoadHFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>>) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.regionserver.StoreFileReader preStoreFileReaderOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.regionserver.StoreFileReader) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.regionserver.StoreFileReader postStoreFileReaderOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.regionserver.StoreFileReader) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.Cell postMutationBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.coprocessor.RegionObserver$MutationType, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public default java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> postIncrementBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>) throws java.io.IOException;\n  public default java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> postAppendBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker postInstantiateDeleteTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker) throws java.io.IOException;\n  public default void preWALAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n}\n;;;No, this class is not a message definition. It is an enum type that defines possible values for a specific attribute in the HBase coprocessor RegionObserver class. It is not designed to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/RegionServerCoprocessor.class;;;public interface org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor extends org.apache.hadoop.hbase.Coprocessor {\n  public default java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionServerObserver> getRegionServerObserver();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/RegionServerCoprocessorEnvironment.class;;;public interface org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment extends org.apache.hadoop.hbase.CoprocessorEnvironment<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor> {\n  public abstract org.apache.hadoop.hbase.ServerName getServerName();\n  public abstract org.apache.hadoop.hbase.regionserver.OnlineRegions getOnlineRegions();\n  public abstract org.apache.hadoop.hbase.client.Connection getConnection();\n  public abstract org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.metrics.MetricRegistry getMetricRegistryForRegionServer();\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an interface definition that specifies methods that can be implemented by a class, but it does not represent a specific message that can be sent or received.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/coprocessor/RegionServerObserver.class;;;public interface org.apache.hadoop.hbase.coprocessor.RegionServerObserver {\n  public default void preStopRegionServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.replication.ReplicationEndpoint postCreateReplicationEndPoint(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.replication.ReplicationEndpoint);\n  public default void preReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preClearCompactionQueues(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postClearCompactionQueues(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preExecuteProcedures(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public default void postExecuteProcedures(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n}\n;;;No, this is an interface for a class that serves as an observer on a HBase region server. It defines several methods that can be overridden to implement custom behavior in response to specific events happening on the region server.;;;N;;;No. The above class is a set of method declarations defining the interface for an observer for events happening in a HBase RegionServer. It is not a task definition that can be put on a task queue.;;;N
org/apache/hadoop/hbase/coprocessor/WALCoprocessor.class;;;public interface org.apache.hadoop.hbase.coprocessor.WALCoprocessor extends org.apache.hadoop.hbase.Coprocessor {\n  public abstract java.util.Optional<org.apache.hadoop.hbase.coprocessor.WALObserver> getWALObserver();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/WALCoprocessorEnvironment.class;;;public interface org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment extends org.apache.hadoop.hbase.CoprocessorEnvironment<org.apache.hadoop.hbase.coprocessor.WALCoprocessor> {\n  public abstract org.apache.hadoop.hbase.wal.WAL getWAL();\n  public abstract org.apache.hadoop.hbase.metrics.MetricRegistry getMetricRegistryForRegionServer();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/WALObserver.class;;;public interface org.apache.hadoop.hbase.coprocessor.WALObserver {\n  public default void preWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void postWALWrite(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void preWALRoll(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public default void postWALRoll(org.apache.hadoop.hbase.coprocessor.ObserverContext<? extends org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n;;;No. This is an interface defining methods that can be implemented by a class to act as a WAL observer in HBase. It is not a message definition that can be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/errorhandling/ForeignException$ProxyThrowable.class;;;class org.apache.hadoop.hbase.errorhandling.ForeignException$ProxyThrowable extends java.lang.Throwable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/errorhandling/ForeignException.class;;;public class org.apache.hadoop.hbase.errorhandling.ForeignException extends java.io.IOException {\n  public org.apache.hadoop.hbase.errorhandling.ForeignException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.hbase.errorhandling.ForeignException(java.lang.String, java.lang.String);\n  public java.lang.String getSource();\n  public boolean isRemote();\n  public java.lang.String toString();\n  public static byte[] serialize(java.lang.String, java.lang.Throwable);\n  public static org.apache.hadoop.hbase.errorhandling.ForeignException deserialize(byte[]) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/errorhandling/ForeignExceptionDispatcher.class;;;public class org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher implements org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener,org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare {\n  public org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher(java.lang.String);\n  public org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher();\n  public java.lang.String getName();\n  public synchronized void receive(org.apache.hadoop.hbase.errorhandling.ForeignException);\n  public synchronized void rethrowException() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public synchronized boolean hasException();\n  public synchronized org.apache.hadoop.hbase.errorhandling.ForeignException getException();\n  public synchronized void addListener(org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener);\n}\n;;;No. The class implements several interfaces and has methods for handling exceptions and listeners, but it does not define a specific message structure or format to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/errorhandling/ForeignExceptionListener.class;;;public interface org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener {\n  public abstract void receive(org.apache.hadoop.hbase.errorhandling.ForeignException);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/errorhandling/ForeignExceptionSnare.class;;;public interface org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare {\n  public abstract void rethrowException() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public abstract boolean hasException();\n  public abstract org.apache.hadoop.hbase.errorhandling.ForeignException getException();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/errorhandling/TimeoutException.class;;;public class org.apache.hadoop.hbase.errorhandling.TimeoutException extends java.lang.Exception {\n  public org.apache.hadoop.hbase.errorhandling.TimeoutException(java.lang.String, long, long, long);\n  public long getStart();\n  public long getEnd();\n  public long getMaxAllowedOperationTime();\n  public java.lang.String getSourceName();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/errorhandling/TimeoutExceptionInjector$1.class;;;class org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector$1 extends java.util.TimerTask {\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/errorhandling/TimeoutExceptionInjector.class;;;public class org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector {\n  public org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector(org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener, long);\n  public long getMaxTime();\n  public void complete();\n  public synchronized void start() throws java.lang.IllegalStateException;\n  public void trigger();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/executor/EventHandler.class;;;public abstract class org.apache.hadoop.hbase.executor.EventHandler implements java.lang.Runnable, java.lang.Comparable<org.apache.hadoop.hbase.executor.EventHandler> {\n  public org.apache.hadoop.hbase.executor.EventHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.executor.EventType);\n  public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception;\n  public void run();\n  public abstract void process() throws java.io.IOException;\n  public org.apache.hadoop.hbase.executor.EventType getEventType();\n  public int getPriority();\n  public long getSeqid();\n  public int compareTo(org.apache.hadoop.hbase.executor.EventHandler);\n  public java.lang.String toString();\n  public java.lang.String getInformativeName();\n  public int compareTo(java.lang.Object);\n}\n;;;No. This class is an abstract class and cannot be instantiated. It defines methods and behavior that can be inherited and implemented by its subclasses but it does not define a message format or structure.;;;N;;;No.;;;N
org/apache/hadoop/hbase/executor/EventType.class;;;public final class org.apache.hadoop.hbase.executor.EventType extends java.lang.Enum<org.apache.hadoop.hbase.executor.EventType> {\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REGION_CLOSED;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REGION_OPENING;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REGION_OPENED;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REGION_SPLITTING;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REGION_SPLIT;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REGION_FAILED_OPEN;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REGION_MERGING;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REGION_MERGED;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REQUEST_REGION_SPLIT;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_ZK_REQUEST_REGION_MERGE;\n  public static final org.apache.hadoop.hbase.executor.EventType M_RS_OPEN_REGION;\n  public static final org.apache.hadoop.hbase.executor.EventType M_RS_OPEN_ROOT;\n  public static final org.apache.hadoop.hbase.executor.EventType M_RS_OPEN_META;\n  public static final org.apache.hadoop.hbase.executor.EventType M_RS_CLOSE_REGION;\n  public static final org.apache.hadoop.hbase.executor.EventType M_RS_CLOSE_ROOT;\n  public static final org.apache.hadoop.hbase.executor.EventType M_RS_CLOSE_META;\n  public static final org.apache.hadoop.hbase.executor.EventType M_RS_OPEN_PRIORITY_REGION;\n  public static final org.apache.hadoop.hbase.executor.EventType M_RS_SWITCH_RPC_THROTTLE;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_MERGE_REGION;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_DELETE_TABLE;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_DISABLE_TABLE;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_ENABLE_TABLE;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_MODIFY_TABLE;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_ADD_FAMILY;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_DELETE_FAMILY;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_MODIFY_FAMILY;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_CREATE_TABLE;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_SNAPSHOT_TABLE;\n  public static final org.apache.hadoop.hbase.executor.EventType C_M_RESTORE_SNAPSHOT;\n  public static final org.apache.hadoop.hbase.executor.EventType M_ZK_REGION_OFFLINE;\n  public static final org.apache.hadoop.hbase.executor.EventType M_ZK_REGION_CLOSING;\n  public static final org.apache.hadoop.hbase.executor.EventType M_SERVER_SHUTDOWN;\n  public static final org.apache.hadoop.hbase.executor.EventType M_META_SERVER_SHUTDOWN;\n  public static final org.apache.hadoop.hbase.executor.EventType M_MASTER_RECOVERY;\n  public static final org.apache.hadoop.hbase.executor.EventType M_LOG_REPLAY;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_PARALLEL_SEEK;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_LOG_REPLAY;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_REGION_REPLICA_FLUSH;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_COMPACTED_FILES_DISCHARGER;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_REFRESH_PEER;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_REPLAY_SYNC_REPLICATION_WAL;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_CLAIM_REPLICATION_QUEUE;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_SNAPSHOT_REGIONS;\n  public static final org.apache.hadoop.hbase.executor.EventType RS_VERIFY_SNAPSHOT;\n  public static org.apache.hadoop.hbase.executor.EventType[] values();\n  public static org.apache.hadoop.hbase.executor.EventType valueOf(java.lang.String);\n  public int getCode();\n  public static org.apache.hadoop.hbase.executor.EventType get(int);\n}\n;;;No. This is not a message definition, but a class that defines constants and methods related to event types in the HBase database system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/executor/ExecutorService$Executor.class;;;class org.apache.hadoop.hbase.executor.ExecutorService$Executor {\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.executor.ExecutorService$ExecutorStatus getStatus();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/executor/ExecutorService$ExecutorConfig.class;;;public class org.apache.hadoop.hbase.executor.ExecutorService$ExecutorConfig {\n  public static final long KEEP_ALIVE_TIME_MILLIS_DEFAULT;\n  public org.apache.hadoop.hbase.executor.ExecutorService$ExecutorConfig(org.apache.hadoop.hbase.executor.ExecutorService);\n  public org.apache.hadoop.hbase.executor.ExecutorService$ExecutorConfig setExecutorType(org.apache.hadoop.hbase.executor.ExecutorType);\n  public int getCorePoolSize();\n  public org.apache.hadoop.hbase.executor.ExecutorService$ExecutorConfig setCorePoolSize(int);\n  public boolean allowCoreThreadTimeout();\n  public org.apache.hadoop.hbase.executor.ExecutorService$ExecutorConfig setAllowCoreThreadTimeout(boolean);\n  public java.lang.String getName();\n  public long getKeepAliveTimeMillis();\n  public org.apache.hadoop.hbase.executor.ExecutorService$ExecutorConfig setKeepAliveTimeMillis(long);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/executor/ExecutorService$ExecutorStatus.class;;;public class org.apache.hadoop.hbase.executor.ExecutorService$ExecutorStatus {\n  public java.util.List<org.apache.hadoop.hbase.executor.EventHandler> getQueuedEvents();\n  public java.util.List<org.apache.hadoop.hbase.executor.ExecutorService$RunningEventStatus> getRunning();\n  public void dumpTo(java.io.Writer, java.lang.String) throws java.io.IOException;\n}\n;;;yes;;;Y;;;;;;N
org/apache/hadoop/hbase/executor/ExecutorService$RunningEventStatus.class;;;public class org.apache.hadoop.hbase.executor.ExecutorService$RunningEventStatus {\n  public org.apache.hadoop.hbase.executor.ExecutorService$RunningEventStatus(java.lang.Thread, org.apache.hadoop.hbase.executor.EventHandler);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/executor/ExecutorService$TrackingThreadPoolExecutor.class;;;class org.apache.hadoop.hbase.executor.ExecutorService$TrackingThreadPoolExecutor extends java.util.concurrent.ThreadPoolExecutor {\n  public org.apache.hadoop.hbase.executor.ExecutorService$TrackingThreadPoolExecutor(int, int, long, java.util.concurrent.TimeUnit, java.util.concurrent.BlockingQueue<java.lang.Runnable>);\n  public java.util.concurrent.ConcurrentMap<java.lang.Thread, java.lang.Runnable> getRunningTasks();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/executor/ExecutorService.class;;;public class org.apache.hadoop.hbase.executor.ExecutorService {\n  public org.apache.hadoop.hbase.executor.ExecutorService(java.lang.String);\n  public void startExecutorService(org.apache.hadoop.hbase.executor.ExecutorService$ExecutorConfig);\n  public void shutdown();\n  public java.util.concurrent.ThreadPoolExecutor getExecutorThreadPool(org.apache.hadoop.hbase.executor.ExecutorType);\n  public java.util.concurrent.ThreadPoolExecutor getExecutorLazily(org.apache.hadoop.hbase.executor.ExecutorService$ExecutorConfig);\n  public void submit(org.apache.hadoop.hbase.executor.EventHandler);\n  public void delayedSubmit(org.apache.hadoop.hbase.executor.EventHandler, long, java.util.concurrent.TimeUnit);\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.executor.ExecutorService$ExecutorStatus> getAllExecutorStatuses();\n}\n;;;No, the class is a service that provides methods to manage an executor service, rather than a message definition that represents a piece of data being passed between systems.;;;N;;;No.;;;N
org/apache/hadoop/hbase/executor/ExecutorType.class;;;public final class org.apache.hadoop.hbase.executor.ExecutorType extends java.lang.Enum<org.apache.hadoop.hbase.executor.ExecutorType> {\n  public static final org.apache.hadoop.hbase.executor.ExecutorType MASTER_CLOSE_REGION;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType MASTER_OPEN_REGION;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType MASTER_SERVER_OPERATIONS;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType MASTER_TABLE_OPERATIONS;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType MASTER_RS_SHUTDOWN;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType MASTER_META_SERVER_OPERATIONS;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType M_LOG_REPLAY_OPS;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType MASTER_SNAPSHOT_OPERATIONS;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType MASTER_MERGE_OPERATIONS;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_OPEN_REGION;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_OPEN_ROOT;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_OPEN_META;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_CLOSE_REGION;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_CLOSE_ROOT;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_CLOSE_META;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_PARALLEL_SEEK;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_LOG_REPLAY_OPS;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_REGION_REPLICA_FLUSH_OPS;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_COMPACTED_FILES_DISCHARGER;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_OPEN_PRIORITY_REGION;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_REFRESH_PEER;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_REPLAY_SYNC_REPLICATION_WAL;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_SWITCH_RPC_THROTTLE;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_IN_MEMORY_COMPACTION;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_CLAIM_REPLICATION_QUEUE;\n  public static final org.apache.hadoop.hbase.executor.ExecutorType RS_SNAPSHOT_OPERATIONS;\n  public static org.apache.hadoop.hbase.executor.ExecutorType[] values();\n  public static org.apache.hadoop.hbase.executor.ExecutorType valueOf(java.lang.String);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/filter/FilterWrapper$FilterRowRetCode.class;;;public final class org.apache.hadoop.hbase.filter.FilterWrapper$FilterRowRetCode extends java.lang.Enum<org.apache.hadoop.hbase.filter.FilterWrapper$FilterRowRetCode> {\n  public static final org.apache.hadoop.hbase.filter.FilterWrapper$FilterRowRetCode NOT_CALLED;\n  public static final org.apache.hadoop.hbase.filter.FilterWrapper$FilterRowRetCode INCLUDE;\n  public static final org.apache.hadoop.hbase.filter.FilterWrapper$FilterRowRetCode EXCLUDE;\n  public static final org.apache.hadoop.hbase.filter.FilterWrapper$FilterRowRetCode INCLUDE_THIS_FAMILY;\n  public static org.apache.hadoop.hbase.filter.FilterWrapper$FilterRowRetCode[] values();\n  public static org.apache.hadoop.hbase.filter.FilterWrapper$FilterRowRetCode valueOf(java.lang.String);\n}\n;;;No. This is an enum definition, not a message definition. It defines a set of constants that can be used in a message definition.;;;N;;;No. This is not a task definition, it is an enumeration class in Java.;;;N
org/apache/hadoop/hbase/filter/FilterWrapper.class;;;public final class org.apache.hadoop.hbase.filter.FilterWrapper extends org.apache.hadoop.hbase.filter.Filter {\n  public org.apache.hadoop.hbase.filter.FilterWrapper(org.apache.hadoop.hbase.filter.Filter);\n  public byte[] toByteArray() throws java.io.IOException;\n  public static org.apache.hadoop.hbase.filter.FilterWrapper parseFrom(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public void reset() throws java.io.IOException;\n  public boolean filterAllRemaining() throws java.io.IOException;\n  public boolean filterRow() throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell getNextCellHint(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean filterRowKey(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.filter.Filter$ReturnCode filterCell(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell transformCell(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean hasFilterRow();\n  public void filterRowCells(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.filter.FilterWrapper$FilterRowRetCode filterRowCellsWithRet(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public boolean isFamilyEssential(byte[]) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/fs/HFileSystem$1.class;;;final class org.apache.hadoop.hbase.fs.HFileSystem$1 implements java.lang.reflect.InvocationHandler {\n  public java.lang.Object invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[]) throws java.lang.Throwable;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/fs/HFileSystem$ReorderBlocks.class;;;interface org.apache.hadoop.hbase.fs.HFileSystem$ReorderBlocks {\n  public abstract void reorderBlocks(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hdfs.protocol.LocatedBlocks, java.lang.String) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/fs/HFileSystem$ReorderWALBlocks.class;;;class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks implements org.apache.hadoop.hbase.fs.HFileSystem$ReorderBlocks {\n  public void reorderBlocks(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hdfs.protocol.LocatedBlocks, java.lang.String) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/fs/HFileSystem.class;;;public class org.apache.hadoop.hbase.fs.HFileSystem extends org.apache.hadoop.fs.FilterFileSystem {\n  public static final org.slf4j.Logger LOG;\n  public org.apache.hadoop.hbase.fs.HFileSystem(org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.fs.HFileSystem(org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.fs.FileSystem getNoChecksumFs();\n  public org.apache.hadoop.fs.FileSystem getBackingFs() throws java.io.IOException;\n  public void setStoragePolicy(org.apache.hadoop.fs.Path, java.lang.String);\n  public java.lang.String getStoragePolicyName(org.apache.hadoop.fs.Path);\n  public boolean useHBaseChecksum();\n  public void close() throws java.io.IOException;\n  public static boolean addLocationsOrderInterceptor(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem get(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileSystem getLocalFs(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataOutputStream createNonRecursive(org.apache.hadoop.fs.Path, boolean, int, short, long, org.apache.hadoop.util.Progressable) throws java.io.IOException;\n}\n;;;No, the class definition does not contain any message or data properties that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/footer_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.footer_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.footer_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/hbck_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.hbck_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.hbck_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/header_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.header_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.header_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/master_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.master_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.master_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/operationDetails_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.operationDetails_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.operationDetails_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/procedures_jsp$1.class;;;class org.apache.hadoop.hbase.generated.master.procedures_jsp$1 implements java.util.Comparator<org.apache.hadoop.hbase.procedure2.Procedure> {\n  public int compare(org.apache.hadoop.hbase.procedure2.Procedure, org.apache.hadoop.hbase.procedure2.Procedure);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/procedures_jsp$2.class;;;class org.apache.hadoop.hbase.generated.master.procedures_jsp$2 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/procedures_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.procedures_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.procedures_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/processMaster_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.processMaster_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.processMaster_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/quotas_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.quotas_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.quotas_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No. This class is a JSP (JavaServer Page) definition for a web application, and is not related to message queues.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/redirect_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.redirect_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.redirect_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/rits_jsp$1.class;;;class org.apache.hadoop.hbase.generated.master.rits_jsp$1 implements java.util.Comparator<org.apache.hadoop.hbase.master.assignment.RegionStateNode> {\n  public int compare(org.apache.hadoop.hbase.master.assignment.RegionStateNode, org.apache.hadoop.hbase.master.assignment.RegionStateNode);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/rits_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.rits_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.rits_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No. This is a JSP (JavaServer Pages) implementation class, not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/rsgroup_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.rsgroup_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.rsgroup_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/snapshot_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.snapshot_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.snapshot_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/snapshotsStats_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.snapshotsStats_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.snapshotsStats_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/table_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.table_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.table_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition for a JSP page in the Apache HBase master web interface.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/tablesDetailed_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.tablesDetailed_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.tablesDetailed_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/userSnapshots_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.userSnapshots_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.userSnapshots_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/master/zk_jsp.class;;;public final class org.apache.hadoop.hbase.generated.master.zk_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.master.zk_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No. It is a Java class that extends HttpJspBase and implements JspSourceDependent. It does not have any fields or methods that define a message to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/regionserver/footer_jsp.class;;;public final class org.apache.hadoop.hbase.generated.regionserver.footer_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.regionserver.footer_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/regionserver/header_jsp.class;;;public final class org.apache.hadoop.hbase.generated.regionserver.header_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.regionserver.header_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/regionserver/processRS_jsp.class;;;public final class org.apache.hadoop.hbase.generated.regionserver.processRS_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.regionserver.processRS_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/regionserver/region_jsp.class;;;public final class org.apache.hadoop.hbase.generated.regionserver.region_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.regionserver.region_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/regionserver/regionserver_jsp.class;;;public final class org.apache.hadoop.hbase.generated.regionserver.regionserver_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.regionserver.regionserver_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No. It is a Java class definition for a JSP file.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/regionserver/rsOperationDetails_jsp.class;;;public final class org.apache.hadoop.hbase.generated.regionserver.rsOperationDetails_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.regionserver.rsOperationDetails_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/generated/regionserver/storeFile_jsp.class;;;public final class org.apache.hadoop.hbase.generated.regionserver.storeFile_jsp extends org.apache.jasper.runtime.HttpJspBase implements org.apache.jasper.runtime.JspSourceDependent {\n  public org.apache.hadoop.hbase.generated.regionserver.storeFile_jsp();\n  public java.util.List<java.lang.String> getDependants();\n  public void _jspService(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException, javax.servlet.ServletException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/FSDataInputStreamWrapper$1.class;;;class org.apache.hadoop.hbase.io.FSDataInputStreamWrapper$1 {\n}\n;;;No. This is a class definition and not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/FSDataInputStreamWrapper$ReadStatistics.class;;;class org.apache.hadoop.hbase.io.FSDataInputStreamWrapper$ReadStatistics {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue, as it does not represent a unit of work to be executed asynchronously.;;;N
org/apache/hadoop/hbase/io/FSDataInputStreamWrapper.class;;;public class org.apache.hadoop.hbase.io.FSDataInputStreamWrapper implements java.io.Closeable {\n  public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, long) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.FileLink, boolean, long) throws java.io.IOException;\n  public void prepareForBlockReader(boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FSDataInputStream);\n  public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper(org.apache.hadoop.fs.FSDataInputStream, org.apache.hadoop.fs.FSDataInputStream);\n  public boolean shouldUseHBaseChecksum();\n  public org.apache.hadoop.fs.FSDataInputStream getStream(boolean);\n  public org.apache.hadoop.fs.FSDataInputStream fallbackToFsChecksum(int) throws java.io.IOException;\n  public void checksumOk();\n  public static long getTotalBytesRead();\n  public static long getLocalBytesRead();\n  public static long getShortCircuitBytesRead();\n  public static long getZeroCopyBytesRead();\n  public void close();\n  public org.apache.hadoop.hbase.fs.HFileSystem getHfs();\n  public void unbuffer();\n}\n;;;No. This class does not define the structure of a message and does not contain any message-specific properties or methods. It is a wrapper class for handling input streams in the Hadoop ecosystem.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/FileLink$FileLinkInputStream.class;;;class org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream extends java.io.InputStream implements org.apache.hadoop.fs.Seekable,org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.CanSetDropBehind,org.apache.hadoop.fs.CanSetReadahead,org.apache.hadoop.fs.CanUnbuffer {\n  public org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.FileLink) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.FileLink$FileLinkInputStream(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.io.FileLink, int) throws java.io.IOException;\n  public int read() throws java.io.IOException;\n  public int read(byte[]) throws java.io.IOException;\n  public int read(byte[], int, int) throws java.io.IOException;\n  public int read(long, byte[], int, int) throws java.io.IOException;\n  public void readFully(long, byte[]) throws java.io.IOException;\n  public void readFully(long, byte[], int, int) throws java.io.IOException;\n  public long skip(long) throws java.io.IOException;\n  public int available() throws java.io.IOException;\n  public void seek(long) throws java.io.IOException;\n  public long getPos() throws java.io.IOException;\n  public boolean seekToNewSource(long) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public synchronized void mark(int);\n  public synchronized void reset() throws java.io.IOException;\n  public boolean markSupported();\n  public void unbuffer();\n  public void setReadahead(java.lang.Long) throws java.io.IOException, java.lang.UnsupportedOperationException;\n  public void setDropBehind(java.lang.Boolean) throws java.io.IOException, java.lang.UnsupportedOperationException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/FileLink.class;;;public class org.apache.hadoop.hbase.io.FileLink {\n  public static final java.lang.String BACK_REFERENCES_DIRECTORY_PREFIX;\n  public org.apache.hadoop.hbase.io.FileLink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path...);\n  public org.apache.hadoop.hbase.io.FileLink(java.util.Collection<org.apache.hadoop.fs.Path>);\n  public org.apache.hadoop.fs.Path[] getLocations();\n  public java.lang.String toString();\n  public boolean exists(org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getAvailablePath(org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getFileStatus(org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public org.apache.hadoop.fs.FSDataInputStream open(org.apache.hadoop.fs.FileSystem, int) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FSDataInputStream getUnderlyingFileLinkInputStream(org.apache.hadoop.fs.FSDataInputStream);\n  public static org.apache.hadoop.fs.Path getBackReferencesDir(org.apache.hadoop.fs.Path, java.lang.String);\n  public static java.lang.String getBackReferenceFileName(org.apache.hadoop.fs.Path);\n  public static boolean isBackReferencesDir(org.apache.hadoop.fs.Path);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;No. This class appears to be related to file management in Hadoop and does not define any specific message or message format.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/HFileLink.class;;;public class org.apache.hadoop.hbase.io.HFileLink extends org.apache.hadoop.hbase.io.FileLink {\n  public static final java.lang.String LINK_NAME_REGEX;\n  public static final java.util.regex.Pattern LINK_NAME_PATTERN;\n  public org.apache.hadoop.hbase.io.HFileLink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public static final org.apache.hadoop.hbase.io.HFileLink buildFromHFileLinkPattern(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static final org.apache.hadoop.hbase.io.HFileLink buildFromHFileLinkPattern(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public static org.apache.hadoop.fs.Path createPath(org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String, java.lang.String);\n  public static org.apache.hadoop.hbase.io.HFileLink build(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getOriginPath();\n  public org.apache.hadoop.fs.Path getArchivePath();\n  public org.apache.hadoop.fs.Path getMobPath();\n  public static boolean isHFileLink(org.apache.hadoop.fs.Path);\n  public static boolean isHFileLink(java.lang.String);\n  public static java.lang.String getReferencedHFileName(java.lang.String);\n  public static java.lang.String getReferencedRegionName(java.lang.String);\n  public static org.apache.hadoop.hbase.TableName getReferencedTableName(java.lang.String);\n  public static java.lang.String createHFileLinkName(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String);\n  public static java.lang.String createHFileLinkName(org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String);\n  public static java.lang.String create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, java.lang.String) throws java.io.IOException;\n  public static java.lang.String create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, boolean) throws java.io.IOException;\n  public static java.lang.String create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static java.lang.String create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public static java.lang.String create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public static java.lang.String createFromHFileLink(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, boolean) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getHFileFromBackReference(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public static org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.TableName, java.lang.String> parseBackReferenceName(java.lang.String);\n  public static org.apache.hadoop.fs.Path getHFileFromBackReference(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n;;;Yes, this class might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/HalfStoreFileReader$1.class;;;class org.apache.hadoop.hbase.io.HalfStoreFileReader$1 implements org.apache.hadoop.hbase.io.hfile.HFileScanner {\n  public boolean atEnd;\n  public org.apache.hadoop.hbase.Cell getKey();\n  public java.lang.String getKeyString();\n  public java.nio.ByteBuffer getValue();\n  public java.lang.String getValueString();\n  public org.apache.hadoop.hbase.Cell getCell();\n  public boolean next() throws java.io.IOException;\n  public boolean seekTo() throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.hfile.HFile$Reader getReader();\n  public boolean isSeeked();\n  public int seekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public int reseekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean seekBefore(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell getNextIndexedKey();\n  public void close();\n  public void shipped() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/HalfStoreFileReader.class;;;public class org.apache.hadoop.hbase.io.HalfStoreFileReader extends org.apache.hadoop.hbase.regionserver.StoreFileReader {\n  public org.apache.hadoop.hbase.io.HalfStoreFileReader(org.apache.hadoop.hbase.io.hfile.ReaderContext, org.apache.hadoop.hbase.io.hfile.HFileInfo, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, java.util.concurrent.atomic.AtomicInteger, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(boolean, boolean, boolean);\n  public boolean passesKeyRangeFilter(org.apache.hadoop.hbase.client.Scan);\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> getLastKey();\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> midKey() throws java.io.IOException;\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> getFirstKey();\n  public long getEntries();\n  public long getFilterEntries();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/MetricsIO.class;;;public class org.apache.hadoop.hbase.io.MetricsIO {\n  public org.apache.hadoop.hbase.io.MetricsIO(org.apache.hadoop.hbase.io.MetricsIOWrapper);\n  public org.apache.hadoop.hbase.io.MetricsIOSource getMetricsSource();\n  public org.apache.hadoop.hbase.io.MetricsIOWrapper getWrapper();\n  public void updateFsReadTime(long);\n  public void updateFsPreadTime(long);\n  public void updateFsWriteTime(long);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/io/MetricsIOWrapperImpl.class;;;public class org.apache.hadoop.hbase.io.MetricsIOWrapperImpl implements org.apache.hadoop.hbase.io.MetricsIOWrapper {\n  public org.apache.hadoop.hbase.io.MetricsIOWrapperImpl();\n  public long getChecksumFailures();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/Reference$Range.class;;;final class org.apache.hadoop.hbase.io.Reference$Range extends java.lang.Enum<org.apache.hadoop.hbase.io.Reference$Range> {\n  public static final org.apache.hadoop.hbase.io.Reference$Range top;\n  public static final org.apache.hadoop.hbase.io.Reference$Range bottom;\n  public static org.apache.hadoop.hbase.io.Reference$Range[] values();\n  public static org.apache.hadoop.hbase.io.Reference$Range valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/Reference.class;;;public class org.apache.hadoop.hbase.io.Reference {\n  public static org.apache.hadoop.hbase.io.Reference createTopReference(byte[]);\n  public static org.apache.hadoop.hbase.io.Reference createBottomReference(byte[]);\n  public org.apache.hadoop.hbase.io.Reference();\n  public org.apache.hadoop.hbase.io.Reference$Range getFileRegion();\n  public byte[] getSplitKey();\n  public java.lang.String toString();\n  public static boolean isTopFileRegion(org.apache.hadoop.hbase.io.Reference$Range);\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path write(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.io.Reference read(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.FSProtos$Reference convert();\n  public static org.apache.hadoop.hbase.io.Reference convert(org.apache.hadoop.hbase.shaded.protobuf.generated.FSProtos$Reference);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/WALLink.class;;;public class org.apache.hadoop.hbase.io.WALLink extends org.apache.hadoop.hbase.io.FileLink {\n  public org.apache.hadoop.hbase.io.WALLink(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.WALLink(org.apache.hadoop.fs.Path, java.lang.String, java.lang.String);\n  public org.apache.hadoop.hbase.io.WALLink(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/WritableWithSize.class;;;public interface org.apache.hadoop.hbase.io.WritableWithSize {\n  public abstract long getWritableSize();\n}\n;;;No. This class only defines an interface method that returns the size of the object when written to disk. It does not define any message structure or content.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/AgeSnapshot.class;;;public class org.apache.hadoop.hbase.io.hfile.AgeSnapshot {\n  public double get75thPercentile();\n  public double get95thPercentile();\n  public double get98thPercentile();\n  public double get99thPercentile();\n  public double get999thPercentile();\n  public double getMean();\n  public double getMax();\n  public double getMin();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/BlockCache.class;;;public interface org.apache.hadoop.hbase.io.hfile.BlockCache extends java.lang.Iterable<org.apache.hadoop.hbase.io.hfile.CachedBlock> {\n  public abstract void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean);\n  public abstract void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable);\n  public abstract org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean);\n  public default org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.BlockType);\n  public abstract boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public abstract int evictBlocksByHfileName(java.lang.String);\n  public abstract org.apache.hadoop.hbase.io.hfile.CacheStats getStats();\n  public abstract void shutdown();\n  public abstract long size();\n  public abstract long getMaxSize();\n  public abstract long getFreeSize();\n  public abstract long getCurrentSize();\n  public abstract long getCurrentDataSize();\n  public abstract long getBlockCount();\n  public abstract long getDataBlockCount();\n  public abstract java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator();\n  public abstract org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches();\n  public default boolean isMetaBlock(org.apache.hadoop.hbase.io.hfile.BlockType);\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/BlockCacheFactory$ExternalBlockCaches.class;;;final class org.apache.hadoop.hbase.io.hfile.BlockCacheFactory$ExternalBlockCaches extends java.lang.Enum<org.apache.hadoop.hbase.io.hfile.BlockCacheFactory$ExternalBlockCaches> {\n  public static final org.apache.hadoop.hbase.io.hfile.BlockCacheFactory$ExternalBlockCaches memcached;\n  public static org.apache.hadoop.hbase.io.hfile.BlockCacheFactory$ExternalBlockCaches[] values();\n  public static org.apache.hadoop.hbase.io.hfile.BlockCacheFactory$ExternalBlockCaches valueOf(java.lang.String);\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue. It is a Java class definition.;;;N
org/apache/hadoop/hbase/io/hfile/BlockCacheFactory.class;;;public final class org.apache.hadoop.hbase.io.hfile.BlockCacheFactory {\n  public static final java.lang.String BLOCKCACHE_POLICY_KEY;\n  public static final java.lang.String BLOCKCACHE_POLICY_DEFAULT;\n  public static final java.lang.String BUCKET_CACHE_PERSISTENT_PATH_KEY;\n  public static final java.lang.String BUCKET_CACHE_WRITER_THREADS_KEY;\n  public static final java.lang.String BUCKET_CACHE_WRITER_QUEUE_KEY;\n  public static final java.lang.String BUCKET_CACHE_BUCKETS_KEY;\n  public static final int DEFAULT_BUCKET_CACHE_WRITER_THREADS;\n  public static final int DEFAULT_BUCKET_CACHE_WRITER_QUEUE;\n  public static final java.lang.String BLOCKCACHE_BLOCKSIZE_KEY;\n  public static org.apache.hadoop.hbase.io.hfile.BlockCache createBlockCache(org.apache.hadoop.conf.Configuration);\n}\n;;;No, this class provides methods and constants to create and configure a block cache, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/BlockCacheKey.class;;;public class org.apache.hadoop.hbase.io.hfile.BlockCacheKey implements org.apache.hadoop.hbase.io.HeapSize,java.io.Serializable {\n  public static final long FIXED_OVERHEAD;\n  public org.apache.hadoop.hbase.io.hfile.BlockCacheKey(java.lang.String, long);\n  public org.apache.hadoop.hbase.io.hfile.BlockCacheKey(java.lang.String, long, boolean, org.apache.hadoop.hbase.io.hfile.BlockType);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n  public long heapSize();\n  public java.lang.String getHfileName();\n  public boolean isPrimary();\n  public long getOffset();\n  public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n}\n;;;No, this class does not have any relevant information or data to be put on a message queue. It is simply a key used for caching block objects.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/BlockCacheUtil$1.class;;;final class org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$1 extends org.apache.hbase.thirdparty.com.google.gson.TypeAdapter<org.apache.hadoop.hbase.metrics.impl.FastLongHistogram> {\n  public void write(org.apache.hbase.thirdparty.com.google.gson.stream.JsonWriter, org.apache.hadoop.hbase.metrics.impl.FastLongHistogram) throws java.io.IOException;\n  public org.apache.hadoop.hbase.metrics.impl.FastLongHistogram read(org.apache.hbase.thirdparty.com.google.gson.stream.JsonReader) throws java.io.IOException;\n  public java.lang.Object read(org.apache.hbase.thirdparty.com.google.gson.stream.JsonReader) throws java.io.IOException;\n  public void write(org.apache.hbase.thirdparty.com.google.gson.stream.JsonWriter, java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class definition that provides methods for writing and reading JSON data.;;;N
org/apache/hadoop/hbase/io/hfile/BlockCacheUtil$CachedBlockCountsPerFile.class;;;class org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlockCountsPerFile {\n  public int getCount();\n  public long getSize();\n  public int getCountData();\n  public long getSizeData();\n  public java.lang.String getFilename();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/BlockCacheUtil$CachedBlocksByFile.class;;;public class org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile {\n  public static final int DEFAULT_MAX;\n  public boolean update(org.apache.hadoop.hbase.io.hfile.CachedBlock);\n  public boolean isFull();\n  public java.util.NavigableMap<java.lang.String, java.util.NavigableSet<org.apache.hadoop.hbase.io.hfile.CachedBlock>> getCachedBlockStatsByFile();\n  public int getCount();\n  public int getDataCount();\n  public long getSize();\n  public long getDataSize();\n  public org.apache.hadoop.hbase.io.hfile.AgeSnapshot getAgeInCacheSnapshot();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/BlockCacheUtil.class;;;public class org.apache.hadoop.hbase.io.hfile.BlockCacheUtil {\n  public static final long NANOS_PER_SECOND;\n  public org.apache.hadoop.hbase.io.hfile.BlockCacheUtil();\n  public static java.lang.String toString(org.apache.hadoop.hbase.io.hfile.CachedBlock, long);\n  public static java.lang.String toJSON(java.lang.String, java.util.NavigableSet<org.apache.hadoop.hbase.io.hfile.CachedBlock>) throws java.io.IOException;\n  public static java.lang.String toJSON(org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile) throws java.io.IOException;\n  public static java.lang.String toJSON(org.apache.hadoop.hbase.io.hfile.BlockCache) throws java.io.IOException;\n  public static java.lang.String toStringMinusFileName(org.apache.hadoop.hbase.io.hfile.CachedBlock, long);\n  public static org.apache.hadoop.hbase.io.hfile.BlockCacheUtil$CachedBlocksByFile getLoadedCachedBlocksByFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public static int validateBlockAddition(org.apache.hadoop.hbase.io.hfile.Cacheable, org.apache.hadoop.hbase.io.hfile.Cacheable, org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public static boolean shouldReplaceExistingCacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCache, org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/BlockCachesIterator.class;;;class org.apache.hadoop.hbase.io.hfile.BlockCachesIterator implements java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> {\n  public boolean hasNext();\n  public org.apache.hadoop.hbase.io.hfile.CachedBlock next();\n  public void remove();\n  public java.lang.Object next();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/BlockPriority.class;;;public final class org.apache.hadoop.hbase.io.hfile.BlockPriority extends java.lang.Enum<org.apache.hadoop.hbase.io.hfile.BlockPriority> {\n  public static final org.apache.hadoop.hbase.io.hfile.BlockPriority SINGLE;\n  public static final org.apache.hadoop.hbase.io.hfile.BlockPriority MULTI;\n  public static final org.apache.hadoop.hbase.io.hfile.BlockPriority MEMORY;\n  public static org.apache.hadoop.hbase.io.hfile.BlockPriority[] values();\n  public static org.apache.hadoop.hbase.io.hfile.BlockPriority valueOf(java.lang.String);\n}\n;;;No. This class does not contain any fields or methods that define the structure or content of a message that might be put on a message queue. It simply defines a set of constants and methods for working with those constants.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/BlockWithScanInfo.class;;;public class org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo {\n  public org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo(org.apache.hadoop.hbase.io.hfile.HFileBlock, org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock getHFileBlock();\n  public org.apache.hadoop.hbase.Cell getNextIndexedKey();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CacheConfig$1.class;;;class org.apache.hadoop.hbase.io.hfile.CacheConfig$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It may be an internal implementation class in the HBase library.;;;N
org/apache/hadoop/hbase/io/hfile/CacheConfig.class;;;public class org.apache.hadoop.hbase.io.hfile.CacheConfig {\n  public static final org.apache.hadoop.hbase.io.hfile.CacheConfig DISABLED;\n  public static final java.lang.String CACHE_DATA_ON_READ_KEY;\n  public static final java.lang.String CACHE_BLOCKS_ON_WRITE_KEY;\n  public static final java.lang.String CACHE_INDEX_BLOCKS_ON_WRITE_KEY;\n  public static final java.lang.String CACHE_BLOOM_BLOCKS_ON_WRITE_KEY;\n  public static final java.lang.String CACHE_DATA_BLOCKS_COMPRESSED_KEY;\n  public static final java.lang.String EVICT_BLOCKS_ON_CLOSE_KEY;\n  public static final java.lang.String PREFETCH_BLOCKS_ON_OPEN_KEY;\n  public static final java.lang.String CACHE_COMPACTED_BLOCKS_ON_WRITE_KEY;\n  public static final java.lang.String CACHE_COMPACTED_BLOCKS_ON_WRITE_THRESHOLD_KEY;\n  public static final java.lang.String DROP_BEHIND_CACHE_COMPACTION_KEY;\n  public static final boolean DEFAULT_CACHE_DATA_ON_READ;\n  public static final boolean DEFAULT_CACHE_DATA_ON_WRITE;\n  public static final boolean DEFAULT_IN_MEMORY;\n  public static final boolean DEFAULT_CACHE_INDEXES_ON_WRITE;\n  public static final boolean DEFAULT_CACHE_BLOOMS_ON_WRITE;\n  public static final boolean DEFAULT_EVICT_ON_CLOSE;\n  public static final boolean DEFAULT_CACHE_DATA_COMPRESSED;\n  public static final boolean DEFAULT_PREFETCH_ON_OPEN;\n  public static final boolean DEFAULT_CACHE_COMPACTED_BLOCKS_ON_WRITE;\n  public static final boolean DROP_BEHIND_CACHE_COMPACTION_DEFAULT;\n  public static final long DEFAULT_CACHE_COMPACTED_BLOCKS_ON_WRITE_THRESHOLD;\n  public org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, org.apache.hadoop.hbase.io.hfile.BlockCache, org.apache.hadoop.hbase.io.ByteBuffAllocator);\n  public org.apache.hadoop.hbase.io.hfile.CacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig);\n  public boolean shouldCacheDataOnRead();\n  public boolean shouldDropBehindCompaction();\n  public boolean shouldCacheBlockOnRead(org.apache.hadoop.hbase.io.hfile.BlockType$BlockCategory);\n  public boolean isInMemory();\n  public boolean shouldCacheDataOnWrite();\n  public void setCacheDataOnWrite(boolean);\n  public void enableCacheOnWrite();\n  public boolean shouldCacheIndexesOnWrite();\n  public boolean shouldCacheBloomsOnWrite();\n  public boolean shouldEvictOnClose();\n  public void setEvictOnClose(boolean);\n  public boolean shouldCacheDataCompressed();\n  public boolean shouldCacheCompressed(org.apache.hadoop.hbase.io.hfile.BlockType$BlockCategory);\n  public boolean shouldPrefetchOnOpen();\n  public boolean shouldCacheCompactedBlocksOnWrite();\n  public long getCacheCompactedBlocksOnWriteThreshold();\n  public boolean shouldReadBlockFromCache(org.apache.hadoop.hbase.io.hfile.BlockType);\n  public boolean shouldLockOnCacheMiss(org.apache.hadoop.hbase.io.hfile.BlockType);\n  public java.util.Optional<org.apache.hadoop.hbase.io.hfile.BlockCache> getBlockCache();\n  public boolean isCombinedBlockCache();\n  public org.apache.hadoop.hbase.io.ByteBuffAllocator getByteBuffAllocator();\n  public java.lang.String toString();\n}\n;;;No. This is not a message definition. It is a class definition that provides methods for managing the cache configuration for HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CacheStats$1.class;;;class org.apache.hadoop.hbase.io.hfile.CacheStats$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CacheStats.class;;;public class org.apache.hadoop.hbase.io.hfile.CacheStats {\n  public org.apache.hadoop.hbase.io.hfile.CacheStats(java.lang.String);\n  public org.apache.hadoop.hbase.io.hfile.CacheStats(java.lang.String, int);\n  public java.lang.String toString();\n  public void miss(boolean, boolean, org.apache.hadoop.hbase.io.hfile.BlockType);\n  public void hit(boolean, boolean, org.apache.hadoop.hbase.io.hfile.BlockType);\n  public void evict();\n  public void evicted(long, boolean);\n  public long failInsert();\n  public long getDataMissCount();\n  public long getLeafIndexMissCount();\n  public long getBloomChunkMissCount();\n  public long getMetaMissCount();\n  public long getRootIndexMissCount();\n  public long getIntermediateIndexMissCount();\n  public long getFileInfoMissCount();\n  public long getGeneralBloomMetaMissCount();\n  public long getDeleteFamilyBloomMissCount();\n  public long getTrailerMissCount();\n  public long getDataHitCount();\n  public long getLeafIndexHitCount();\n  public long getBloomChunkHitCount();\n  public long getMetaHitCount();\n  public long getRootIndexHitCount();\n  public long getIntermediateIndexHitCount();\n  public long getFileInfoHitCount();\n  public long getGeneralBloomMetaHitCount();\n  public long getDeleteFamilyBloomHitCount();\n  public long getTrailerHitCount();\n  public long getRequestCount();\n  public long getRequestCachingCount();\n  public long getMissCount();\n  public long getPrimaryMissCount();\n  public long getMissCachingCount();\n  public long getHitCount();\n  public long getPrimaryHitCount();\n  public long getHitCachingCount();\n  public long getEvictionCount();\n  public long getEvictedCount();\n  public long getPrimaryEvictedCount();\n  public double getHitRatio();\n  public double getHitCachingRatio();\n  public double getMissRatio();\n  public double getMissCachingRatio();\n  public double evictedPerEviction();\n  public long getFailedInserts();\n  public void rollMetricsPeriod();\n  public long getSumHitCountsPastNPeriods();\n  public long getSumRequestCountsPastNPeriods();\n  public long getSumHitCachingCountsPastNPeriods();\n  public long getSumRequestCachingCountsPastNPeriods();\n  public double getHitRatioPastNPeriods();\n  public double getHitCachingRatioPastNPeriods();\n  public org.apache.hadoop.hbase.io.hfile.AgeSnapshot getAgeAtEvictionSnapshot();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/Cacheable.class;;;public interface org.apache.hadoop.hbase.io.hfile.Cacheable extends org.apache.hadoop.hbase.io.HeapSize,org.apache.hadoop.hbase.nio.HBaseReferenceCounted {\n  public abstract int getSerializedLength();\n  public abstract void serialize(java.nio.ByteBuffer, boolean);\n  public abstract org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable> getDeserializer();\n  public abstract org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n  public default org.apache.hadoop.hbase.io.hfile.Cacheable retain();\n  public default int refCnt();\n  public default boolean release();\n  public default org.apache.hbase.thirdparty.io.netty.util.ReferenceCounted retain();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CacheableDeserializer.class;;;public interface org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<T extends org.apache.hadoop.hbase.io.hfile.Cacheable> {\n  public abstract T deserialize(org.apache.hadoop.hbase.nio.ByteBuff, org.apache.hadoop.hbase.io.ByteBuffAllocator) throws java.io.IOException;\n  public abstract int getDeserializerIdentifier();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/CacheableDeserializerIdManager.class;;;public class org.apache.hadoop.hbase.io.hfile.CacheableDeserializerIdManager {\n  public org.apache.hadoop.hbase.io.hfile.CacheableDeserializerIdManager();\n  public static int registerDeserializer(org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable>);\n  public static org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable> getDeserializer(int);\n  public static java.util.Map<java.lang.Integer, java.lang.String> save();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CachedBlock.class;;;public interface org.apache.hadoop.hbase.io.hfile.CachedBlock extends java.lang.Comparable<org.apache.hadoop.hbase.io.hfile.CachedBlock> {\n  public abstract org.apache.hadoop.hbase.io.hfile.BlockPriority getBlockPriority();\n  public abstract org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n  public abstract long getOffset();\n  public abstract long getSize();\n  public abstract long getCachedTime();\n  public abstract java.lang.String getFilename();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/ChecksumUtil.class;;;public class org.apache.hadoop.hbase.io.hfile.ChecksumUtil {\n  public static final org.slf4j.Logger LOG;\n  public static final int CHECKSUM_BUF_SIZE;\n  public org.apache.hadoop.hbase.io.hfile.ChecksumUtil();\n  public static void generateExceptionForChecksumFailureForTest(boolean);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CombinedBlockCache$CombinedCacheStats.class;;;public class org.apache.hadoop.hbase.io.hfile.CombinedBlockCache$CombinedCacheStats extends org.apache.hadoop.hbase.io.hfile.CacheStats {\n  public org.apache.hadoop.hbase.io.hfile.CacheStats getLruCacheStats();\n  public org.apache.hadoop.hbase.io.hfile.CacheStats getBucketCacheStats();\n  public long getDataMissCount();\n  public long getLeafIndexMissCount();\n  public long getBloomChunkMissCount();\n  public long getMetaMissCount();\n  public long getRootIndexMissCount();\n  public long getIntermediateIndexMissCount();\n  public long getFileInfoMissCount();\n  public long getGeneralBloomMetaMissCount();\n  public long getDeleteFamilyBloomMissCount();\n  public long getTrailerMissCount();\n  public long getDataHitCount();\n  public long getLeafIndexHitCount();\n  public long getBloomChunkHitCount();\n  public long getMetaHitCount();\n  public long getRootIndexHitCount();\n  public long getIntermediateIndexHitCount();\n  public long getFileInfoHitCount();\n  public long getGeneralBloomMetaHitCount();\n  public long getDeleteFamilyBloomHitCount();\n  public long getTrailerHitCount();\n  public long getRequestCount();\n  public long getRequestCachingCount();\n  public long getMissCount();\n  public long getPrimaryMissCount();\n  public long getMissCachingCount();\n  public long getHitCount();\n  public long getPrimaryHitCount();\n  public long getHitCachingCount();\n  public long getEvictionCount();\n  public long getEvictedCount();\n  public long getPrimaryEvictedCount();\n  public void rollMetricsPeriod();\n  public long getFailedInserts();\n  public long getSumHitCountsPastNPeriods();\n  public long getSumRequestCountsPastNPeriods();\n  public long getSumHitCachingCountsPastNPeriods();\n  public long getSumRequestCachingCountsPastNPeriods();\n}\n;;;No, this class is not a message definition that would typically be put on a message queue. It appears to be a Java class that defines a set of methods for accessing statistics related to a cache implementation.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CombinedBlockCache.class;;;public class org.apache.hadoop.hbase.io.hfile.CombinedBlockCache implements org.apache.hadoop.hbase.io.hfile.ResizableBlockCache,org.apache.hadoop.hbase.io.HeapSize {\n  public org.apache.hadoop.hbase.io.hfile.CombinedBlockCache(org.apache.hadoop.hbase.io.hfile.FirstLevelBlockCache, org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public long heapSize();\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean);\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable);\n  public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean);\n  public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.BlockType);\n  public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public int evictBlocksByHfileName(java.lang.String);\n  public org.apache.hadoop.hbase.io.hfile.CacheStats getStats();\n  public void shutdown();\n  public long size();\n  public long getMaxSize();\n  public long getCurrentDataSize();\n  public long getFreeSize();\n  public long getCurrentSize();\n  public long getBlockCount();\n  public long getDataBlockCount();\n  public java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator();\n  public org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches();\n  public void setMaxSize(long);\n  public int getRpcRefCount(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public org.apache.hadoop.hbase.io.hfile.FirstLevelBlockCache getFirstLevelCache();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/CompoundBloomFilter.class;;;public class org.apache.hadoop.hbase.io.hfile.CompoundBloomFilter extends org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterBase implements org.apache.hadoop.hbase.util.BloomFilter {\n  public org.apache.hadoop.hbase.io.hfile.CompoundBloomFilter(java.io.DataInput, org.apache.hadoop.hbase.io.hfile.HFile$Reader) throws java.io.IOException;\n  public boolean contains(byte[], int, int, org.apache.hadoop.hbase.nio.ByteBuff);\n  public boolean contains(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.nio.ByteBuff, org.apache.hadoop.hbase.regionserver.BloomType);\n  public boolean supportsAutoLoading();\n  public int getNumChunks();\n  public void enableTestingStats();\n  public java.lang.String formatTestingStats();\n  public long getNumQueriesForTesting(int);\n  public long getNumPositivesForTesting(int);\n  public java.lang.String toString();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a Java class that represents a compound bloom filter implementation and provides various methods to interact with it.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterBase.class;;;public class org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterBase implements org.apache.hadoop.hbase.util.BloomFilterBase {\n  public static final int VERSION;\n  public org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterBase();\n  public long getMaxKeys();\n  public long getKeyCount();\n  public long getByteSize();\n}\n;;;No, it is not a complete message definition that can be put on a message queue. It is a class definition for a bloom filter implementation in the Hadoop HBase library. It needs to be instantiated and populated with data before it can be used as a message on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a CompoundBloomFilterBase.;;;N
org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter$1.class;;;class org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterWriter$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter$MetaWriter.class;;;class org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterWriter$MetaWriter implements org.apache.hadoop.io.Writable {\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n  public void write(java.io.DataOutput) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter$ReadyChunk.class;;;class org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterWriter$ReadyChunk {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/CompoundBloomFilterWriter.class;;;public class org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterWriter extends org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterBase implements org.apache.hadoop.hbase.util.BloomFilterWriter,org.apache.hadoop.hbase.io.hfile.InlineBlockWriter {\n  public org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterWriter(int, float, int, int, boolean, org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.regionserver.BloomType);\n  public boolean shouldWriteBlock(boolean);\n  public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public void beforeShipped() throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell getPrevCell();\n  public void writeInlineBlock(java.io.DataOutput) throws java.io.IOException;\n  public void blockWritten(long, int, int);\n  public org.apache.hadoop.hbase.io.hfile.BlockType getInlineBlockType();\n  public void compactBloom();\n  public org.apache.hadoop.io.Writable getMetaWriter();\n  public org.apache.hadoop.io.Writable getDataWriter();\n  public boolean getCacheOnWrite();\n}\n;;;No, it is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/CorruptHFileException.class;;;public class org.apache.hadoop.hbase.io.hfile.CorruptHFileException extends org.apache.hadoop.hbase.DoNotRetryIOException {\n  public org.apache.hadoop.hbase.io.hfile.CorruptHFileException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.hbase.io.hfile.CorruptHFileException(java.lang.String);\n}\n;;;No. This is a Java class definition for an exception and is not intended to be placed on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/ExclusiveMemHFileBlock.class;;;public class org.apache.hadoop.hbase.io.hfile.ExclusiveMemHFileBlock extends org.apache.hadoop.hbase.io.hfile.HFileBlock {\n  public int refCnt();\n  public org.apache.hadoop.hbase.io.hfile.ExclusiveMemHFileBlock retain();\n  public boolean release();\n  public boolean isSharedMem();\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock retain();\n  public org.apache.hadoop.hbase.io.hfile.Cacheable retain();\n  public org.apache.hbase.thirdparty.io.netty.util.ReferenceCounted retain();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/FirstLevelBlockCache.class;;;public interface org.apache.hadoop.hbase.io.hfile.FirstLevelBlockCache extends org.apache.hadoop.hbase.io.hfile.ResizableBlockCache,org.apache.hadoop.hbase.io.HeapSize {\n  public abstract boolean containsBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public abstract void setVictimCache(org.apache.hadoop.hbase.io.hfile.BlockCache);\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/FixedFileTrailer.class;;;public class org.apache.hadoop.hbase.io.hfile.FixedFileTrailer {\n  public int getTrailerSize();\n  public java.lang.String toString();\n  public static org.apache.hadoop.hbase.io.hfile.FixedFileTrailer readFromStream(org.apache.hadoop.fs.FSDataInputStream, long) throws java.io.IOException;\n  public void expectMajorVersion(int);\n  public void expectMinorVersion(int);\n  public void expectAtLeastMajorVersion(int);\n  public long getFileInfoOffset();\n  public void setFileInfoOffset(long);\n  public long getLoadOnOpenDataOffset();\n  public void setLoadOnOpenOffset(long);\n  public int getDataIndexCount();\n  public void setDataIndexCount(int);\n  public int getMetaIndexCount();\n  public void setMetaIndexCount(int);\n  public long getTotalUncompressedBytes();\n  public void setTotalUncompressedBytes(long);\n  public long getEntryCount();\n  public void setEntryCount(long);\n  public org.apache.hadoop.hbase.io.compress.Compression$Algorithm getCompressionCodec();\n  public void setCompressionCodec(org.apache.hadoop.hbase.io.compress.Compression$Algorithm);\n  public int getNumDataIndexLevels();\n  public void setNumDataIndexLevels(int);\n  public long getLastDataBlockOffset();\n  public void setLastDataBlockOffset(long);\n  public long getFirstDataBlockOffset();\n  public void setFirstDataBlockOffset(long);\n  public java.lang.String getComparatorClassName();\n  public int getMajorVersion();\n  public int getMinorVersion();\n  public void setComparatorClass(java.lang.Class<? extends org.apache.hadoop.hbase.CellComparator>);\n  public long getUncompressedDataIndexSize();\n  public void setUncompressedDataIndexSize(long);\n  public byte[] getEncryptionKey();\n  public void setEncryptionKey(byte[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFile$CachingBlockReader.class;;;public interface org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader {\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileBlock readBlock(long, long, boolean, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.BlockType, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFile$Reader.class;;;public interface org.apache.hadoop.hbase.io.hfile.HFile$Reader extends java.io.Closeable,org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader {\n  public abstract java.lang.String getName();\n  public abstract org.apache.hadoop.hbase.CellComparator getComparator();\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(org.apache.hadoop.conf.Configuration, boolean, boolean, boolean);\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileBlock getMetaBlock(java.lang.String, boolean) throws java.io.IOException;\n  public abstract java.util.Optional<org.apache.hadoop.hbase.Cell> getLastKey();\n  public abstract java.util.Optional<org.apache.hadoop.hbase.Cell> midKey() throws java.io.IOException;\n  public abstract long length();\n  public abstract long getEntries();\n  public abstract java.util.Optional<org.apache.hadoop.hbase.Cell> getFirstKey();\n  public abstract long indexSize();\n  public abstract java.util.Optional<byte[]> getFirstRowKey();\n  public abstract java.util.Optional<byte[]> getLastRowKey();\n  public abstract org.apache.hadoop.hbase.io.hfile.FixedFileTrailer getTrailer();\n  public abstract void setDataBlockIndexReader(org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$CellBasedKeyBlockIndexReader);\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$CellBasedKeyBlockIndexReader getDataBlockIndexReader();\n  public abstract void setMetaBlockIndexReader(org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$ByteArrayKeyBlockIndexReader);\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$ByteArrayKeyBlockIndexReader getMetaBlockIndexReader();\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(org.apache.hadoop.conf.Configuration, boolean, boolean);\n  public abstract java.io.DataInput getGeneralBloomFilterMetadata() throws java.io.IOException;\n  public abstract java.io.DataInput getDeleteBloomFilterMetadata() throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.Path getPath();\n  public abstract void close(boolean) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getDataBlockEncoding();\n  public abstract boolean hasMVCCInfo();\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileContext getFileContext();\n  public abstract boolean isPrimaryReplicaReader();\n  public abstract org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveEncodingInCache(boolean);\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReader getUncachedBlockReader();\n  public abstract boolean prefetchComplete();\n  public abstract void unbufferStream();\n  public abstract org.apache.hadoop.hbase.io.hfile.ReaderContext getContext();\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileInfo getHFileInfo();\n  public abstract void setDataBlockEncoder(org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder);\n}\n;;;Yes, it could be a message definition for a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFile$Writer.class;;;public interface org.apache.hadoop.hbase.io.hfile.HFile$Writer extends java.io.Closeable,org.apache.hadoop.hbase.regionserver.CellSink,org.apache.hadoop.hbase.regionserver.ShipperListener {\n  public static final byte[] MAX_MEMSTORE_TS_KEY;\n  public abstract void appendFileInfo(byte[], byte[]) throws java.io.IOException;\n  public abstract org.apache.hadoop.fs.Path getPath();\n  public abstract void addInlineBlockWriter(org.apache.hadoop.hbase.io.hfile.InlineBlockWriter);\n  public abstract void appendMetaBlock(java.lang.String, org.apache.hadoop.io.Writable);\n  public abstract void addGeneralBloomFilter(org.apache.hadoop.hbase.util.BloomFilterWriter);\n  public abstract void addDeleteFamilyBloomFilter(org.apache.hadoop.hbase.util.BloomFilterWriter) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileContext getFileContext();\n}\n;;;yes;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFile$WriterFactory.class;;;public class org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory {\n  public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withPath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withOutputStream(org.apache.hadoop.fs.FSDataOutputStream);\n  public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withFavoredNodes(java.net.InetSocketAddress[]);\n  public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withFileContext(org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory withShouldDropCacheBehind(boolean);\n  public org.apache.hadoop.hbase.io.hfile.HFile$Writer create() throws java.io.IOException;\n}\n;;;No, this class is a factory class for creating HFile writers and does not represent a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFile.class;;;public final class org.apache.hadoop.hbase.io.hfile.HFile {\n  public static final int MAXIMUM_KEY_LENGTH;\n  public static final org.apache.hadoop.hbase.io.compress.Compression$Algorithm DEFAULT_COMPRESSION_ALGORITHM;\n  public static final int MIN_FORMAT_VERSION;\n  public static final int MAX_FORMAT_VERSION;\n  public static final int MIN_FORMAT_VERSION_WITH_TAGS;\n  public static final java.lang.String DEFAULT_COMPRESSION;\n  public static final java.lang.String BLOOM_FILTER_DATA_KEY;\n  public static final int MIN_NUM_HFILE_PATH_LEVELS;\n  public static final int DEFAULT_BYTES_PER_CHECKSUM;\n  public static final java.util.concurrent.atomic.LongAdder DATABLOCK_READ_COUNT;\n  public static final java.lang.String FORMAT_VERSION_KEY;\n  public static final long getAndResetChecksumFailuresCount();\n  public static final long getChecksumFailuresCount();\n  public static final void updateReadLatency(long, boolean);\n  public static final void updateWriteLatency(long);\n  public static int getFormatVersion(org.apache.hadoop.conf.Configuration);\n  public static final org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory getWriterFactoryNoCache(org.apache.hadoop.conf.Configuration);\n  public static final org.apache.hadoop.hbase.io.hfile.HFile$WriterFactory getWriterFactory(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig);\n  public static org.apache.hadoop.hbase.io.hfile.HFile$Reader createReader(org.apache.hadoop.hbase.io.hfile.ReaderContext, org.apache.hadoop.hbase.io.hfile.HFileInfo, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.io.hfile.HFile$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.io.hfile.HFile$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.CacheConfig, boolean, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static boolean isHFileFormat(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static boolean isHFileFormat(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus) throws java.io.IOException;\n  public static java.lang.String[] getSupportedCompressionAlgorithms();\n  public static java.util.List<org.apache.hadoop.fs.Path> getStoreFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void checkFormatVersion(int) throws java.lang.IllegalArgumentException;\n  public static void checkHFileVersion(org.apache.hadoop.conf.Configuration);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n;;;Yes, it is a message definition.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$1.class;;;class org.apache.hadoop.hbase.io.hfile.HFileBlock$1 {\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$BlockDeserializer.class;;;public final class org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockDeserializer implements org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable> {\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock deserialize(org.apache.hadoop.hbase.nio.ByteBuff, org.apache.hadoop.hbase.io.ByteBuffAllocator) throws java.io.IOException;\n  public int getDeserializerIdentifier();\n  public org.apache.hadoop.hbase.io.hfile.Cacheable deserialize(org.apache.hadoop.hbase.nio.ByteBuff, org.apache.hadoop.hbase.io.ByteBuffAllocator) throws java.io.IOException;\n}\n;;;No. It is a class definition for a deserializer used in HBase for reading data from the file system. It is not something that would typically be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$BlockIterator.class;;;interface org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockIterator {\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileBlock nextBlock() throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileBlock nextBlockWithBlockType(org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException;\n  public abstract void freeBlocks();\n}\n;;;No. This is an interface for a class in the HBase library, but it is not a standalone message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$BlockWritable.class;;;interface org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockWritable {\n  public abstract org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n  public abstract void writeToBlock(java.io.DataOutput) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$FSReader.class;;;interface org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReader {\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileBlock readBlockData(long, long, boolean, boolean, boolean) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockIterator blockRange(long, long);\n  public abstract void closeStreams() throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext getBlockDecodingContext();\n  public abstract org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext getDefaultBlockDecodingContext();\n  public abstract void setIncludesMemStoreTS(boolean);\n  public abstract void setDataBlockEncoder(org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder, org.apache.hadoop.conf.Configuration);\n  public abstract void unbufferStream();\n}\n;;;No, this class is not a message definition. It is an interface for a class that reads data from HBase HFiles.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$FSReaderImpl$1.class;;;class org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl$1 implements org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockIterator {\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock nextBlock() throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock nextBlockWithBlockType(org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException;\n  public void freeBlocks();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$FSReaderImpl.class;;;class org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReaderImpl implements org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReader {\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockIterator blockRange(long, long);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock readBlockData(long, long, boolean, boolean, boolean) throws java.io.IOException;\n  public void setIncludesMemStoreTS(boolean);\n  public void setDataBlockEncoder(org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext getBlockDecodingContext();\n  public org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext getDefaultBlockDecodingContext();\n  public void closeStreams() throws java.io.IOException;\n  public void unbufferStream();\n  public java.lang.String toString();\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class definition for a class in the Hadoop HBase library.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$Header.class;;;class org.apache.hadoop.hbase.io.hfile.HFileBlock$Header {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$PrefetchedHeader.class;;;class org.apache.hadoop.hbase.io.hfile.HFileBlock$PrefetchedHeader {\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$Writer$State.class;;;final class org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State extends java.lang.Enum<org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State> {\n  public static final org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State INIT;\n  public static final org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State WRITING;\n  public static final org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State BLOCK_READY;\n  public static org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State[] values();\n  public static org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer$State valueOf(java.lang.String);\n}\n;;;No. This is an enum definition and not a message definition. It defines different states that a class (HFileBlock$Writer) can be in, rather than representing any data that needs to be passed around.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock$Writer.class;;;class org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer implements org.apache.hadoop.hbase.regionserver.ShipperListener {\n  public void beforeShipped();\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder, org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder, org.apache.hadoop.hbase.io.hfile.HFileContext, org.apache.hadoop.hbase.io.ByteBuffAllocator);\n  public int encodedBlockSizeWritten();\n}\n;;;No, this is not a message definition. It is a class definition for a Java class with four methods.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlock.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileBlock implements org.apache.hadoop.hbase.io.hfile.Cacheable {\n  public static final long FIXED_OVERHEAD;\n  public static final boolean FILL_HEADER;\n  public static final boolean DONT_FILL_HEADER;\n  public static final int MULTI_BYTE_BUFFER_HEAP_SIZE;\n  public static final int BLOCK_METADATA_SPACE;\n  public static final org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable> BLOCK_DESERIALIZER;\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock(org.apache.hadoop.hbase.io.hfile.BlockType, int, int, long, org.apache.hadoop.hbase.nio.ByteBuff, boolean, long, int, int, org.apache.hadoop.hbase.io.hfile.HFileContext, org.apache.hadoop.hbase.io.ByteBuffAllocator);\n  public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n  public int refCnt();\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock retain();\n  public boolean release();\n  public int getOnDiskSizeWithHeader();\n  public org.apache.hadoop.hbase.nio.ByteBuff getBufferWithoutHeader();\n  public org.apache.hadoop.hbase.nio.ByteBuff getBufferWithoutHeader(boolean);\n  public org.apache.hadoop.hbase.nio.ByteBuff getBufferReadOnly();\n  public org.apache.hadoop.hbase.io.ByteBuffAllocator getByteBuffAllocator();\n  public java.lang.String toString();\n  public boolean isUnpacked();\n  public long heapSize();\n  public boolean isSharedMem();\n  public int getSerializedLength();\n  public void serialize(java.nio.ByteBuffer, boolean);\n  public java.nio.ByteBuffer getMetaData(java.nio.ByteBuffer);\n  public org.apache.hadoop.hbase.io.hfile.CacheableDeserializer<org.apache.hadoop.hbase.io.hfile.Cacheable> getDeserializer();\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public int headerSize();\n  public static int headerSize(boolean);\n  public org.apache.hadoop.hbase.io.hfile.HFileContext getHFileContext();\n  public org.apache.hadoop.hbase.io.hfile.Cacheable retain();\n  public org.apache.hbase.thirdparty.io.netty.util.ReferenceCounted retain();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlockBuilder.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder {\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder();\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withBlockType(org.apache.hadoop.hbase.io.hfile.BlockType);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withOnDiskSizeWithoutHeader(int);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withOnDiskDataSizeWithHeader(int);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withUncompressedSizeWithoutHeader(int);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withPrevBlockOffset(long);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withByteBuff(org.apache.hadoop.hbase.nio.ByteBuff);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withFillHeader(boolean);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withOffset(long);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withNextBlockOnDiskSize(int);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withHFileContext(org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withByteBuffAllocator(org.apache.hadoop.hbase.io.ByteBuffAllocator);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockBuilder withShared(boolean);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock build();\n}\n;;;No. This class is a builder pattern for building instances of the `HFileBlock` class, but it does not define any actual message or data structure that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlockIndex$BlockIndexChunk.class;;;class org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexChunk {\n  public void add(byte[], long, int);\n  public void clear();\n  public int getEntryBySubEntry(long);\n  public byte[] getMidKeyMetadata() throws java.io.IOException;\n  public int getNumEntries();\n  public byte[] getBlockKey(int);\n  public long getBlockOffset(int);\n  public int getOnDiskDataSize(int);\n  public long getCumulativeNumKV(int);\n}\n;;;No. This class seems to contain methods that operate on an instance of the class, rather than being a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlockIndex$BlockIndexReader.class;;;abstract class org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader implements org.apache.hadoop.hbase.io.HeapSize {\n  public abstract boolean isEmpty();\n  public void ensureNonEmpty();\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock seekToDataBlock(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.hfile.HFileBlock, boolean, boolean, boolean, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding, org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo loadDataBlockWithScanInfo(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.hfile.HFileBlock, boolean, boolean, boolean, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding, org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.Cell midkey(org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader) throws java.io.IOException;\n  public long getRootBlockOffset(int);\n  public int getRootBlockDataSize(int);\n  public int getRootBlockCount();\n  public abstract int rootBlockContainingKey(byte[], int, int, org.apache.hadoop.hbase.CellComparator);\n  public int rootBlockContainingKey(byte[], int, int);\n  public abstract int rootBlockContainingKey(org.apache.hadoop.hbase.Cell);\n  public void readRootIndex(java.io.DataInput, int) throws java.io.IOException;\n  public java.io.DataInputStream readRootIndex(org.apache.hadoop.hbase.io.hfile.HFileBlock, int) throws java.io.IOException;\n  public void readMultiLevelIndexRoot(org.apache.hadoop.hbase.io.hfile.HFileBlock, int) throws java.io.IOException;\n  public long heapSize();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlockIndex$BlockIndexWriter.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter implements org.apache.hadoop.hbase.io.hfile.InlineBlockWriter {\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter();\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexWriter(org.apache.hadoop.hbase.io.hfile.HFileBlock$Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, java.lang.String);\n  public void setMaxChunkSize(int);\n  public void setMinIndexNumEntries(int);\n  public long writeIndexBlocks(org.apache.hadoop.fs.FSDataOutputStream) throws java.io.IOException;\n  public void writeSingleLevelIndex(java.io.DataOutput, java.lang.String) throws java.io.IOException;\n  public final int getNumRootEntries();\n  public int getNumLevels();\n  public boolean shouldWriteBlock(boolean);\n  public void writeInlineBlock(java.io.DataOutput) throws java.io.IOException;\n  public void blockWritten(long, int, int);\n  public org.apache.hadoop.hbase.io.hfile.BlockType getInlineBlockType();\n  public void addEntry(byte[], long, int);\n  public void ensureSingleLevel() throws java.io.IOException;\n  public boolean getCacheOnWrite();\n  public long getTotalUncompressedSize();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlockIndex$ByteArrayKeyBlockIndexReader.class;;;class org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$ByteArrayKeyBlockIndexReader extends org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader {\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$ByteArrayKeyBlockIndexReader(int);\n  public boolean isEmpty();\n  public byte[] getRootBlockKey(int);\n  public org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo loadDataBlockWithScanInfo(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.hfile.HFileBlock, boolean, boolean, boolean, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding, org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell midkey(org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader) throws java.io.IOException;\n  public int rootBlockContainingKey(byte[], int, int, org.apache.hadoop.hbase.CellComparator);\n  public int rootBlockContainingKey(org.apache.hadoop.hbase.Cell);\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlockIndex$CellBasedKeyBlockIndexReader.class;;;class org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$CellBasedKeyBlockIndexReader extends org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$BlockIndexReader {\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$CellBasedKeyBlockIndexReader(org.apache.hadoop.hbase.CellComparator, int);\n  public boolean isEmpty();\n  public org.apache.hadoop.hbase.Cell getRootBlockKey(int);\n  public org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo loadDataBlockWithScanInfo(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.hfile.HFileBlock, boolean, boolean, boolean, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding, org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell midkey(org.apache.hadoop.hbase.io.hfile.HFile$CachingBlockReader) throws java.io.IOException;\n  public int rootBlockContainingKey(byte[], int, int, org.apache.hadoop.hbase.CellComparator);\n  public int rootBlockContainingKey(org.apache.hadoop.hbase.Cell);\n  public java.lang.String toString();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileBlockIndex {\n  public static final java.lang.String MAX_CHUNK_SIZE_KEY;\n  public static final java.lang.String MIN_INDEX_NUM_ENTRIES_KEY;\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex();\n  public static int getMaxChunkSize(org.apache.hadoop.conf.Configuration);\n  public static int getMinIndexNumEntries(org.apache.hadoop.conf.Configuration);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoder.class;;;public interface org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder {\n  public static final byte[] DATA_BLOCK_ENCODING;\n  public abstract void startBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException;\n  public abstract void encode(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException;\n  public abstract void endBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream, byte[], org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException;\n  public abstract boolean useEncodedScanner();\n  public abstract void saveMetadata(org.apache.hadoop.hbase.io.hfile.HFile$Writer) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getDataBlockEncoding();\n  public abstract org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveEncodingInCache(boolean);\n  public abstract org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext newDataBlockEncodingContext(org.apache.hadoop.conf.Configuration, byte[], org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public abstract org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext newDataBlockDecodingContext(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.HFileContext);\n}\n;;;No, this is an interface definition for a data block encoder in the Apache Hadoop HBase project. It cannot be directly put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileDataBlockEncoderImpl.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl implements org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder {\n  public org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl(org.apache.hadoop.hbase.io.encoding.DataBlockEncoding);\n  public static org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder createFromFileInfo(org.apache.hadoop.hbase.io.hfile.HFileInfo) throws java.io.IOException;\n  public void saveMetadata(org.apache.hadoop.hbase.io.hfile.HFile$Writer) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getDataBlockEncoding();\n  public boolean useEncodedScanner(boolean);\n  public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveEncodingInCache(boolean);\n  public void encode(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException;\n  public boolean useEncodedScanner();\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext newDataBlockEncodingContext(org.apache.hadoop.conf.Configuration, byte[], org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext newDataBlockDecodingContext(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public void startBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException;\n  public void endBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream, byte[], org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileInfo.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileInfo implements java.util.SortedMap<byte[], byte[]> {\n  public static final byte[] MAX_TAGS_LEN;\n  public org.apache.hadoop.hbase.io.hfile.HFileInfo();\n  public org.apache.hadoop.hbase.io.hfile.HFileInfo(org.apache.hadoop.hbase.io.hfile.ReaderContext, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.hfile.HFileInfo append(byte[], byte[], boolean) throws java.io.IOException;\n  public static boolean isReservedFileInfoKey(byte[]);\n  public void clear();\n  public java.util.Comparator<? super byte[]> comparator();\n  public boolean containsKey(java.lang.Object);\n  public boolean containsValue(java.lang.Object);\n  public java.util.Set<java.util.Map$Entry<byte[], byte[]>> entrySet();\n  public boolean equals(java.lang.Object);\n  public byte[] firstKey();\n  public byte[] get(java.lang.Object);\n  public int hashCode();\n  public java.util.SortedMap<byte[], byte[]> headMap(byte[]);\n  public boolean isEmpty();\n  public java.util.Set<byte[]> keySet();\n  public byte[] lastKey();\n  public byte[] put(byte[], byte[]);\n  public void putAll(java.util.Map<? extends byte[], ? extends byte[]>);\n  public byte[] remove(java.lang.Object);\n  public int size();\n  public java.util.SortedMap<byte[], byte[]> subMap(byte[], byte[]);\n  public java.util.SortedMap<byte[], byte[]> tailMap(byte[]);\n  public java.util.Collection<byte[]> values();\n  public void initTrailerAndContext(org.apache.hadoop.hbase.io.hfile.ReaderContext, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void initMetaAndIndex(org.apache.hadoop.hbase.io.hfile.HFile$Reader) throws java.io.IOException;\n  public void close();\n  public int getMajorVersion();\n  public void setTrailer(org.apache.hadoop.hbase.io.hfile.FixedFileTrailer);\n  public org.apache.hadoop.hbase.io.hfile.FixedFileTrailer getTrailer();\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$CellBasedKeyBlockIndexReader getDataBlockIndexReader();\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$ByteArrayKeyBlockIndexReader getMetaBlockIndexReader();\n  public org.apache.hadoop.hbase.io.hfile.HFileContext getHFileContext();\n  public java.util.List<org.apache.hadoop.hbase.io.hfile.HFileBlock> getLoadOnOpenBlocks();\n  public org.apache.hadoop.hbase.Cell getLastKeyCell();\n  public int getAvgKeyLen();\n  public int getAvgValueLen();\n  public boolean shouldIncludeMemStoreTS();\n  public boolean isDecodeMemstoreTS();\n  public java.lang.Object lastKey();\n  public java.lang.Object firstKey();\n  public java.util.SortedMap tailMap(java.lang.Object);\n  public java.util.SortedMap headMap(java.lang.Object);\n  public java.util.SortedMap subMap(java.lang.Object, java.lang.Object);\n  public java.lang.Object remove(java.lang.Object);\n  public java.lang.Object put(java.lang.Object, java.lang.Object);\n  public java.lang.Object get(java.lang.Object);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFilePreadReader$1.class;;;class org.apache.hadoop.hbase.io.hfile.HFilePreadReader$1 implements java.lang.Runnable {\n  public void run();\n}\n;;;No. This is a Java class definition and does not contain any message definition or data that can be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/io/hfile/HFilePreadReader.class;;;public class org.apache.hadoop.hbase.io.hfile.HFilePreadReader extends org.apache.hadoop.hbase.io.hfile.HFileReaderImpl {\n  public org.apache.hadoop.hbase.io.hfile.HFilePreadReader(org.apache.hadoop.hbase.io.hfile.ReaderContext, org.apache.hadoop.hbase.io.hfile.HFileInfo, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void close(boolean) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for reading HFiles in HBase.;;;N
org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter$1.class;;;class org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$1 {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter$KeyValueStatsCollector.class;;;class org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$KeyValueStatsCollector {\n  public void collect(org.apache.hadoop.hbase.Cell);\n  public void finish();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter$SimpleReporter$Builder.class;;;public class org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter$Builder {\n  public org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter$Builder outputTo(java.io.PrintStream);\n  public org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter$Builder filter(com.codahale.metrics.MetricFilter);\n  public org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter build();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter$SimpleReporter.class;;;class org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter extends com.codahale.metrics.ScheduledReporter {\n  public static org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter$SimpleReporter$Builder forRegistry(com.codahale.metrics.MetricRegistry);\n  public void report(java.util.SortedMap<java.lang.String, com.codahale.metrics.Gauge>, java.util.SortedMap<java.lang.String, com.codahale.metrics.Counter>, java.util.SortedMap<java.lang.String, com.codahale.metrics.Histogram>, java.util.SortedMap<java.lang.String, com.codahale.metrics.Meter>, java.util.SortedMap<java.lang.String, com.codahale.metrics.Timer>);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.class;;;public class org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter();\n  public org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter(org.apache.hadoop.conf.Configuration);\n  public void setPrintStreams(java.io.PrintStream, java.io.PrintStream);\n  public boolean parseOptions(java.lang.String[]) throws org.apache.hbase.thirdparty.org.apache.commons.cli.ParseException, java.io.IOException;\n  public int run(java.lang.String[]);\n  public int processFile(org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileReaderImpl$BlockIndexNotLoadedException.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileReaderImpl$BlockIndexNotLoadedException extends java.lang.IllegalStateException {\n  public org.apache.hadoop.hbase.io.hfile.HFileReaderImpl$BlockIndexNotLoadedException(org.apache.hadoop.fs.Path);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileReaderImpl$EncodedScanner.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileReaderImpl$EncodedScanner extends org.apache.hadoop.hbase.io.hfile.HFileReaderImpl$HFileScannerImpl {\n  public org.apache.hadoop.hbase.io.hfile.HFileReaderImpl$EncodedScanner(org.apache.hadoop.hbase.io.hfile.HFile$Reader, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.HFileContext, org.apache.hadoop.conf.Configuration);\n  public boolean isSeeked();\n  public void setNonSeekedState();\n  public boolean next() throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell getKey();\n  public java.nio.ByteBuffer getValue();\n  public org.apache.hadoop.hbase.Cell getCell();\n  public java.lang.String getKeyString();\n  public java.lang.String getValueString();\n  public int compareKey(org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.Cell);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileReaderImpl$HFileScannerImpl.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileReaderImpl$HFileScannerImpl implements org.apache.hadoop.hbase.io.hfile.HFileScanner {\n  public org.apache.hadoop.hbase.io.hfile.HFileReaderImpl$HFileScannerImpl(org.apache.hadoop.hbase.io.hfile.HFile$Reader, boolean, boolean, boolean);\n  public boolean isSeeked();\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.io.hfile.HFile$Reader getReader();\n  public void close();\n  public org.apache.hadoop.hbase.Cell getNextIndexedKey();\n  public int seekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public int reseekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public int seekTo(org.apache.hadoop.hbase.Cell, boolean) throws java.io.IOException;\n  public boolean seekBefore(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveDataBlockEncoding();\n  public org.apache.hadoop.hbase.Cell getCell();\n  public org.apache.hadoop.hbase.Cell getKey();\n  public java.nio.ByteBuffer getValue();\n  public boolean next() throws java.io.IOException;\n  public boolean seekTo() throws java.io.IOException;\n  public java.lang.String getKeyString();\n  public java.lang.String getValueString();\n  public int compareKey(org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.Cell);\n  public void shipped() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileReaderImpl$NotSeekedException.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileReaderImpl$NotSeekedException extends java.lang.IllegalStateException {\n  public org.apache.hadoop.hbase.io.hfile.HFileReaderImpl$NotSeekedException(org.apache.hadoop.fs.Path);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/HFileReaderImpl.class;;;public abstract class org.apache.hadoop.hbase.io.hfile.HFileReaderImpl implements org.apache.hadoop.hbase.io.hfile.HFile$Reader,org.apache.hadoop.conf.Configurable {\n  public static final int MINOR_VERSION_WITH_CHECKSUM;\n  public static final int MINOR_VERSION_NO_CHECKSUM;\n  public static final int PBUF_TRAILER_MINOR_VERSION;\n  public static final int KEY_VALUE_LEN_SIZE;\n  public org.apache.hadoop.hbase.io.hfile.HFileReaderImpl(org.apache.hadoop.hbase.io.hfile.ReaderContext, org.apache.hadoop.hbase.io.hfile.HFileInfo, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String toString();\n  public long length();\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> getFirstKey();\n  public java.util.Optional<byte[]> getFirstRowKey();\n  public java.util.Optional<byte[]> getLastRowKey();\n  public long getEntries();\n  public org.apache.hadoop.hbase.CellComparator getComparator();\n  public org.apache.hadoop.hbase.io.compress.Compression$Algorithm getCompressionAlgorithm();\n  public long indexSize();\n  public java.lang.String getName();\n  public void setDataBlockEncoder(org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder);\n  public void setDataBlockIndexReader(org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$CellBasedKeyBlockIndexReader);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$CellBasedKeyBlockIndexReader getDataBlockIndexReader();\n  public void setMetaBlockIndexReader(org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$ByteArrayKeyBlockIndexReader);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlockIndex$ByteArrayKeyBlockIndexReader getMetaBlockIndexReader();\n  public org.apache.hadoop.hbase.io.hfile.FixedFileTrailer getTrailer();\n  public org.apache.hadoop.hbase.io.hfile.ReaderContext getContext();\n  public org.apache.hadoop.hbase.io.hfile.HFileInfo getHFileInfo();\n  public boolean isPrimaryReplicaReader();\n  public org.apache.hadoop.fs.Path getPath();\n  public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getDataBlockEncoding();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock getMetaBlock(java.lang.String, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock readBlock(long, long, boolean, boolean, boolean, boolean, org.apache.hadoop.hbase.io.hfile.BlockType, org.apache.hadoop.hbase.io.encoding.DataBlockEncoding) throws java.io.IOException;\n  public boolean hasMVCCInfo();\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> getLastKey();\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> midKey() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveEncodingInCache(boolean);\n  public org.apache.hadoop.hbase.io.hfile.HFileBlock$FSReader getUncachedBlockReader();\n  public java.io.DataInput getGeneralBloomFilterMetadata() throws java.io.IOException;\n  public java.io.DataInput getDeleteBloomFilterMetadata() throws java.io.IOException;\n  public boolean isFileInfoLoaded();\n  public org.apache.hadoop.hbase.io.hfile.HFileContext getFileContext();\n  public boolean prefetchComplete();\n  public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(org.apache.hadoop.conf.Configuration, boolean, boolean);\n  public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(org.apache.hadoop.conf.Configuration, boolean, boolean, boolean);\n  public int getMajorVersion();\n  public void unbufferStream();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileScanner.class;;;public interface org.apache.hadoop.hbase.io.hfile.HFileScanner extends org.apache.hadoop.hbase.regionserver.Shipper,java.io.Closeable {\n  public abstract int seekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public abstract int reseekTo(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public abstract boolean seekBefore(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public abstract boolean seekTo() throws java.io.IOException;\n  public abstract boolean next() throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.Cell getKey();\n  public abstract java.nio.ByteBuffer getValue();\n  public abstract org.apache.hadoop.hbase.Cell getCell();\n  public abstract java.lang.String getKeyString();\n  public abstract java.lang.String getValueString();\n  public abstract org.apache.hadoop.hbase.io.hfile.HFile$Reader getReader();\n  public abstract boolean isSeeked();\n  public abstract org.apache.hadoop.hbase.Cell getNextIndexedKey();\n  public abstract void close();\n}\n;;;No. This is an interface for a scanner used in HBase file operations, but it is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface for a class in the Hadoop framework.;;;N
org/apache/hadoop/hbase/io/hfile/HFileStreamReader.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileStreamReader extends org.apache.hadoop.hbase.io.hfile.HFileReaderImpl {\n  public org.apache.hadoop.hbase.io.hfile.HFileStreamReader(org.apache.hadoop.hbase.io.hfile.ReaderContext, org.apache.hadoop.hbase.io.hfile.HFileInfo, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void close(boolean) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileUtil.class;;;class org.apache.hadoop.hbase.io.hfile.HFileUtil {\n  public static void seekOnMultipleSources(org.apache.hadoop.fs.FSDataInputStream, long) throws java.io.IOException;\n}\n;;;No. This is a static method definition of a utility class and not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileWriterImpl$1.class;;;class org.apache.hadoop.hbase.io.hfile.HFileWriterImpl$1 implements org.apache.hadoop.hbase.io.hfile.HFileBlock$BlockWritable {\n  public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n  public void writeToBlock(java.io.DataOutput) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/HFileWriterImpl.class;;;public class org.apache.hadoop.hbase.io.hfile.HFileWriterImpl implements org.apache.hadoop.hbase.io.hfile.HFile$Writer {\n  public static final java.lang.String UNIFIED_ENCODED_BLOCKSIZE_RATIO;\n  public static final byte[] KEY_VALUE_VERSION;\n  public static final int KEY_VALUE_VER_WITH_MEMSTORE;\n  public org.apache.hadoop.hbase.io.hfile.HFileWriterImpl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream, org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public void appendFileInfo(byte[], byte[]) throws java.io.IOException;\n  public long getPos() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getPath();\n  public java.lang.String toString();\n  public static org.apache.hadoop.hbase.io.compress.Compression$Algorithm compressionByName(java.lang.String);\n  public static org.apache.hadoop.hbase.Cell getMidpoint(org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public void appendMetaBlock(java.lang.String, org.apache.hadoop.io.Writable);\n  public void close() throws java.io.IOException;\n  public void addInlineBlockWriter(org.apache.hadoop.hbase.io.hfile.InlineBlockWriter);\n  public void addGeneralBloomFilter(org.apache.hadoop.hbase.util.BloomFilterWriter);\n  public void addDeleteFamilyBloomFilter(org.apache.hadoop.hbase.util.BloomFilterWriter);\n  public org.apache.hadoop.hbase.io.hfile.HFileContext getFileContext();\n  public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public void beforeShipped() throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell getLastCell();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/InclusiveCombinedBlockCache.class;;;public class org.apache.hadoop.hbase.io.hfile.InclusiveCombinedBlockCache extends org.apache.hadoop.hbase.io.hfile.CombinedBlockCache {\n  public org.apache.hadoop.hbase.io.hfile.InclusiveCombinedBlockCache(org.apache.hadoop.hbase.io.hfile.FirstLevelBlockCache, org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean);\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean);\n  public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n}\n;;;No, this is a Java class definition for the InclusiveCombinedBlockCache class in the HBase library. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/IndexOnlyLruBlockCache.class;;;public class org.apache.hadoop.hbase.io.hfile.IndexOnlyLruBlockCache extends org.apache.hadoop.hbase.io.hfile.LruBlockCache {\n  public org.apache.hadoop.hbase.io.hfile.IndexOnlyLruBlockCache(long, long, boolean, org.apache.hadoop.conf.Configuration);\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/InlineBlockWriter.class;;;public interface org.apache.hadoop.hbase.io.hfile.InlineBlockWriter {\n  public abstract boolean shouldWriteBlock(boolean);\n  public abstract void writeInlineBlock(java.io.DataOutput) throws java.io.IOException;\n  public abstract void blockWritten(long, int, int);\n  public abstract org.apache.hadoop.hbase.io.hfile.BlockType getInlineBlockType();\n  public abstract boolean getCacheOnWrite();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/InvalidHFileException.class;;;public class org.apache.hadoop.hbase.io.hfile.InvalidHFileException extends java.io.IOException {\n  public org.apache.hadoop.hbase.io.hfile.InvalidHFileException();\n  public org.apache.hadoop.hbase.io.hfile.InvalidHFileException(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache$1$1.class;;;class org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$1$1 implements org.apache.hadoop.hbase.io.hfile.CachedBlock {\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.io.hfile.BlockPriority getBlockPriority();\n  public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n  public long getOffset();\n  public long getSize();\n  public long getCachedTime();\n  public java.lang.String getFilename();\n  public int compareTo(org.apache.hadoop.hbase.io.hfile.CachedBlock);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public int compareTo(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache$1.class;;;class org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$1 implements java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> {\n  public boolean hasNext();\n  public org.apache.hadoop.hbase.io.hfile.CachedBlock next();\n  public void remove();\n  public java.lang.Object next();\n}\n;;;No. This is a class definition for an iterator and not a message definition that can be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache$2.class;;;class org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$2 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache$BlockBucket.class;;;class org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$BlockBucket implements java.lang.Comparable<org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$BlockBucket> {\n  public org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$BlockBucket(org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache, java.lang.String, long, long, long);\n  public void add(org.apache.hadoop.hbase.io.hfile.LruCachedBlock);\n  public long free(long);\n  public long overflow();\n  public long totalSize();\n  public int compareTo(org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$BlockBucket);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n}\n;;;No, this class is not a message definition. It is a class definition for a data structure used in the implementation of the LruAdaptiveBlockCache in the HBase system.;;;N;;;No;;;N
org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache$EvictionThread.class;;;class org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$EvictionThread extends java.lang.Thread {\n  public org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$EvictionThread(org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache);\n  public void run();\n  public void evict();\n}\n;;;No. This is a class that defines a thread used for eviction in the LruAdaptiveBlockCache. It is not a message definition that represents data being passed between different parts of a system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache$StatisticsThread.class;;;class org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$StatisticsThread extends java.lang.Thread {\n  public org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache$StatisticsThread(org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache);\n  public void run();\n}\n;;;No;;;N;;;No, it is not a task definition. It is a thread class definition, which represents a concurrent execution unit, but it is not specifically designed to be put on a task queue.;;;N
org/apache/hadoop/hbase/io/hfile/LruAdaptiveBlockCache.class;;;public class org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache implements org.apache.hadoop.hbase.io.hfile.FirstLevelBlockCache {\n  public static final long CACHE_FIXED_OVERHEAD;\n  public org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache(long, long);\n  public org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache(long, long, boolean);\n  public org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache(long, long, boolean, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache(long, long, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.io.hfile.LruAdaptiveBlockCache(long, long, boolean, int, float, int, float, float, float, float, float, float, boolean, long, int, long, float);\n  public void setVictimCache(org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public void setMaxSize(long);\n  public int getCacheDataBlockPercent();\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean);\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable);\n  public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean);\n  public boolean containsBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public int evictBlocksByHfileName(java.lang.String);\n  public java.lang.String toString();\n  public long getMaxSize();\n  public long getCurrentSize();\n  public long getCurrentDataSize();\n  public long getFreeSize();\n  public long size();\n  public long getBlockCount();\n  public long getDataBlockCount();\n  public void logStats();\n  public org.apache.hadoop.hbase.io.hfile.CacheStats getStats();\n  public long heapSize();\n  public java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator();\n  public void shutdown();\n  public void clearCache();\n  public java.util.Map<org.apache.hadoop.hbase.io.encoding.DataBlockEncoding, java.lang.Integer> getEncodingCountsForTest();\n  public org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches();\n}\n;;;Yes, this class might be a message definition that could be added to a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/LruBlockCache$1$1.class;;;class org.apache.hadoop.hbase.io.hfile.LruBlockCache$1$1 implements org.apache.hadoop.hbase.io.hfile.CachedBlock {\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.io.hfile.BlockPriority getBlockPriority();\n  public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n  public long getOffset();\n  public long getSize();\n  public long getCachedTime();\n  public java.lang.String getFilename();\n  public int compareTo(org.apache.hadoop.hbase.io.hfile.CachedBlock);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public int compareTo(java.lang.Object);\n}\n;;;No. This is a class definition, not a message definition. It is not related to any specific message or data format.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/io/hfile/LruBlockCache$1.class;;;class org.apache.hadoop.hbase.io.hfile.LruBlockCache$1 implements java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> {\n  public boolean hasNext();\n  public org.apache.hadoop.hbase.io.hfile.CachedBlock next();\n  public void remove();\n  public java.lang.Object next();\n}\n;;;No, this is not a message definition. It is a class implementing an Iterator interface for a block cache in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/LruBlockCache$2.class;;;class org.apache.hadoop.hbase.io.hfile.LruBlockCache$2 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/LruBlockCache$BlockBucket.class;;;class org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket implements java.lang.Comparable<org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket> {\n  public org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket(org.apache.hadoop.hbase.io.hfile.LruBlockCache, java.lang.String, long, long, long);\n  public void add(org.apache.hadoop.hbase.io.hfile.LruCachedBlock);\n  public long free(long);\n  public long overflow();\n  public long totalSize();\n  public int compareTo(org.apache.hadoop.hbase.io.hfile.LruBlockCache$BlockBucket);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n}\n;;;No. This class is an implementation detail of the LruBlockCache in the Apache HBase library and not a message definition for a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/LruBlockCache$EvictionThread.class;;;class org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread extends java.lang.Thread {\n  public org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread(org.apache.hadoop.hbase.io.hfile.LruBlockCache);\n  public void run();\n  public void evict();\n  public boolean isGo();\n}\n;;;No. This is a class definition for a thread in the HBase library, but it does not define a message that could be put on a message queue.;;;N;;;No, it is not a task definition. It is a class definition for a thread that is responsible for evicting elements from a cache.;;;N
org/apache/hadoop/hbase/io/hfile/LruBlockCache$StatisticsThread.class;;;class org.apache.hadoop.hbase.io.hfile.LruBlockCache$StatisticsThread extends java.lang.Thread {\n  public org.apache.hadoop.hbase.io.hfile.LruBlockCache$StatisticsThread(org.apache.hadoop.hbase.io.hfile.LruBlockCache);\n  public void run();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/LruBlockCache.class;;;public class org.apache.hadoop.hbase.io.hfile.LruBlockCache implements org.apache.hadoop.hbase.io.hfile.FirstLevelBlockCache {\n  public static final long CACHE_FIXED_OVERHEAD;\n  public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long);\n  public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, boolean);\n  public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, boolean, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.io.hfile.LruBlockCache(long, long, boolean, int, float, int, float, float, float, float, float, float, boolean, long);\n  public void setVictimCache(org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public void setMaxSize(long);\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean);\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable);\n  public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean);\n  public boolean containsBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public int evictBlocksByHfileName(java.lang.String);\n  public java.lang.String toString();\n  public long getMaxSize();\n  public long getCurrentSize();\n  public long getCurrentDataSize();\n  public long getFreeSize();\n  public long size();\n  public long getBlockCount();\n  public long getDataBlockCount();\n  public void logStats();\n  public org.apache.hadoop.hbase.io.hfile.CacheStats getStats();\n  public long heapSize();\n  public java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator();\n  public void shutdown();\n  public void clearCache();\n  public java.util.Map<org.apache.hadoop.hbase.io.encoding.DataBlockEncoding, java.lang.Integer> getEncodingCountsForTest();\n  public org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/LruCachedBlock.class;;;public class org.apache.hadoop.hbase.io.hfile.LruCachedBlock implements org.apache.hadoop.hbase.io.HeapSize, java.lang.Comparable<org.apache.hadoop.hbase.io.hfile.LruCachedBlock> {\n  public static final long PER_BLOCK_OVERHEAD;\n  public org.apache.hadoop.hbase.io.hfile.LruCachedBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, long);\n  public org.apache.hadoop.hbase.io.hfile.LruCachedBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, long, boolean);\n  public void access(long);\n  public long getCachedTime();\n  public long heapSize();\n  public int compareTo(org.apache.hadoop.hbase.io.hfile.LruCachedBlock);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public org.apache.hadoop.hbase.io.hfile.Cacheable getBuffer();\n  public org.apache.hadoop.hbase.io.hfile.BlockCacheKey getCacheKey();\n  public org.apache.hadoop.hbase.io.hfile.BlockPriority getPriority();\n  public int compareTo(java.lang.Object);\n}\n;;;No. This class represents a data structure for caching blocks in HBase and is not related to message passing or message queues.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/LruCachedBlockQueue.class;;;public class org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue implements org.apache.hadoop.hbase.io.HeapSize {\n  public org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue(long, long);\n  public void add(org.apache.hadoop.hbase.io.hfile.LruCachedBlock);\n  public org.apache.hadoop.hbase.io.hfile.LruCachedBlock poll();\n  public org.apache.hadoop.hbase.io.hfile.LruCachedBlock pollLast();\n  public long heapSize();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder$1.class;;;class org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder$1 {\n}\n;;;No.;;;N;;;No, it is not a task or a message definition. It seems to be a class definition for a Java class in the Hadoop HBase library.;;;N
org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder$NoneEncodingState.class;;;class org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder$NoneEncodingState extends org.apache.hadoop.hbase.io.encoding.EncodingState {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/NoOpDataBlockEncoder.class;;;public class org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder implements org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder {\n  public static final org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder INSTANCE;\n  public void encode(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException;\n  public boolean useEncodedScanner();\n  public void saveMetadata(org.apache.hadoop.hbase.io.hfile.HFile$Writer);\n  public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getDataBlockEncoding();\n  public org.apache.hadoop.hbase.io.encoding.DataBlockEncoding getEffectiveEncodingInCache(boolean);\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext newDataBlockEncodingContext(org.apache.hadoop.conf.Configuration, byte[], org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public org.apache.hadoop.hbase.io.encoding.HFileBlockDecodingContext newDataBlockDecodingContext(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public void startBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream) throws java.io.IOException;\n  public void endBlockEncoding(org.apache.hadoop.hbase.io.encoding.HFileBlockEncodingContext, java.io.DataOutputStream, byte[], org.apache.hadoop.hbase.io.hfile.BlockType) throws java.io.IOException;\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/PrefetchExecutor$1.class;;;final class org.apache.hadoop.hbase.io.hfile.PrefetchExecutor$1 implements java.util.concurrent.ThreadFactory {\n  public java.lang.Thread newThread(java.lang.Runnable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/PrefetchExecutor.class;;;public final class org.apache.hadoop.hbase.io.hfile.PrefetchExecutor {\n  public static void request(org.apache.hadoop.fs.Path, java.lang.Runnable);\n  public static void complete(org.apache.hadoop.fs.Path);\n  public static void cancel(org.apache.hadoop.fs.Path);\n  public static boolean isCompleted(org.apache.hadoop.fs.Path);\n}\n;;;No. \n\nThis is a class that provides static methods to request, complete, cancel, and check the status of prefetching of HBase data file blocks. It is not a message definition that is put on a message queue.;;;N;;;No, it is not a task definition. It provides methods for requesting, completing, cancelling, and checking the completion status of a file path, but it does not describe a specific task to be performed.;;;N
org/apache/hadoop/hbase/io/hfile/ReaderContext$ReaderType.class;;;public final class org.apache.hadoop.hbase.io.hfile.ReaderContext$ReaderType extends java.lang.Enum<org.apache.hadoop.hbase.io.hfile.ReaderContext$ReaderType> {\n  public static final org.apache.hadoop.hbase.io.hfile.ReaderContext$ReaderType PREAD;\n  public static final org.apache.hadoop.hbase.io.hfile.ReaderContext$ReaderType STREAM;\n  public static org.apache.hadoop.hbase.io.hfile.ReaderContext$ReaderType[] values();\n  public static org.apache.hadoop.hbase.io.hfile.ReaderContext$ReaderType valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/ReaderContext.class;;;public class org.apache.hadoop.hbase.io.hfile.ReaderContext {\n  public org.apache.hadoop.hbase.io.hfile.ReaderContext(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.fs.HFileSystem, boolean, org.apache.hadoop.hbase.io.hfile.ReaderContext$ReaderType);\n  public org.apache.hadoop.fs.Path getFilePath();\n  public org.apache.hadoop.hbase.io.FSDataInputStreamWrapper getInputStreamWrapper();\n  public long getFileSize();\n  public org.apache.hadoop.hbase.fs.HFileSystem getFileSystem();\n  public boolean isPrimaryReplicaReader();\n  public org.apache.hadoop.hbase.io.hfile.ReaderContext$ReaderType getReaderType();\n  public boolean isPreadAllBytes();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It appears to be a Java class definition for an object that may be used in a larger program or system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/ReaderContextBuilder.class;;;public class org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder {\n  public org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder();\n  public org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder withFilePath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder withFileSize(long);\n  public org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder withInputStreamWrapper(org.apache.hadoop.hbase.io.FSDataInputStreamWrapper);\n  public org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder withFileSystem(org.apache.hadoop.hbase.fs.HFileSystem);\n  public org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder withFileSystem(org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder withPrimaryReplicaReader(boolean);\n  public org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder withReaderType(org.apache.hadoop.hbase.io.hfile.ReaderContext$ReaderType);\n  public org.apache.hadoop.hbase.io.hfile.ReaderContextBuilder withFileSystemAndPath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.hfile.ReaderContext build();\n}\n;;;No. This class is a builder class used to construct ReaderContext objects, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/ResizableBlockCache.class;;;public interface org.apache.hadoop.hbase.io.hfile.ResizableBlockCache extends org.apache.hadoop.hbase.io.hfile.BlockCache {\n  public abstract void setMaxSize(long);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/io/hfile/SharedMemHFileBlock.class;;;public class org.apache.hadoop.hbase.io.hfile.SharedMemHFileBlock extends org.apache.hadoop.hbase.io.hfile.HFileBlock {\n  public boolean isSharedMem();\n}\n;;;No. This is a Java class definition and not a message definition. It cannot be put on a message queue unless it is serialized into a message format such as JSON or protobuf.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache$1.class;;;class org.apache.hadoop.hbase.io.hfile.TinyLfuBlockCache$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache$CachedBlockView.class;;;final class org.apache.hadoop.hbase.io.hfile.TinyLfuBlockCache$CachedBlockView implements org.apache.hadoop.hbase.io.hfile.CachedBlock {\n  public org.apache.hadoop.hbase.io.hfile.TinyLfuBlockCache$CachedBlockView(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, long);\n  public org.apache.hadoop.hbase.io.hfile.BlockPriority getBlockPriority();\n  public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n  public long getOffset();\n  public long getSize();\n  public long getCachedTime();\n  public java.lang.String getFilename();\n  public int compareTo(org.apache.hadoop.hbase.io.hfile.CachedBlock);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n}\n;;;No. This is not a message definition, but rather a class definition for a CachedBlockView in the TinyLfuBlockCache class of the Apache HBase library. It is not something that would typically be put on a message queue.;;;N;;;No, it is not a task definition but a message definition that might be put on a message queue.;;;N
org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache$EvictionListener.class;;;final class org.apache.hadoop.hbase.io.hfile.TinyLfuBlockCache$EvictionListener implements com.github.benmanes.caffeine.cache.RemovalListener<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable> {\n  public void onRemoval(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, com.github.benmanes.caffeine.cache.RemovalCause);\n  public void onRemoval(java.lang.Object, java.lang.Object, com.github.benmanes.caffeine.cache.RemovalCause);\n}\n;;;No, it is not a message definition. It is a Java class implementing the RemovalListener interface for caching purposes in Hadoop HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/TinyLfuBlockCache.class;;;public final class org.apache.hadoop.hbase.io.hfile.TinyLfuBlockCache implements org.apache.hadoop.hbase.io.hfile.FirstLevelBlockCache {\n  public org.apache.hadoop.hbase.io.hfile.TinyLfuBlockCache(long, long, java.util.concurrent.Executor, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.io.hfile.TinyLfuBlockCache(long, long, long, java.util.concurrent.Executor);\n  public void setVictimCache(org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public long size();\n  public long getFreeSize();\n  public long getCurrentSize();\n  public long getBlockCount();\n  public long heapSize();\n  public void setMaxSize(long);\n  public boolean containsBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean);\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean);\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable);\n  public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public int evictBlocksByHfileName(java.lang.String);\n  public org.apache.hadoop.hbase.io.hfile.CacheStats getStats();\n  public void shutdown();\n  public org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches();\n  public java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator();\n  public java.lang.String toString();\n  public long getMaxSize();\n  public long getCurrentDataSize();\n  public long getDataBlockCount();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator$1.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$1 implements java.util.Comparator<java.lang.Integer> {\n  public int compare(java.lang.Integer, java.lang.Integer);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator$Bucket.class;;;public final class org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket {\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket(long);\n  public boolean isUninstantiated();\n  public int sizeIndex();\n  public int getItemAllocationSize();\n  public boolean hasFreeSpace();\n  public boolean isCompletelyFree();\n  public int freeCount();\n  public int usedCount();\n  public int getFreeBytes();\n  public int getUsedBytes();\n  public long getBaseOffset();\n  public long allocate();\n  public void addAllocation(long) throws org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocatorException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator$BucketSizeInfo.class;;;final class org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo {\n  public synchronized void instantiateBucket(org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket);\n  public int sizeIndex();\n  public long allocateBlock();\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket findAndRemoveCompletelyFreeBucket();\n  public void freeBlock(org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket, long);\n  public synchronized org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics statistics();\n  public java.lang.String toString();\n}\n;;;No, it is not a message definition. It is a class that defines methods for allocating and managing buckets in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator$IndexStatistics.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics {\n  public long freeCount();\n  public long usedCount();\n  public long totalCount();\n  public long freeBytes();\n  public long usedBytes();\n  public long totalBytes();\n  public long itemSize();\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics(long, long, long);\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$IndexStatistics();\n  public void setTo(long, long, long);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocator.class;;;public final class org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator {\n  public static final int FEWEST_ITEMS_IN_BUCKET;\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$BucketSizeInfo roundUpToBucketSizeInfo(int);\n  public java.lang.String toString();\n  public long getUsedSize();\n  public long getFreeSize();\n  public long getTotalSize();\n  public synchronized long allocateBlock(int) throws org.apache.hadoop.hbase.io.hfile.bucket.CacheFullException, org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocatorException;\n  public synchronized int freeBlock(long);\n  public int sizeIndexOfAllocation(long);\n  public int sizeOfAllocation(long);\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator$Bucket[] getBuckets();\n  public long freeBlock(long[]);\n  public int getBucketIndex(long);\n  public java.util.Set<java.lang.Integer> getLeastFilledBuckets(java.util.Set<java.lang.Integer>, int);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocatorException.class;;;public class org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocatorException extends java.io.IOException {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache$1.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$1 extends java.io.FileInputStream {\n  public void close() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache$2$1.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2$1 implements org.apache.hadoop.hbase.io.hfile.CachedBlock {\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.io.hfile.BlockPriority getBlockPriority();\n  public org.apache.hadoop.hbase.io.hfile.BlockType getBlockType();\n  public long getOffset();\n  public long getSize();\n  public long getCachedTime();\n  public java.lang.String getFilename();\n  public int compareTo(org.apache.hadoop.hbase.io.hfile.CachedBlock);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public int compareTo(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache$2.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$2 implements java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> {\n  public boolean hasNext();\n  public org.apache.hadoop.hbase.io.hfile.CachedBlock next();\n  public void remove();\n  public java.lang.Object next();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache$3.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$3 {\n}\n;;;no;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache$BucketEntryGroup.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup {\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$BucketEntryGroup(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache, long, long, long);\n  public void add(java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry>);\n  public long free(long);\n  public long overflow();\n  public long totalSize();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache$RAMCache.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMCache {\n  public boolean containsKey(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry get(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry putIfAbsent(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry);\n  public boolean remove(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public boolean remove(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, java.util.function.Consumer<org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry>);\n  public boolean isEmpty();\n  public void clear();\n}\n;;;No. It is a class definition for the RAMCache class in the org.apache.hadoop.hbase.io.hfile.bucket.BucketCache package. It does not contain any message-related properties or methods, and as such, would not typically be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache$RAMQueueEntry.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$RAMQueueEntry {\n  public org.apache.hadoop.hbase.io.hfile.Cacheable getData();\n  public org.apache.hadoop.hbase.io.hfile.BlockCacheKey getKey();\n  public void access(long);\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry writeToCache(org.apache.hadoop.hbase.io.hfile.bucket.IOEngine, org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator, java.util.concurrent.atomic.LongAdder, java.util.function.Function<org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry, org.apache.hadoop.hbase.io.ByteBuffAllocator$Recycler>, java.nio.ByteBuffer) throws java.io.IOException;\n}\n;;;No, it is a class definition of an object, but it does not specify any message to be sent or received on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache$StatisticsThread.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$StatisticsThread extends java.lang.Thread {\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$StatisticsThread(org.apache.hadoop.hbase.io.hfile.bucket.BucketCache);\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache$WriterThread.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache$WriterThread extends java.lang.Thread {\n  public void run();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue, as it extends a Thread class and does not implement any interface or extend any class related to task definition.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.class;;;public class org.apache.hadoop.hbase.io.hfile.bucket.BucketCache implements org.apache.hadoop.hbase.io.hfile.BlockCache,org.apache.hadoop.hbase.io.HeapSize {\n  public static final int DEFAULT_ERROR_TOLERATION_DURATION;\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache(java.lang.String, long, int, int[], int, int, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketCache(java.lang.String, long, int, int[], int, int, java.lang.String, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public long getMaxSize();\n  public java.lang.String getIoEngine();\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable);\n  public void cacheBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean);\n  public void cacheBlockWithWait(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.Cacheable, boolean, boolean);\n  public org.apache.hadoop.hbase.io.hfile.Cacheable getBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey, boolean, boolean, boolean);\n  public boolean evictBlock(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public boolean evictBlockIfNoRpcReferenced(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n  public void logStats();\n  public long getRealCacheSize();\n  public long acceptableSize();\n  public void shutdown();\n  public org.apache.hadoop.hbase.io.hfile.CacheStats getStats();\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator getAllocator();\n  public long heapSize();\n  public long size();\n  public long getCurrentDataSize();\n  public long getFreeSize();\n  public long getBlockCount();\n  public long getDataBlockCount();\n  public long getCurrentSize();\n  public int evictBlocksByHfileName(java.lang.String);\n  public java.util.Iterator<org.apache.hadoop.hbase.io.hfile.CachedBlock> iterator();\n  public org.apache.hadoop.hbase.io.hfile.BlockCache[] getBlockCaches();\n  public int getRpcRefCount(org.apache.hadoop.hbase.io.hfile.BlockCacheKey);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketCacheStats.class;;;public class org.apache.hadoop.hbase.io.hfile.bucket.BucketCacheStats extends org.apache.hadoop.hbase.io.hfile.CacheStats {\n  public java.lang.String toString();\n  public void ioHit(long);\n  public long getIOHitsPerSecond();\n  public double getIOTimePerHit();\n  public void reset();\n  public long getAllocationFailCount();\n  public void allocationFailed();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketEntry$BucketEntryHandler.class;;;interface org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry$BucketEntryHandler<T> {\n  public abstract T handle();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue as it does not represent a specific task that needs to be performed. Instead, it is an interface with a method signature that defines a way to handle a BucketEntry of a certain generic type T.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketEntry.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry implements org.apache.hadoop.hbase.nio.HBaseReferenceCounted {\n  public int getLength();\n  public org.apache.hadoop.hbase.io.hfile.BlockPriority getPriority();\n  public int refCnt();\n  public org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry retain();\n  public boolean release();\n  public org.apache.hbase.thirdparty.io.netty.util.ReferenceCounted retain();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketProtoUtils$1.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.BucketProtoUtils$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/BucketProtoUtils.class;;;final class org.apache.hadoop.hbase.io.hfile.bucket.BucketProtoUtils {\n}\n;;;No. This is a utility class in HBase for working with bucket files and is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a utility class in the HBase library.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/ByteBufferIOEngine.class;;;public class org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine implements org.apache.hadoop.hbase.io.hfile.bucket.IOEngine {\n  public org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine(long) throws java.io.IOException;\n  public java.lang.String toString();\n  public boolean isPersistent();\n  public boolean usesSharedMemory();\n  public org.apache.hadoop.hbase.io.hfile.Cacheable read(org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry) throws java.io.IOException;\n  public void write(java.nio.ByteBuffer, long) throws java.io.IOException;\n  public void write(org.apache.hadoop.hbase.nio.ByteBuff, long) throws java.io.IOException;\n  public void sync();\n  public void shutdown();\n}\n;;;Yes, it is a message definition for a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/bucket/CacheFullException.class;;;public class org.apache.hadoop.hbase.io.hfile.bucket.CacheFullException extends java.io.IOException {\n  public int bucketIndex();\n  public int requestedSize();\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/bucket/CachedEntryQueue.class;;;public class org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue {\n  public org.apache.hadoop.hbase.io.hfile.bucket.CachedEntryQueue(long, long);\n  public void add(java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry>);\n  public java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry> poll();\n  public java.util.Map$Entry<org.apache.hadoop.hbase.io.hfile.BlockCacheKey, org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry> pollLast();\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition that defines a container for cached entries in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/ExclusiveMemoryMmapIOEngine.class;;;public class org.apache.hadoop.hbase.io.hfile.bucket.ExclusiveMemoryMmapIOEngine extends org.apache.hadoop.hbase.io.hfile.bucket.FileMmapIOEngine {\n  public org.apache.hadoop.hbase.io.hfile.bucket.ExclusiveMemoryMmapIOEngine(java.lang.String, long) throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.hfile.Cacheable read(org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine$1.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine$FileAccessor.class;;;interface org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine$FileAccessor {\n  public abstract int access(java.nio.channels.FileChannel, org.apache.hadoop.hbase.nio.ByteBuff, long) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine$FileReadAccessor.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine$FileReadAccessor implements org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine$FileAccessor {\n  public int access(java.nio.channels.FileChannel, org.apache.hadoop.hbase.nio.ByteBuff, long) throws java.io.IOException;\n}\n;;;No. The given class is an implementation of a file accessor interface in HBase and does not represent a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine$FileWriteAccessor.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine$FileWriteAccessor implements org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine$FileAccessor {\n  public int access(java.nio.channels.FileChannel, org.apache.hadoop.hbase.nio.ByteBuff, long) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/bucket/FileIOEngine.class;;;public class org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine extends org.apache.hadoop.hbase.io.hfile.bucket.PersistentIOEngine {\n  public static final java.lang.String FILE_DELIMITER;\n  public org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine(long, boolean, java.lang.String...) throws java.io.IOException;\n  public java.lang.String toString();\n  public boolean isPersistent();\n  public org.apache.hadoop.hbase.io.hfile.Cacheable read(org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry) throws java.io.IOException;\n  public void write(java.nio.ByteBuffer, long) throws java.io.IOException;\n  public void sync() throws java.io.IOException;\n  public void shutdown();\n  public void write(org.apache.hadoop.hbase.nio.ByteBuff, long) throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/io/hfile/bucket/FileMmapIOEngine$1.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.FileMmapIOEngine$1 implements org.apache.hadoop.hbase.util.ByteBufferAllocator {\n  public java.nio.ByteBuffer allocate(long) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/FileMmapIOEngine.class;;;public abstract class org.apache.hadoop.hbase.io.hfile.bucket.FileMmapIOEngine extends org.apache.hadoop.hbase.io.hfile.bucket.PersistentIOEngine {\n  public org.apache.hadoop.hbase.io.hfile.bucket.FileMmapIOEngine(java.lang.String, long) throws java.io.IOException;\n  public java.lang.String toString();\n  public boolean isPersistent();\n  public abstract org.apache.hadoop.hbase.io.hfile.Cacheable read(org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry) throws java.io.IOException;\n  public void write(java.nio.ByteBuffer, long) throws java.io.IOException;\n  public void write(org.apache.hadoop.hbase.nio.ByteBuff, long) throws java.io.IOException;\n  public void sync() throws java.io.IOException;\n  public void shutdown();\n}\n;;;No, the class is not a message definition that might be put on a message queue. It is a Java class representing an input/output engine for HBase file operations.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/IOEngine.class;;;public interface org.apache.hadoop.hbase.io.hfile.bucket.IOEngine {\n  public abstract boolean isPersistent();\n  public default boolean usesSharedMemory();\n  public abstract org.apache.hadoop.hbase.io.hfile.Cacheable read(org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry) throws java.io.IOException;\n  public abstract void write(java.nio.ByteBuffer, long) throws java.io.IOException;\n  public abstract void write(org.apache.hadoop.hbase.nio.ByteBuff, long) throws java.io.IOException;\n  public abstract void sync() throws java.io.IOException;\n  public abstract void shutdown();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/PersistentIOEngine$DuFileCommand.class;;;class org.apache.hadoop.hbase.io.hfile.bucket.PersistentIOEngine$DuFileCommand extends org.apache.hadoop.util.Shell$ShellCommandExecutor {\n  public java.lang.String[] getExecString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/PersistentIOEngine.class;;;public abstract class org.apache.hadoop.hbase.io.hfile.bucket.PersistentIOEngine implements org.apache.hadoop.hbase.io.hfile.bucket.IOEngine {\n  public org.apache.hadoop.hbase.io.hfile.bucket.PersistentIOEngine(java.lang.String...);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/io/hfile/bucket/SharedMemoryMmapIOEngine.class;;;public class org.apache.hadoop.hbase.io.hfile.bucket.SharedMemoryMmapIOEngine extends org.apache.hadoop.hbase.io.hfile.bucket.FileMmapIOEngine {\n  public org.apache.hadoop.hbase.io.hfile.bucket.SharedMemoryMmapIOEngine(java.lang.String, long) throws java.io.IOException;\n  public boolean usesSharedMemory();\n  public org.apache.hadoop.hbase.io.hfile.Cacheable read(org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/io/util/MemorySizeUtil.class;;;public class org.apache.hadoop.hbase.io.util.MemorySizeUtil {\n  public static final java.lang.String MEMSTORE_SIZE_KEY;\n  public static final java.lang.String MEMSTORE_SIZE_OLD_KEY;\n  public static final java.lang.String MEMSTORE_SIZE_LOWER_LIMIT_KEY;\n  public static final java.lang.String MEMSTORE_SIZE_LOWER_LIMIT_OLD_KEY;\n  public static final java.lang.String OFFHEAP_MEMSTORE_SIZE_KEY;\n  public static final float DEFAULT_MEMSTORE_SIZE;\n  public static final float DEFAULT_MEMSTORE_SIZE_LOWER_LIMIT;\n  public org.apache.hadoop.hbase.io.util.MemorySizeUtil();\n  public static java.lang.management.MemoryUsage safeGetHeapMemoryUsage();\n  public static void checkForClusterFreeHeapMemoryLimit(org.apache.hadoop.conf.Configuration);\n  public static float getGlobalMemStoreHeapPercent(org.apache.hadoop.conf.Configuration, boolean);\n  public static float getGlobalMemStoreHeapLowerMark(org.apache.hadoop.conf.Configuration, boolean);\n  public static org.apache.hadoop.hbase.util.Pair<java.lang.Long, java.lang.management.MemoryType> getGlobalMemStoreSize(org.apache.hadoop.conf.Configuration);\n  public static long getOnheapGlobalMemStoreSize(org.apache.hadoop.conf.Configuration);\n  public static float getBlockCacheHeapPercent(org.apache.hadoop.conf.Configuration);\n  public static long getOnHeapCacheSize(org.apache.hadoop.conf.Configuration);\n  public static long getBucketCacheSize(org.apache.hadoop.conf.Configuration);\n}\n;;;Yes, it is a message definition.;;;Y;;;;;;N
org/apache/hadoop/hbase/ipc/AdaptiveLifoCoDelCallQueue.class;;;public class org.apache.hadoop.hbase.ipc.AdaptiveLifoCoDelCallQueue implements java.util.concurrent.BlockingQueue<org.apache.hadoop.hbase.ipc.CallRunner> {\n  public org.apache.hadoop.hbase.ipc.AdaptiveLifoCoDelCallQueue(int, int, int, double, java.util.concurrent.atomic.LongAdder, java.util.concurrent.atomic.LongAdder);\n  public void updateTunables(int, int, double);\n  public org.apache.hadoop.hbase.ipc.CallRunner take() throws java.lang.InterruptedException;\n  public org.apache.hadoop.hbase.ipc.CallRunner poll();\n  public boolean offer(org.apache.hadoop.hbase.ipc.CallRunner);\n  public int size();\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.ipc.CallRunner poll(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public org.apache.hadoop.hbase.ipc.CallRunner peek();\n  public boolean remove(java.lang.Object);\n  public boolean contains(java.lang.Object);\n  public java.lang.Object[] toArray();\n  public <T> T[] toArray(T[]);\n  public void clear();\n  public int drainTo(java.util.Collection<? super org.apache.hadoop.hbase.ipc.CallRunner>);\n  public int drainTo(java.util.Collection<? super org.apache.hadoop.hbase.ipc.CallRunner>, int);\n  public java.util.Iterator<org.apache.hadoop.hbase.ipc.CallRunner> iterator();\n  public boolean add(org.apache.hadoop.hbase.ipc.CallRunner);\n  public org.apache.hadoop.hbase.ipc.CallRunner remove();\n  public org.apache.hadoop.hbase.ipc.CallRunner element();\n  public boolean addAll(java.util.Collection<? extends org.apache.hadoop.hbase.ipc.CallRunner>);\n  public boolean isEmpty();\n  public boolean containsAll(java.util.Collection<?>);\n  public boolean removeAll(java.util.Collection<?>);\n  public boolean retainAll(java.util.Collection<?>);\n  public int remainingCapacity();\n  public void put(org.apache.hadoop.hbase.ipc.CallRunner) throws java.lang.InterruptedException;\n  public boolean offer(org.apache.hadoop.hbase.ipc.CallRunner, long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public java.lang.Object poll(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public java.lang.Object take() throws java.lang.InterruptedException;\n  public boolean offer(java.lang.Object, long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public void put(java.lang.Object) throws java.lang.InterruptedException;\n  public boolean offer(java.lang.Object);\n  public boolean add(java.lang.Object);\n  public java.lang.Object peek();\n  public java.lang.Object element();\n  public java.lang.Object poll();\n  public java.lang.Object remove();\n}\n;;;Yes, this class is a message definition that might be put on a message queue. It implements the BlockingQueue interface which is commonly used for message queues in Java.;;;Y;;;;;;N
org/apache/hadoop/hbase/ipc/AnnotationReadingPriorityFunction.class;;;public abstract class org.apache.hadoop.hbase.ipc.AnnotationReadingPriorityFunction<T extends org.apache.hadoop.hbase.HBaseRpcServicesBase<?>> implements org.apache.hadoop.hbase.ipc.PriorityFunction {\n  public org.apache.hadoop.hbase.ipc.AnnotationReadingPriorityFunction(T);\n  public int getPriority(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.security.User);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/ipc/BalancedQueueRpcExecutor.class;;;public class org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor extends org.apache.hadoop.hbase.ipc.RpcExecutor {\n  public org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n  public org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor(java.lang.String, int, java.lang.String, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n  public boolean dispatch(org.apache.hadoop.hbase.ipc.CallRunner);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/ipc/BufferChain.class;;;class org.apache.hadoop.hbase.ipc.BufferChain {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/CallQueueInfo.class;;;public class org.apache.hadoop.hbase.ipc.CallQueueInfo {\n  public java.util.Set<java.lang.String> getCallQueueNames();\n  public java.util.Set<java.lang.String> getCalledMethodNames(java.lang.String);\n  public long getCallMethodCount(java.lang.String, java.lang.String);\n  public long getCallMethodSize(java.lang.String, java.lang.String);\n}\n;;;No. This class provides information about the call queue, but it is not a message definition. It does not define the content of a message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/CallRunner.class;;;public class org.apache.hadoop.hbase.ipc.CallRunner {\n  public org.apache.hadoop.hbase.ipc.RpcCall getRpcCall();\n  public void setStatus(org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler);\n  public void run();\n  public void drop();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/ipc/EmptyServiceNameException.class;;;public class org.apache.hadoop.hbase.ipc.EmptyServiceNameException extends org.apache.hadoop.hbase.ipc.FatalConnectionException {\n  public org.apache.hadoop.hbase.ipc.EmptyServiceNameException();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/ipc/FastPathBalancedQueueRpcExecutor.class;;;public class org.apache.hadoop.hbase.ipc.FastPathBalancedQueueRpcExecutor extends org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor {\n  public org.apache.hadoop.hbase.ipc.FastPathBalancedQueueRpcExecutor(java.lang.String, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n  public org.apache.hadoop.hbase.ipc.FastPathBalancedQueueRpcExecutor(java.lang.String, int, java.lang.String, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n  public boolean dispatch(org.apache.hadoop.hbase.ipc.CallRunner);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/FastPathRWQueueRpcExecutor.class;;;public class org.apache.hadoop.hbase.ipc.FastPathRWQueueRpcExecutor extends org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor {\n  public org.apache.hadoop.hbase.ipc.FastPathRWQueueRpcExecutor(java.lang.String, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n  public boolean dispatch(org.apache.hadoop.hbase.ipc.CallRunner);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/ipc/FastPathRpcHandler.class;;;public class org.apache.hadoop.hbase.ipc.FastPathRpcHandler extends org.apache.hadoop.hbase.ipc.RpcHandler {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/FifoRpcScheduler$1.class;;;class org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1 extends org.apache.hadoop.hbase.ipc.FifoRpcScheduler$FifoCallRunner {\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/FifoRpcScheduler$FifoCallRunner.class;;;class org.apache.hadoop.hbase.ipc.FifoRpcScheduler$FifoCallRunner implements java.lang.Runnable {\n  public void run();\n}\n;;;No, it is not a message definition. It is a class definition for a runnable object.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/ipc/FifoRpcScheduler.class;;;public class org.apache.hadoop.hbase.ipc.FifoRpcScheduler extends org.apache.hadoop.hbase.ipc.RpcScheduler {\n  public org.apache.hadoop.hbase.ipc.FifoRpcScheduler(org.apache.hadoop.conf.Configuration, int);\n  public void init(org.apache.hadoop.hbase.ipc.RpcScheduler$Context);\n  public void start();\n  public void stop();\n  public boolean dispatch(org.apache.hadoop.hbase.ipc.CallRunner);\n  public int getGeneralQueueLength();\n  public int getPriorityQueueLength();\n  public int getReplicationQueueLength();\n  public int getActiveRpcHandlerCount();\n  public int getActiveGeneralRpcHandlerCount();\n  public int getActivePriorityRpcHandlerCount();\n  public int getActiveReplicationRpcHandlerCount();\n  public int getActiveMetaPriorityRpcHandlerCount();\n  public long getNumGeneralCallsDropped();\n  public long getNumLifoModeSwitches();\n  public int getWriteQueueLength();\n  public int getReadQueueLength();\n  public int getScanQueueLength();\n  public int getActiveWriteRpcHandlerCount();\n  public int getActiveReadRpcHandlerCount();\n  public int getActiveScanRpcHandlerCount();\n  public int getMetaPriorityQueueLength();\n  public org.apache.hadoop.hbase.ipc.CallQueueInfo getCallQueueInfo();\n}\n;;;No. This class is an implementation of the RpcScheduler interface in Hadoop HBase and provides methods for controlling and querying various queues and statistics related to Remote Procedure Calls (RPCs) in the HBase IPC layer. It is not a message definition that could be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class that defines methods and properties for a First-In-First-Out (FIFO) Remote Procedure Call (RPC) scheduler in the Apache HBase software project.;;;N
org/apache/hadoop/hbase/ipc/HBaseRPCErrorHandler.class;;;public interface org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler {\n  public abstract boolean checkOOME(java.lang.Throwable);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface that defines a method to be implemented for error handling in the HBase RPC mechanism.;;;N
org/apache/hadoop/hbase/ipc/MasterFifoRpcScheduler.class;;;public class org.apache.hadoop.hbase.ipc.MasterFifoRpcScheduler extends org.apache.hadoop.hbase.ipc.FifoRpcScheduler {\n  public static final java.lang.String MASTER_SERVER_REPORT_HANDLER_COUNT;\n  public org.apache.hadoop.hbase.ipc.MasterFifoRpcScheduler(org.apache.hadoop.conf.Configuration, int, int);\n  public void start();\n  public void stop();\n  public boolean dispatch(org.apache.hadoop.hbase.ipc.CallRunner);\n  public int getGeneralQueueLength();\n  public int getActiveRpcHandlerCount();\n  public org.apache.hadoop.hbase.ipc.CallQueueInfo getCallQueueInfo();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/MetaRWQueueRpcExecutor.class;;;public class org.apache.hadoop.hbase.ipc.MetaRWQueueRpcExecutor extends org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor {\n  public static final java.lang.String META_CALL_QUEUE_READ_SHARE_CONF_KEY;\n  public static final java.lang.String META_CALL_QUEUE_SCAN_SHARE_CONF_KEY;\n  public static final float DEFAULT_META_CALL_QUEUE_READ_SHARE;\n  public org.apache.hadoop.hbase.ipc.MetaRWQueueRpcExecutor(java.lang.String, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/MetricsHBaseServer.class;;;public class org.apache.hadoop.hbase.ipc.MetricsHBaseServer {\n  public org.apache.hadoop.hbase.ipc.MetricsHBaseServer(java.lang.String, org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper);\n  public void exception(java.lang.Throwable);\n  public org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource getMetricsSource();\n  public org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper getHBaseServerWrapper();\n}\n;;;No, it is not a message definition. It is a class definition for monitoring and managing metrics related to HBase server performance.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/MetricsHBaseServerWrapperImpl.class;;;public class org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl implements org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper {\n  public long getTotalQueueSize();\n  public int getGeneralQueueLength();\n  public int getReplicationQueueLength();\n  public int getPriorityQueueLength();\n  public int getMetaPriorityQueueLength();\n  public int getNumOpenConnections();\n  public int getActiveRpcHandlerCount();\n  public int getActiveGeneralRpcHandlerCount();\n  public int getActivePriorityRpcHandlerCount();\n  public int getActiveMetaPriorityRpcHandlerCount();\n  public int getActiveReplicationRpcHandlerCount();\n  public long getNumGeneralCallsDropped();\n  public long getNumLifoModeSwitches();\n  public int getWriteQueueLength();\n  public int getReadQueueLength();\n  public int getScanQueueLength();\n  public int getActiveWriteRpcHandlerCount();\n  public int getActiveReadRpcHandlerCount();\n  public int getActiveScanRpcHandlerCount();\n  public long getNettyDmUsage();\n}\n;;;No. This class contains methods to retrieve metrics related to an HBase server and is not a message definition to be put on a message queue.;;;N;;;No;;;N
org/apache/hadoop/hbase/ipc/NettyRpcFrameDecoder.class;;;public class org.apache.hadoop.hbase.ipc.NettyRpcFrameDecoder extends org.apache.hbase.thirdparty.io.netty.handler.codec.ByteToMessageDecoder {\n  public org.apache.hadoop.hbase.ipc.NettyRpcFrameDecoder(int);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/NettyRpcServer$1.class;;;class org.apache.hadoop.hbase.ipc.NettyRpcServer$1 extends org.apache.hbase.thirdparty.io.netty.channel.ChannelInitializer<org.apache.hbase.thirdparty.io.netty.channel.Channel> {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/ipc/NettyRpcServer.class;;;public class org.apache.hadoop.hbase.ipc.NettyRpcServer extends org.apache.hadoop.hbase.ipc.RpcServer {\n  public static final org.slf4j.Logger LOG;\n  public static final java.lang.String HBASE_NETTY_EVENTLOOP_RPCSERVER_THREADCOUNT_KEY;\n  public org.apache.hadoop.hbase.ipc.NettyRpcServer(org.apache.hadoop.hbase.Server, java.lang.String, java.util.List<org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.RpcScheduler, boolean) throws java.io.IOException;\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public void setSocketSendBufSize(int);\n  public int getNumOpenConnections();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a Java class that defines methods and properties to be used for implementing a NettyRpcServer.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/NettyRpcServerPreambleHandler.class;;;class org.apache.hadoop.hbase.ipc.NettyRpcServerPreambleHandler extends org.apache.hbase.thirdparty.io.netty.channel.SimpleChannelInboundHandler<org.apache.hbase.thirdparty.io.netty.buffer.ByteBuf> {\n  public org.apache.hadoop.hbase.ipc.NettyRpcServerPreambleHandler(org.apache.hadoop.hbase.ipc.NettyRpcServer);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/NettyRpcServerRequestDecoder.class;;;class org.apache.hadoop.hbase.ipc.NettyRpcServerRequestDecoder extends org.apache.hbase.thirdparty.io.netty.channel.ChannelInboundHandlerAdapter {\n  public org.apache.hadoop.hbase.ipc.NettyRpcServerRequestDecoder(org.apache.hbase.thirdparty.io.netty.channel.group.ChannelGroup, org.apache.hadoop.hbase.ipc.MetricsHBaseServer);\n  public void channelActive(org.apache.hbase.thirdparty.io.netty.channel.ChannelHandlerContext) throws java.lang.Exception;\n  public void channelRead(org.apache.hbase.thirdparty.io.netty.channel.ChannelHandlerContext, java.lang.Object) throws java.lang.Exception;\n  public void channelInactive(org.apache.hbase.thirdparty.io.netty.channel.ChannelHandlerContext) throws java.lang.Exception;\n  public void exceptionCaught(org.apache.hbase.thirdparty.io.netty.channel.ChannelHandlerContext, java.lang.Throwable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/NettyRpcServerResponseEncoder.class;;;class org.apache.hadoop.hbase.ipc.NettyRpcServerResponseEncoder extends org.apache.hbase.thirdparty.io.netty.channel.ChannelOutboundHandlerAdapter {\n  public void write(org.apache.hbase.thirdparty.io.netty.channel.ChannelHandlerContext, java.lang.Object, org.apache.hbase.thirdparty.io.netty.channel.ChannelPromise) throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/NettyServerCall.class;;;class org.apache.hadoop.hbase.ipc.NettyServerCall extends org.apache.hadoop.hbase.ipc.ServerCall<org.apache.hadoop.hbase.ipc.NettyServerRpcConnection> {\n  public synchronized void sendResponseIfReady() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/NettyServerRpcConnection.class;;;class org.apache.hadoop.hbase.ipc.NettyServerRpcConnection extends org.apache.hadoop.hbase.ipc.ServerRpcConnection {\n  public synchronized void close();\n  public boolean isConnectionOpen();\n  public org.apache.hadoop.hbase.ipc.NettyServerCall createCall(int, org.apache.hbase.thirdparty.com.google.protobuf.BlockingService, org.apache.hbase.thirdparty.com.google.protobuf.Descriptors$MethodDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner, long, java.net.InetAddress, int, org.apache.hadoop.hbase.ipc.RpcServer$CallCleanup);\n  public org.apache.hadoop.hbase.ipc.ServerCall createCall(int, org.apache.hbase.thirdparty.com.google.protobuf.BlockingService, org.apache.hbase.thirdparty.com.google.protobuf.Descriptors$MethodDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner, long, java.net.InetAddress, int, org.apache.hadoop.hbase.ipc.RpcServer$CallCleanup);\n}\n;;;No. This class contains method definitions for a class and does not represent a message that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/PluggableBlockingQueue.class;;;public abstract class org.apache.hadoop.hbase.ipc.PluggableBlockingQueue implements java.util.concurrent.BlockingQueue<org.apache.hadoop.hbase.ipc.CallRunner> {\n  public org.apache.hadoop.hbase.ipc.PluggableBlockingQueue(int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration);\n}\n;;;No. This is a class definition and cannot be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/ipc/PluggableRpcQueueNotFound.class;;;public class org.apache.hadoop.hbase.ipc.PluggableRpcQueueNotFound extends java.lang.RuntimeException {\n  public org.apache.hadoop.hbase.ipc.PluggableRpcQueueNotFound(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/PriorityFunction.class;;;public interface org.apache.hadoop.hbase.ipc.PriorityFunction {\n  public abstract int getPriority(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.security.User);\n  public abstract long getDeadline(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message);\n}\n;;;No, it is an interface for a priority function in the Hadoop ecosystem. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/QosPriority.class;;;public interface org.apache.hadoop.hbase.ipc.QosPriority extends java.lang.annotation.Annotation {\n  public abstract int priority();\n}\n;;;No, it is an interface definition for a Java annotation and does not represent a message to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/QueueBalancer.class;;;public interface org.apache.hadoop.hbase.ipc.QueueBalancer {\n  public abstract int getNextQueue(org.apache.hadoop.hbase.ipc.CallRunner);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/ipc/RPCTInfoGetter.class;;;final class org.apache.hadoop.hbase.ipc.RPCTInfoGetter implements io.opentelemetry.context.propagation.TextMapGetter<org.apache.hadoop.hbase.shaded.protobuf.generated.TracingProtos$RPCTInfo> {\n  public java.lang.Iterable<java.lang.String> keys(org.apache.hadoop.hbase.shaded.protobuf.generated.TracingProtos$RPCTInfo);\n  public java.lang.String get(org.apache.hadoop.hbase.shaded.protobuf.generated.TracingProtos$RPCTInfo, java.lang.String);\n  public java.lang.String get(java.lang.Object, java.lang.String);\n  public java.lang.Iterable keys(java.lang.Object);\n}\n;;;No. This is not a message definition. It is a class that implements the TextMapGetter interface in OpenTelemetry context propagation.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RWQueueRpcExecutor.class;;;public class org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor extends org.apache.hadoop.hbase.ipc.RpcExecutor {\n  public static final java.lang.String CALL_QUEUE_READ_SHARE_CONF_KEY;\n  public static final java.lang.String CALL_QUEUE_SCAN_SHARE_CONF_KEY;\n  public org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor(java.lang.String, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n  public boolean dispatch(org.apache.hadoop.hbase.ipc.CallRunner);\n  public int getWriteQueueLength();\n  public int getReadQueueLength();\n  public int getScanQueueLength();\n  public int getActiveHandlerCount();\n  public int getActiveWriteHandlerCount();\n  public int getActiveReadHandlerCount();\n  public int getActiveScanHandlerCount();\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RandomQueueBalancer.class;;;public class org.apache.hadoop.hbase.ipc.RandomQueueBalancer implements org.apache.hadoop.hbase.ipc.QueueBalancer {\n  public org.apache.hadoop.hbase.ipc.RandomQueueBalancer(org.apache.hadoop.conf.Configuration, java.lang.String, java.util.List<java.util.concurrent.BlockingQueue<org.apache.hadoop.hbase.ipc.CallRunner>>);\n  public int getNextQueue(org.apache.hadoop.hbase.ipc.CallRunner);\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class that implements the `QueueBalancer` interface and provides methods related to queue balancing in HBase's IPC (Inter-Process Communication) framework.;;;N
org/apache/hadoop/hbase/ipc/RpcCall.class;;;public interface org.apache.hadoop.hbase.ipc.RpcCall extends org.apache.hadoop.hbase.ipc.RpcCallContext {\n  public abstract org.apache.hbase.thirdparty.com.google.protobuf.BlockingService getService();\n  public abstract org.apache.hbase.thirdparty.com.google.protobuf.Descriptors$MethodDescriptor getMethod();\n  public abstract org.apache.hbase.thirdparty.com.google.protobuf.Message getParam();\n  public abstract org.apache.hadoop.hbase.CellScanner getCellScanner();\n  public abstract long getReceiveTime();\n  public abstract long getStartTime();\n  public abstract void setStartTime(long);\n  public abstract int getTimeout();\n  public abstract int getPriority();\n  public abstract long getDeadline();\n  public abstract long getSize();\n  public abstract org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader getHeader();\n  public abstract int getRemotePort();\n  public abstract void setResponse(org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner, java.lang.Throwable, java.lang.String);\n  public abstract void sendResponseIfReady() throws java.io.IOException;\n  public abstract void cleanup();\n  public abstract java.lang.String toShortString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcCallContext.class;;;public interface org.apache.hadoop.hbase.ipc.RpcCallContext {\n  public abstract long disconnectSince();\n  public abstract boolean isClientCellBlockSupported();\n  public abstract java.util.Optional<org.apache.hadoop.hbase.security.User> getRequestUser();\n  public default java.util.Optional<java.lang.String> getRequestUserName();\n  public abstract java.net.InetAddress getRemoteAddress();\n  public abstract org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$VersionInfo getClientVersionInfo();\n  public abstract void setCallBack(org.apache.hadoop.hbase.ipc.RpcCallback);\n  public abstract boolean isRetryImmediatelySupported();\n  public abstract long getResponseCellSize();\n  public abstract void incrementResponseCellSize(long);\n  public abstract long getResponseBlockSize();\n  public abstract void incrementResponseBlockSize(long);\n  public abstract long getResponseExceptionSize();\n  public abstract void incrementResponseExceptionSize(long);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcCallback.class;;;public interface org.apache.hadoop.hbase.ipc.RpcCallback {\n  public abstract void run() throws java.io.IOException;\n}\n;;;No, this class is a callback interface used in the Hadoop distributed computing framework. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcExecutor$CallPriorityComparator.class;;;class org.apache.hadoop.hbase.ipc.RpcExecutor$CallPriorityComparator implements java.util.Comparator<org.apache.hadoop.hbase.ipc.CallRunner> {\n  public org.apache.hadoop.hbase.ipc.RpcExecutor$CallPriorityComparator(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction);\n  public int compare(org.apache.hadoop.hbase.ipc.CallRunner, org.apache.hadoop.hbase.ipc.CallRunner);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class that implements the Comparator interface and is used for sorting CallRunners in the HBase IPC mechanism. It is not a task that can be executed on a task queue.;;;N
org/apache/hadoop/hbase/ipc/RpcExecutor.class;;;public abstract class org.apache.hadoop.hbase.ipc.RpcExecutor {\n  public static final java.lang.String CALL_QUEUE_HANDLER_FACTOR_CONF_KEY;\n  public static final java.lang.String QUEUE_MAX_CALL_DELAY_CONF_KEY;\n  public static final java.lang.String CALL_QUEUE_TYPE_CODEL_CONF_VALUE;\n  public static final java.lang.String CALL_QUEUE_TYPE_DEADLINE_CONF_VALUE;\n  public static final java.lang.String CALL_QUEUE_TYPE_FIFO_CONF_VALUE;\n  public static final java.lang.String CALL_QUEUE_TYPE_PLUGGABLE_CONF_VALUE;\n  public static final java.lang.String CALL_QUEUE_TYPE_CONF_KEY;\n  public static final java.lang.String CALL_QUEUE_TYPE_CONF_DEFAULT;\n  public static final java.lang.String CALL_QUEUE_QUEUE_BALANCER_CLASS;\n  public static final java.lang.Class<?> CALL_QUEUE_QUEUE_BALANCER_CLASS_DEFAULT;\n  public static final java.lang.String CALL_QUEUE_CODEL_TARGET_DELAY;\n  public static final java.lang.String CALL_QUEUE_CODEL_INTERVAL;\n  public static final java.lang.String CALL_QUEUE_CODEL_LIFO_THRESHOLD;\n  public static final int CALL_QUEUE_CODEL_DEFAULT_TARGET_DELAY;\n  public static final int CALL_QUEUE_CODEL_DEFAULT_INTERVAL;\n  public static final double CALL_QUEUE_CODEL_DEFAULT_LIFO_THRESHOLD;\n  public static final java.lang.String PLUGGABLE_CALL_QUEUE_CLASS_NAME;\n  public static final java.lang.String PLUGGABLE_CALL_QUEUE_WITH_FAST_PATH_ENABLED;\n  public org.apache.hadoop.hbase.ipc.RpcExecutor(java.lang.String, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n  public org.apache.hadoop.hbase.ipc.RpcExecutor(java.lang.String, int, java.lang.String, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n  public java.util.Map<java.lang.String, java.lang.Long> getCallQueueCountsSummary();\n  public java.util.Map<java.lang.String, java.lang.Long> getCallQueueSizeSummary();\n  public void start(int);\n  public void stop();\n  public abstract boolean dispatch(org.apache.hadoop.hbase.ipc.CallRunner);\n  public static org.apache.hadoop.hbase.ipc.QueueBalancer getBalancer(java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<java.util.concurrent.BlockingQueue<org.apache.hadoop.hbase.ipc.CallRunner>>);\n  public static boolean isDeadlineQueueType(java.lang.String);\n  public static boolean isCodelQueueType(java.lang.String);\n  public static boolean isFifoQueueType(java.lang.String);\n  public static boolean isPluggableQueueType(java.lang.String);\n  public static boolean isPluggableQueueWithFastPath(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public long getNumGeneralCallsDropped();\n  public long getNumLifoModeSwitches();\n  public int getActiveHandlerCount();\n  public int getActiveWriteHandlerCount();\n  public int getActiveReadHandlerCount();\n  public int getActiveScanHandlerCount();\n  public int getQueueLength();\n  public int getReadQueueLength();\n  public int getScanQueueLength();\n  public int getWriteQueueLength();\n  public java.lang.String getName();\n  public void resizeQueues(org.apache.hadoop.conf.Configuration);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n}\n;;;No. This is a class definition for an RPC executor and does not represent a message that could be placed on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcHandler.class;;;public class org.apache.hadoop.hbase.ipc.RpcHandler extends java.lang.Thread {\n  public void stopRunning();\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcResponse.class;;;interface org.apache.hadoop.hbase.ipc.RpcResponse {\n  public abstract org.apache.hadoop.hbase.ipc.BufferChain getResponse();\n  public default void done();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcScheduler$Context.class;;;public abstract class org.apache.hadoop.hbase.ipc.RpcScheduler$Context {\n  public org.apache.hadoop.hbase.ipc.RpcScheduler$Context();\n  public abstract java.net.InetSocketAddress getListenerAddress();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcScheduler.class;;;public abstract class org.apache.hadoop.hbase.ipc.RpcScheduler {\n  public static final java.lang.String IPC_SERVER_MAX_CALLQUEUE_LENGTH;\n  public static final java.lang.String IPC_SERVER_PRIORITY_MAX_CALLQUEUE_LENGTH;\n  public static final java.lang.String IPC_SERVER_REPLICATION_MAX_CALLQUEUE_LENGTH;\n  public org.apache.hadoop.hbase.ipc.RpcScheduler();\n  public abstract void init(org.apache.hadoop.hbase.ipc.RpcScheduler$Context);\n  public abstract void start();\n  public abstract void stop();\n  public abstract boolean dispatch(org.apache.hadoop.hbase.ipc.CallRunner);\n  public abstract org.apache.hadoop.hbase.ipc.CallQueueInfo getCallQueueInfo();\n  public abstract int getGeneralQueueLength();\n  public abstract int getPriorityQueueLength();\n  public abstract int getMetaPriorityQueueLength();\n  public abstract int getReplicationQueueLength();\n  public abstract int getActiveRpcHandlerCount();\n  public abstract int getActiveGeneralRpcHandlerCount();\n  public abstract int getActivePriorityRpcHandlerCount();\n  public abstract int getActiveMetaPriorityRpcHandlerCount();\n  public abstract int getActiveReplicationRpcHandlerCount();\n  public abstract long getNumGeneralCallsDropped();\n  public abstract long getNumLifoModeSwitches();\n  public abstract int getWriteQueueLength();\n  public abstract int getReadQueueLength();\n  public abstract int getScanQueueLength();\n  public abstract int getActiveWriteRpcHandlerCount();\n  public abstract int getActiveReadRpcHandlerCount();\n  public abstract int getActiveScanRpcHandlerCount();\n}\n;;;No. This is a class that defines methods for a remote procedure call (RPC) scheduler in the Hadoop Distributed File System (HDFS). It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcSchedulerContext.class;;;class org.apache.hadoop.hbase.ipc.RpcSchedulerContext extends org.apache.hadoop.hbase.ipc.RpcScheduler$Context {\n  public java.net.InetSocketAddress getListenerAddress();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcServer$BlockingServiceAndInterface.class;;;public class org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface {\n  public org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface(org.apache.hbase.thirdparty.com.google.protobuf.BlockingService, java.lang.Class<?>);\n  public java.lang.Class<?> getServiceInterface();\n  public org.apache.hbase.thirdparty.com.google.protobuf.BlockingService getBlockingService();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcServer$CallCleanup.class;;;public interface org.apache.hadoop.hbase.ipc.RpcServer$CallCleanup {\n  public abstract void run();\n}\n;;;No. This is an interface defining a method "run()" and is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcServer.class;;;public abstract class org.apache.hadoop.hbase.ipc.RpcServer implements org.apache.hadoop.hbase.ipc.RpcServerInterface,org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public static final org.slf4j.Logger LOG;\n  public static final byte CURRENT_VERSION;\n  public static final java.lang.String FALLBACK_TO_INSECURE_CLIENT_AUTH;\n  public static final java.lang.String MAX_REQUEST_SIZE;\n  public static final int DEFAULT_MAX_REQUEST_SIZE;\n  public org.apache.hadoop.hbase.ipc.RpcServer(org.apache.hadoop.hbase.Server, java.lang.String, java.util.List<org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.RpcScheduler, boolean) throws java.io.IOException;\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public boolean isStarted();\n  public synchronized void refreshAuthManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier> getSecretManager();\n  public void setSecretManager(org.apache.hadoop.security.token.SecretManager<? extends org.apache.hadoop.security.token.TokenIdentifier>);\n  public org.apache.hadoop.hbase.util.Pair<org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner> call(org.apache.hadoop.hbase.ipc.RpcCall, org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler) throws java.io.IOException;\n  public void setErrorHandler(org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler);\n  public org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler getErrorHandler();\n  public org.apache.hadoop.hbase.ipc.MetricsHBaseServer getMetrics();\n  public void addCallSize(long);\n  public synchronized void authorize(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$ConnectionHeader, java.net.InetAddress) throws org.apache.hadoop.security.authorize.AuthorizationException;\n  public static java.util.Optional<org.apache.hadoop.hbase.ipc.RpcCall> getCurrentCall();\n  public static java.util.Optional<org.apache.hadoop.hbase.ipc.ServerCall<?>> getCurrentServerCallWithCellScanner();\n  public static boolean isInRpcCallContext();\n  public static java.util.Optional<org.apache.hadoop.hbase.ipc.RpcCall> unsetCurrentCall();\n  public static void setCurrentCall(org.apache.hadoop.hbase.ipc.RpcCall);\n  public static java.util.Optional<org.apache.hadoop.hbase.security.User> getRequestUser();\n  public abstract int getNumOpenConnections();\n  public static java.util.Optional<java.lang.String> getRequestUserName();\n  public static java.util.Optional<java.net.InetAddress> getRemoteAddress();\n  public static java.net.InetAddress getRemoteIp();\n  public org.apache.hadoop.hbase.ipc.RpcScheduler getScheduler();\n  public org.apache.hadoop.hbase.io.ByteBuffAllocator getByteBuffAllocator();\n  public void setRsRpcServices(org.apache.hadoop.hbase.regionserver.RSRpcServices);\n  public void setNamedQueueRecorder(org.apache.hadoop.hbase.namequeues.NamedQueueRecorder);\n}\n;;;No, this is not a message definition. It is a class definition for an implementation of a server for handling Remote Procedure Calls (RPCs) in the Apache Hadoop ecosystem. While it is used in message passing, it is not a message definition in itself.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/RpcServerFactory.class;;;public class org.apache.hadoop.hbase.ipc.RpcServerFactory {\n  public static final org.slf4j.Logger LOG;\n  public static final java.lang.String CUSTOM_RPC_SERVER_IMPL_CONF_KEY;\n  public static org.apache.hadoop.hbase.ipc.RpcServer createRpcServer(org.apache.hadoop.hbase.Server, java.lang.String, java.util.List<org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.RpcScheduler) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.ipc.RpcServer createRpcServer(org.apache.hadoop.hbase.Server, java.lang.String, java.util.List<org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.RpcScheduler, boolean) throws java.io.IOException;\n}\n;;;No, this class is a factory class for creating RpcServer instances and does not serve as a message definition.;;;N;;;No, it is not a task definition. It is a class that provides methods for creating an RPC server.;;;N
org/apache/hadoop/hbase/ipc/RpcServerInterface.class;;;public interface org.apache.hadoop.hbase.ipc.RpcServerInterface {\n  public abstract void start();\n  public abstract boolean isStarted();\n  public abstract void stop();\n  public abstract void join() throws java.lang.InterruptedException;\n  public abstract void setSocketSendBufSize(int);\n  public abstract java.net.InetSocketAddress getListenerAddress();\n  public abstract org.apache.hadoop.hbase.util.Pair<org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner> call(org.apache.hadoop.hbase.ipc.RpcCall, org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler) throws java.io.IOException;\n  public abstract void setErrorHandler(org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler);\n  public abstract org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler getErrorHandler();\n  public abstract org.apache.hadoop.hbase.ipc.MetricsHBaseServer getMetrics();\n  public abstract void addCallSize(long);\n  public abstract void refreshAuthManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.PolicyProvider);\n  public abstract org.apache.hadoop.hbase.ipc.RpcScheduler getScheduler();\n  public abstract org.apache.hadoop.hbase.io.ByteBuffAllocator getByteBuffAllocator();\n  public abstract void setRsRpcServices(org.apache.hadoop.hbase.regionserver.RSRpcServices);\n  public abstract void setNamedQueueRecorder(org.apache.hadoop.hbase.namequeues.NamedQueueRecorder);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/ServerCall.class;;;public abstract class org.apache.hadoop.hbase.ipc.ServerCall<T extends org.apache.hadoop.hbase.ipc.ServerRpcConnection> implements org.apache.hadoop.hbase.ipc.RpcCall, org.apache.hadoop.hbase.ipc.RpcResponse {\n  public void done();\n  public void cleanup();\n  public void retainByWAL();\n  public void releaseByWAL();\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader getHeader();\n  public int getPriority();\n  public java.lang.String toShortString();\n  public synchronized void setResponse(org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner, java.lang.Throwable, java.lang.String);\n  public long disconnectSince();\n  public boolean isClientCellBlockSupported();\n  public long getResponseCellSize();\n  public void incrementResponseCellSize(long);\n  public long getResponseBlockSize();\n  public void incrementResponseBlockSize(long);\n  public long getResponseExceptionSize();\n  public void incrementResponseExceptionSize(long);\n  public long getSize();\n  public long getDeadline();\n  public java.util.Optional<org.apache.hadoop.hbase.security.User> getRequestUser();\n  public java.net.InetAddress getRemoteAddress();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$VersionInfo getClientVersionInfo();\n  public synchronized void setCallBack(org.apache.hadoop.hbase.ipc.RpcCallback);\n  public boolean isRetryImmediatelySupported();\n  public org.apache.hbase.thirdparty.com.google.protobuf.BlockingService getService();\n  public org.apache.hbase.thirdparty.com.google.protobuf.Descriptors$MethodDescriptor getMethod();\n  public org.apache.hbase.thirdparty.com.google.protobuf.Message getParam();\n  public org.apache.hadoop.hbase.CellScanner getCellScanner();\n  public long getReceiveTime();\n  public long getStartTime();\n  public void setStartTime(long);\n  public int getTimeout();\n  public int getRemotePort();\n  public synchronized org.apache.hadoop.hbase.ipc.BufferChain getResponse();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/ipc/ServerRpcConnection$ByteBuffByteInput.class;;;class org.apache.hadoop.hbase.ipc.ServerRpcConnection$ByteBuffByteInput extends org.apache.hbase.thirdparty.com.google.protobuf.ByteInput {\n  public byte read(int);\n  public int read(int, byte[], int, int);\n  public int read(int, java.nio.ByteBuffer);\n  public int size();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/ipc/ServerRpcConnection.class;;;abstract class org.apache.hadoop.hbase.ipc.ServerRpcConnection implements java.io.Closeable {\n  public org.apache.hadoop.hbase.ipc.ServerRpcConnection(org.apache.hadoop.hbase.ipc.RpcServer);\n  public java.lang.String toString();\n  public java.lang.String getHostAddress();\n  public java.net.InetAddress getHostInetAddress();\n  public int getRemotePort();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$VersionInfo getVersionInfo();\n  public void saslReadAndProcess(org.apache.hadoop.hbase.nio.ByteBuff) throws java.io.IOException, java.lang.InterruptedException;\n  public void processOneRpc(org.apache.hadoop.hbase.nio.ByteBuff) throws java.io.IOException, java.lang.InterruptedException;\n  public abstract boolean isConnectionOpen();\n  public abstract org.apache.hadoop.hbase.ipc.ServerCall<?> createCall(int, org.apache.hbase.thirdparty.com.google.protobuf.BlockingService, org.apache.hbase.thirdparty.com.google.protobuf.Descriptors$MethodDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner, long, java.net.InetAddress, int, org.apache.hadoop.hbase.ipc.RpcServer$CallCleanup);\n}\n;;;No. This class is an implementation of a server RPC connection and does not represent a message definition.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/ipc/SimpleRpcScheduler.class;;;public class org.apache.hadoop.hbase.ipc.SimpleRpcScheduler extends org.apache.hadoop.hbase.ipc.RpcScheduler implements org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public org.apache.hadoop.hbase.ipc.SimpleRpcScheduler(org.apache.hadoop.conf.Configuration, int, int, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.Abortable, int);\n  public org.apache.hadoop.hbase.ipc.SimpleRpcScheduler(org.apache.hadoop.conf.Configuration, int, int, int, org.apache.hadoop.hbase.ipc.PriorityFunction, int);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public void init(org.apache.hadoop.hbase.ipc.RpcScheduler$Context);\n  public void start();\n  public void stop();\n  public boolean dispatch(org.apache.hadoop.hbase.ipc.CallRunner);\n  public int getMetaPriorityQueueLength();\n  public int getGeneralQueueLength();\n  public int getPriorityQueueLength();\n  public int getReplicationQueueLength();\n  public int getActiveRpcHandlerCount();\n  public int getActiveMetaPriorityRpcHandlerCount();\n  public int getActiveGeneralRpcHandlerCount();\n  public int getActivePriorityRpcHandlerCount();\n  public int getActiveReplicationRpcHandlerCount();\n  public long getNumGeneralCallsDropped();\n  public long getNumLifoModeSwitches();\n  public int getWriteQueueLength();\n  public int getReadQueueLength();\n  public int getScanQueueLength();\n  public int getActiveWriteRpcHandlerCount();\n  public int getActiveReadRpcHandlerCount();\n  public int getActiveScanRpcHandlerCount();\n  public org.apache.hadoop.hbase.ipc.CallQueueInfo getCallQueueInfo();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/SimpleRpcServer$ConnectionManager$1.class;;;class org.apache.hadoop.hbase.ipc.SimpleRpcServer$ConnectionManager$1 extends java.util.TimerTask {\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/ipc/SimpleRpcServer$ConnectionManager.class;;;class org.apache.hadoop.hbase.ipc.SimpleRpcServer$ConnectionManager {\n}\n;;;No. This is a class definition within the Apache Hadoop HBase library and does not represent a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/SimpleRpcServer$Listener$Reader.class;;;class org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener$Reader implements java.lang.Runnable {\n  public void run();\n  public void addConnection(org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/SimpleRpcServer$Listener.class;;;class org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener extends java.lang.Thread {\n  public org.apache.hadoop.hbase.ipc.SimpleRpcServer$Listener(org.apache.hadoop.hbase.ipc.SimpleRpcServer, java.lang.String) throws java.io.IOException;\n  public void run();\n}\n;;;No. This class defines a thread that listens for incoming requests on an RPC server, but it is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/SimpleRpcServer.class;;;public class org.apache.hadoop.hbase.ipc.SimpleRpcServer extends org.apache.hadoop.hbase.ipc.RpcServer {\n  public org.apache.hadoop.hbase.ipc.SimpleRpcServer(org.apache.hadoop.hbase.Server, java.lang.String, java.util.List<org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface>, java.net.InetSocketAddress, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.RpcScheduler, boolean) throws java.io.IOException;\n  public void setSocketSendBufSize(int);\n  public synchronized void start();\n  public synchronized void stop();\n  public synchronized void join() throws java.lang.InterruptedException;\n  public synchronized java.net.InetSocketAddress getListenerAddress();\n  public static void bind(java.net.ServerSocket, java.net.InetSocketAddress, int) throws java.io.IOException;\n  public int getNumOpenConnections();\n}\n;;;No. This is a class definition for a SimpleRpcServer implementation in HBase, but it does not contain any specific message definitions that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/SimpleRpcServerResponder.class;;;class org.apache.hadoop.hbase.ipc.SimpleRpcServerResponder extends java.lang.Thread {\n  public void run();\n  public void registerForWrite(org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/SimpleServerCall.class;;;class org.apache.hadoop.hbase.ipc.SimpleServerCall extends org.apache.hadoop.hbase.ipc.ServerCall<org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection> {\n  public void done();\n  public synchronized void sendResponseIfReady() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/SimpleServerRpcConnection$1.class;;;class org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection$1 extends java.io.InputStream {\n  public int read() throws java.io.IOException;\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/SimpleServerRpcConnection.class;;;class org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection extends org.apache.hadoop.hbase.ipc.ServerRpcConnection {\n  public org.apache.hadoop.hbase.ipc.SimpleServerRpcConnection(org.apache.hadoop.hbase.ipc.SimpleRpcServer, java.nio.channels.SocketChannel, long);\n  public void setLastContact(long);\n  public long getLastContact();\n  public int readAndProcess() throws java.io.IOException, java.lang.InterruptedException;\n  public synchronized void close();\n  public boolean isConnectionOpen();\n  public org.apache.hadoop.hbase.ipc.SimpleServerCall createCall(int, org.apache.hbase.thirdparty.com.google.protobuf.BlockingService, org.apache.hbase.thirdparty.com.google.protobuf.Descriptors$MethodDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner, long, java.net.InetAddress, int, org.apache.hadoop.hbase.ipc.RpcServer$CallCleanup);\n  public org.apache.hadoop.hbase.ipc.ServerCall createCall(int, org.apache.hbase.thirdparty.com.google.protobuf.BlockingService, org.apache.hbase.thirdparty.com.google.protobuf.Descriptors$MethodDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.CellScanner, long, java.net.InetAddress, int, org.apache.hadoop.hbase.ipc.RpcServer$CallCleanup);\n}\n;;;No. This is a class definition for a Java class in the Apache Hadoop HBase library. It is not a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ipc/UnknownServiceException.class;;;public class org.apache.hadoop.hbase.ipc.UnknownServiceException extends org.apache.hadoop.hbase.ipc.FatalConnectionException {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/ActiveMasterManager.class;;;public class org.apache.hadoop.hbase.master.ActiveMasterManager extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public void setInfoPort(int);\n  public void nodeCreated(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n  public void nodeDeleted(java.lang.String);\n  public java.util.Optional<org.apache.hadoop.hbase.ServerName> getActiveMasterServerName();\n  public int getActiveMasterInfoPort();\n  public int getBackupMasterInfoPort(org.apache.hadoop.hbase.ServerName);\n  public void stop();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getBackupMasters();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/CachedClusterId.class;;;public class org.apache.hadoop.hbase.master.CachedClusterId {\n  public static final org.slf4j.Logger LOG;\n  public org.apache.hadoop.hbase.master.CachedClusterId(org.apache.hadoop.hbase.Server, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public java.lang.String getFromCacheOrFetch();\n  public int getCacheStats();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/ClusterSchema.class;;;public interface org.apache.hadoop.hbase.master.ClusterSchema {\n  public static final java.lang.String HBASE_MASTER_CLUSTER_SCHEMA_OPERATION_TIMEOUT_KEY;\n  public static final int DEFAULT_HBASE_MASTER_CLUSTER_SCHEMA_OPERATION_TIMEOUT;\n  public abstract org.apache.hadoop.hbase.master.TableNamespaceManager getTableNamespaceManager();\n  public abstract long createNamespace(org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.util.NonceKey, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException;\n  public abstract long modifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.util.NonceKey, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException;\n  public abstract long deleteNamespace(java.lang.String, org.apache.hadoop.hbase.util.NonceKey, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.NamespaceDescriptor getNamespace(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor> getNamespaces() throws java.io.IOException;\n}\n;;;No, this is not a message definition. It is an interface defining methods for managing namespaces in HBase.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface that defines methods related to managing a namespace in HBase.;;;N
org/apache/hadoop/hbase/master/ClusterSchemaService.class;;;public interface org.apache.hadoop.hbase.master.ClusterSchemaService extends org.apache.hadoop.hbase.master.ClusterSchema,org.apache.hbase.thirdparty.com.google.common.util.concurrent.Service {\n}\n;;;No. This is just an interface definition, not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/ClusterSchemaServiceImpl.class;;;class org.apache.hadoop.hbase.master.ClusterSchemaServiceImpl extends org.apache.hbase.thirdparty.com.google.common.util.concurrent.AbstractService implements org.apache.hadoop.hbase.master.ClusterSchemaService {\n  public org.apache.hadoop.hbase.master.TableNamespaceManager getTableNamespaceManager();\n  public long createNamespace(org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.util.NonceKey, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException;\n  public long modifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.util.NonceKey, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException;\n  public long deleteNamespace(java.lang.String, org.apache.hadoop.hbase.util.NonceKey, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws java.io.IOException;\n  public org.apache.hadoop.hbase.NamespaceDescriptor getNamespace(java.lang.String) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor> getNamespaces() throws java.io.IOException;\n}\n;;;No. This is a class definition, not a message definition. It defines behavior and methods for a class, but it is not a specific message that would be passed between systems.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/ClusterStatusPublisher$1.class;;;class org.apache.hadoop.hbase.master.ClusterStatusPublisher$1 implements java.util.Comparator<java.util.Map$Entry<org.apache.hadoop.hbase.ServerName, java.lang.Integer>> {\n  public int compare(java.util.Map$Entry<org.apache.hadoop.hbase.ServerName, java.lang.Integer>, java.util.Map$Entry<org.apache.hadoop.hbase.ServerName, java.lang.Integer>);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/ClusterStatusPublisher$MulticastPublisher$ClusterMetricsEncoder.class;;;final class org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher$ClusterMetricsEncoder extends org.apache.hbase.thirdparty.io.netty.handler.codec.MessageToMessageEncoder<org.apache.hadoop.hbase.ClusterMetrics> {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/ClusterStatusPublisher$MulticastPublisher$HBaseDatagramChannelFactory.class;;;final class org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher$HBaseDatagramChannelFactory<T extends org.apache.hbase.thirdparty.io.netty.channel.Channel> implements org.apache.hbase.thirdparty.io.netty.channel.ChannelFactory<T> {\n  public T newChannel();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a channel factory class definition.;;;N
org/apache/hadoop/hbase/master/ClusterStatusPublisher$MulticastPublisher.class;;;public class org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher implements org.apache.hadoop.hbase.master.ClusterStatusPublisher$Publisher {\n  public org.apache.hadoop.hbase.master.ClusterStatusPublisher$MulticastPublisher();\n  public java.lang.String toString();\n  public void connect(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void publish(org.apache.hadoop.hbase.ClusterMetrics);\n  public void close();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/ClusterStatusPublisher$Publisher.class;;;public interface org.apache.hadoop.hbase.master.ClusterStatusPublisher$Publisher extends java.io.Closeable {\n  public abstract void connect(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract void publish(org.apache.hadoop.hbase.ClusterMetrics);\n  public abstract void close();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/ClusterStatusPublisher.class;;;public class org.apache.hadoop.hbase.master.ClusterStatusPublisher extends org.apache.hadoop.hbase.ScheduledChore {\n  public static final java.lang.String STATUS_PUBLISHER_CLASS;\n  public static final java.lang.Class<? extends org.apache.hadoop.hbase.master.ClusterStatusPublisher$Publisher> DEFAULT_STATUS_PUBLISHER_CLASS;\n  public static final java.lang.String STATUS_PUBLISH_PERIOD;\n  public static final int DEFAULT_STATUS_PUBLISH_PERIOD;\n  public static final int MAX_SERVER_PER_MESSAGE;\n  public static final int NB_SEND;\n  public org.apache.hadoop.hbase.master.ClusterStatusPublisher(org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.conf.Configuration, java.lang.Class<? extends org.apache.hadoop.hbase.master.ClusterStatusPublisher$Publisher>) throws java.io.IOException;\n  public java.lang.String toString();\n}\n;;;No, this class is not a message definition. It is a utility class used in the HBase distributed data store for publishing cluster status.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/DeadServer.class;;;public class org.apache.hadoop.hbase.master.DeadServer {\n  public org.apache.hadoop.hbase.master.DeadServer();\n  public synchronized boolean isDeadServer(org.apache.hadoop.hbase.ServerName);\n  public synchronized java.util.Set<org.apache.hadoop.hbase.ServerName> copyServerNames();\n  public synchronized int size();\n  public synchronized java.lang.String toString();\n  public synchronized java.util.Date getTimeOfDeath(org.apache.hadoop.hbase.ServerName);\n  public synchronized boolean removeDeadServer(org.apache.hadoop.hbase.ServerName);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/DrainingServerTracker$1.class;;;class org.apache.hadoop.hbase.master.DrainingServerTracker$1 implements org.apache.hadoop.hbase.master.ServerListener {\n  public void serverAdded(org.apache.hadoop.hbase.ServerName);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/DrainingServerTracker.class;;;public class org.apache.hadoop.hbase.master.DrainingServerTracker extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public org.apache.hadoop.hbase.master.DrainingServerTracker(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.Abortable, org.apache.hadoop.hbase.master.ServerManager);\n  public void start() throws org.apache.zookeeper.KeeperException, java.io.IOException;\n  public void nodeDeleted(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$1.class;;;class org.apache.hadoop.hbase.master.HMaster$1 implements org.apache.hadoop.hbase.procedure2.store.ProcedureStore$ProcedureStoreListener {\n  public void abortProcess();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$10.class;;;class org.apache.hadoop.hbase.master.HMaster$10 implements org.apache.hadoop.hbase.master.HMaster$TableDescriptorGetter {\n  public org.apache.hadoop.hbase.client.TableDescriptor get() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$11.class;;;class org.apache.hadoop.hbase.master.HMaster$11 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$12.class;;;class org.apache.hadoop.hbase.master.HMaster$12 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$13.class;;;class org.apache.hadoop.hbase.master.HMaster$13 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$14.class;;;class org.apache.hadoop.hbase.master.HMaster$14 implements org.apache.hadoop.hbase.master.HMaster$TableDescriptorGetter {\n  public org.apache.hadoop.hbase.client.TableDescriptor get() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/HMaster$15.class;;;class org.apache.hadoop.hbase.master.HMaster$15 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/HMaster$16.class;;;class org.apache.hadoop.hbase.master.HMaster$16 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/HMaster$17.class;;;class org.apache.hadoop.hbase.master.HMaster$17 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$18.class;;;class org.apache.hadoop.hbase.master.HMaster$18 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$19.class;;;class org.apache.hadoop.hbase.master.HMaster$19 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$2.class;;;class org.apache.hadoop.hbase.master.HMaster$2 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$20.class;;;class org.apache.hadoop.hbase.master.HMaster$20 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition that extends another class in the HBase library.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$21.class;;;class org.apache.hadoop.hbase.master.HMaster$21 {\n}\n;;;No. \n\nThis is a class definition in Java that represents an anonymous inner class of the HMaster class in the HBase library. It does not represent a message type that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$3.class;;;class org.apache.hadoop.hbase.master.HMaster$3 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$4.class;;;class org.apache.hadoop.hbase.master.HMaster$4 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$5.class;;;class org.apache.hadoop.hbase.master.HMaster$5 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$6.class;;;class org.apache.hadoop.hbase.master.HMaster$6 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$7.class;;;class org.apache.hadoop.hbase.master.HMaster$7 implements org.apache.hadoop.hbase.master.HMaster$TableDescriptorGetter {\n  public org.apache.hadoop.hbase.client.TableDescriptor get() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$8.class;;;class org.apache.hadoop.hbase.master.HMaster$8 implements org.apache.hadoop.hbase.master.HMaster$TableDescriptorGetter {\n  public org.apache.hadoop.hbase.client.TableDescriptor get() throws java.io.IOException;\n}\n;;;No. It is a Java class definition and cannot be put on a message queue directly.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition that implements an interface, and it defines a method that returns a TableDescriptor object, without any indication of performing a specific task.;;;N
org/apache/hadoop/hbase/master/HMaster$9.class;;;class org.apache.hadoop.hbase.master.HMaster$9 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster$TableDescriptorGetter.class;;;public interface org.apache.hadoop.hbase.master.HMaster$TableDescriptorGetter {\n  public abstract org.apache.hadoop.hbase.client.TableDescriptor get() throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMaster.class;;;public class org.apache.hadoop.hbase.master.HMaster extends org.apache.hadoop.hbase.HBaseServerBase<org.apache.hadoop.hbase.master.MasterRpcServices> implements org.apache.hadoop.hbase.master.MasterServices {\n  public static final java.lang.String MASTER;\n  public static final java.lang.String HBASE_MASTER_WAIT_ON_SERVICE_IN_SECONDS;\n  public static final int DEFAULT_HBASE_MASTER_WAIT_ON_SERVICE_IN_SECONDS;\n  public static final java.lang.String HBASE_MASTER_CLEANER_INTERVAL;\n  public static final int DEFAULT_HBASE_MASTER_CLEANER_INTERVAL;\n  public static final java.lang.String WARMUP_BEFORE_MOVE;\n  public org.apache.hadoop.hbase.master.HMaster(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void run();\n  public org.apache.hadoop.hbase.master.MasterRpcServices getMasterRpcServices();\n  public boolean balanceSwitch(boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.MetricsMaster getMasterMetrics();\n  public boolean waitForMetaOnline();\n  public void updateConfigurationForQuotasObserver(org.apache.hadoop.conf.Configuration);\n  public boolean isCatalogJanitorEnabled();\n  public org.apache.hadoop.hbase.master.ServerManager getServerManager();\n  public org.apache.hadoop.hbase.master.MasterFileSystem getMasterFileSystem();\n  public org.apache.hadoop.hbase.master.MasterWalManager getMasterWalManager();\n  public org.apache.hadoop.hbase.master.SplitWALManager getSplitWALManager();\n  public org.apache.hadoop.hbase.master.TableStateManager getTableStateManager();\n  public org.apache.hadoop.hbase.client.BalanceResponse balance() throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.BalanceResponse balanceOrUpdateMetrics() throws java.io.IOException;\n  public boolean skipRegionManagementAction(java.lang.String);\n  public org.apache.hadoop.hbase.client.BalanceResponse balance(org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.master.RegionPlan> executeRegionPlansWithThrottling(java.util.List<org.apache.hadoop.hbase.master.RegionPlan>);\n  public org.apache.hadoop.hbase.master.normalizer.RegionNormalizerManager getRegionNormalizerManager();\n  public boolean normalizeRegions(org.apache.hadoop.hbase.client.NormalizeTableFilterParams, boolean) throws java.io.IOException;\n  public java.lang.String getClientIdAuditPrefix();\n  public void setCatalogJanitorEnabled(boolean);\n  public long mergeRegions(org.apache.hadoop.hbase.client.RegionInfo[], boolean, long, long) throws java.io.IOException;\n  public long splitRegion(org.apache.hadoop.hbase.client.RegionInfo, byte[], long, long) throws java.io.IOException;\n  public void move(byte[], byte[]) throws java.io.IOException;\n  public long createTable(org.apache.hadoop.hbase.client.TableDescriptor, byte[][], long, long) throws java.io.IOException;\n  public long createSystemTable(org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public long deleteTable(org.apache.hadoop.hbase.TableName, long, long) throws java.io.IOException;\n  public long truncateTable(org.apache.hadoop.hbase.TableName, boolean, long, long) throws java.io.IOException;\n  public long addColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, long, long) throws java.io.IOException;\n  public long modifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, long, long) throws java.io.IOException;\n  public long modifyColumnStoreFileTracker(org.apache.hadoop.hbase.TableName, byte[], java.lang.String, long, long) throws java.io.IOException;\n  public long deleteColumn(org.apache.hadoop.hbase.TableName, byte[], long, long) throws java.io.IOException;\n  public long enableTable(org.apache.hadoop.hbase.TableName, long, long) throws java.io.IOException;\n  public long disableTable(org.apache.hadoop.hbase.TableName, long, long) throws java.io.IOException;\n  public long modifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, long, long) throws java.io.IOException;\n  public long modifyTableStoreFileTracker(org.apache.hadoop.hbase.TableName, java.lang.String, long, long) throws java.io.IOException;\n  public long restoreSnapshot(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, long, long, boolean, java.lang.String) throws java.io.IOException;\n  public void checkTableModifiable(org.apache.hadoop.hbase.TableName) throws java.io.IOException, org.apache.hadoop.hbase.TableNotFoundException, org.apache.hadoop.hbase.TableNotDisabledException;\n  public org.apache.hadoop.hbase.ClusterMetrics getClusterMetricsWithoutCoprocessor() throws java.io.InterruptedIOException;\n  public org.apache.hadoop.hbase.ClusterMetrics getClusterMetricsWithoutCoprocessor(java.util.EnumSet<org.apache.hadoop.hbase.ClusterMetrics$Option>) throws java.io.InterruptedIOException;\n  public org.apache.hadoop.hbase.ClusterMetrics getClusterMetrics() throws java.io.IOException;\n  public org.apache.hadoop.hbase.ClusterMetrics getClusterMetrics(java.util.EnumSet<org.apache.hadoop.hbase.ClusterMetrics$Option>) throws java.io.IOException;\n  public int getActiveMasterInfoPort();\n  public int getBackupMasterInfoPort(org.apache.hadoop.hbase.ServerName);\n  public static java.lang.String getLoadedCoprocessors();\n  public long getMasterStartTime();\n  public long getMasterActiveTime();\n  public long getMasterFinishedInitializationTime();\n  public int getNumWALFiles();\n  public org.apache.hadoop.hbase.procedure2.store.ProcedureStore getProcedureStore();\n  public int getRegionServerInfoPort(org.apache.hadoop.hbase.ServerName);\n  public java.lang.String getRegionServerVersion(org.apache.hadoop.hbase.ServerName);\n  public void checkIfShouldMoveSystemRegionAsync();\n  public java.lang.String[] getMasterCoprocessors();\n  public void abort(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost getMasterCoprocessorHost();\n  public org.apache.hadoop.hbase.quotas.MasterQuotaManager getMasterQuotaManager();\n  public org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> getMasterProcedureExecutor();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public org.apache.hadoop.hbase.master.assignment.AssignmentManager getAssignmentManager();\n  public org.apache.hadoop.hbase.master.janitor.CatalogJanitor getCatalogJanitor();\n  public org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer getRegionServerFatalLogBuffer();\n  public void shutdown() throws java.io.IOException;\n  public void stopMaster() throws java.io.IOException;\n  public void stop(java.lang.String);\n  public boolean isActiveMaster();\n  public boolean isInitialized();\n  public boolean isOnline();\n  public boolean isInMaintenanceMode();\n  public void setInitialized(boolean);\n  public org.apache.hadoop.hbase.procedure2.ProcedureEvent<?> getInitializedEvent();\n  public double getAverageLoad();\n  public boolean registerService(org.apache.hbase.thirdparty.com.google.protobuf.Service);\n  public static org.apache.hadoop.hbase.master.HMaster constructMaster(java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, org.apache.hadoop.conf.Configuration);\n  public static void main(java.lang.String[]);\n  public org.apache.hadoop.hbase.master.cleaner.HFileCleaner getHFileCleaner();\n  public java.util.List<org.apache.hadoop.hbase.master.cleaner.HFileCleaner> getHFileCleaners();\n  public org.apache.hadoop.hbase.master.cleaner.LogCleaner getLogCleaner();\n  public org.apache.hadoop.hbase.master.snapshot.SnapshotManager getSnapshotManager();\n  public org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost getMasterProcedureManagerHost();\n  public org.apache.hadoop.hbase.master.ClusterSchema getClusterSchema();\n  public java.util.List<java.lang.String> listNamespaces() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.TableName> listTableNamesByNamespace(java.lang.String) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.client.TableDescriptor> listTableDescriptorsByNamespace(java.lang.String) throws java.io.IOException;\n  public boolean abortProcedure(long, boolean) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.procedure2.Procedure<?>> getProcedures() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.procedure2.LockedResource> getLocks() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.client.TableDescriptor> listTableDescriptors(java.lang.String, java.lang.String, java.util.List<org.apache.hadoop.hbase.TableName>, boolean) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.TableName> listTableNames(java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public long getLastMajorCompactionTimestamp(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public long getLastMajorCompactionTimestampForRegion(byte[]) throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionInfoResponse$CompactionState getMobCompactionState(org.apache.hadoop.hbase.TableName);\n  public void reportMobCompactionStart(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void reportMobCompactionEnd(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public boolean isBalancerOn();\n  public boolean isNormalizerOn();\n  public boolean isSplitOrMergeEnabled(org.apache.hadoop.hbase.client.MasterSwitchType);\n  public java.lang.String getLoadBalancerClassName();\n  public org.apache.hadoop.hbase.master.SplitOrMergeTracker getSplitOrMergeTracker();\n  public org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer getLoadBalancer();\n  public org.apache.hadoop.hbase.favored.FavoredNodesManager getFavoredNodesManager();\n  public long addReplicationPeer(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig, boolean) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public long removeReplicationPeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public long enableReplicationPeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public long disableReplicationPeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public org.apache.hadoop.hbase.replication.ReplicationPeerConfig getReplicationPeerConfig(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public long updateReplicationPeerConfig(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.replication.ReplicationPeerDescription> listReplicationPeers(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public long transitReplicationPeerSyncReplicationState(java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public void decommissionRegionServers(java.util.List<org.apache.hadoop.hbase.ServerName>, boolean) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.ServerName> listDecommissionedRegionServers();\n  public void recommissionRegionServer(org.apache.hadoop.hbase.ServerName, java.util.List<byte[]>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.locking.LockManager getLockManager();\n  public org.apache.hadoop.hbase.quotas.QuotaObserverChore getQuotaObserverChore();\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshotNotifier getSpaceQuotaSnapshotNotifier();\n  public void remoteProcedureCompleted(long);\n  public void remoteProcedureFailed(long, org.apache.hadoop.hbase.procedure2.RemoteProcedureException);\n  public org.apache.hadoop.hbase.master.replication.ReplicationPeerManager getReplicationPeerManager();\n  public java.util.HashMap<java.lang.String, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.replication.ReplicationLoadSource>>> getReplicationLoad(org.apache.hadoop.hbase.ServerName[]);\n  public static void decorateMasterConfiguration(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.quotas.SnapshotQuotaObserverChore getSnapshotQuotaObserverChore();\n  public org.apache.hadoop.hbase.master.ActiveMasterManager getActiveMasterManager();\n  public org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager getSyncReplicationReplayWALManager();\n  public org.apache.hadoop.hbase.master.hbck.HbckChore getHbckChore();\n  public void runReplicationBarrierCleaner();\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfoManager getRSGroupInfoManager();\n  public org.apache.hadoop.hbase.client.CompactionState getCompactionState(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.master.zksyncer.MetaLocationSyncer getMetaLocationSyncer();\n  public org.apache.hadoop.hbase.master.region.MasterRegion getMasterRegion();\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public java.lang.String getClusterId();\n  public java.util.Optional<org.apache.hadoop.hbase.ServerName> getActiveMaster();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getBackupMasters();\n  public java.util.Iterator<org.apache.hadoop.hbase.ServerName> getBootstrapNodes();\n  public java.util.List<org.apache.hadoop.hbase.HRegionLocation> getMetaLocations();\n  public void flushMasterStore() throws java.io.IOException;\n  public java.util.Collection<org.apache.hadoop.hbase.ServerName> getLiveRegionServers();\n  public org.apache.hadoop.hbase.conf.ConfigurationManager getConfigurationManager();\n  public org.apache.hadoop.hbase.master.LoadBalancer getLoadBalancer();\n}\n;;;Yes, this class might be put on a message queue as it contains methods that can be invoked as messages by other components or services in the system.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/HMasterCommandLine$LocalHMaster.class;;;public class org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster extends org.apache.hadoop.hbase.master.HMaster {\n  public org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/HMasterCommandLine.class;;;public class org.apache.hadoop.hbase.master.HMasterCommandLine extends org.apache.hadoop.hbase.util.ServerCommandLine {\n  public org.apache.hadoop.hbase.master.HMasterCommandLine(java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>);\n  public int run(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No.;;;N;;;No. It is a command line interface for the HBase master. It is not a task definition that can be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterAnnotationReadingPriorityFunction.class;;;public class org.apache.hadoop.hbase.master.MasterAnnotationReadingPriorityFunction extends org.apache.hadoop.hbase.ipc.AnnotationReadingPriorityFunction<org.apache.hadoop.hbase.master.MasterRpcServices> {\n  public static final int META_TRANSITION_QOS;\n  public long getDeadline(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$1.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$1 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$10.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$10 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a method within a master coprocessor for Apache HBase. It is not a complete message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$100.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$100 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class is a Java class that extends another class, and it contains two methods. It is not a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$101.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$101 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class is a definition of a method or operation to be executed by a master observer in the HBase distributed database system, but it does not define any data structure or message format that could be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$102.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$102 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$103.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$103 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. It is not a message definition. It is a class definition of a specific implementation of a MasterObserverOperation in Hadoop HBase.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$104.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$104 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$105.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$105 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$106.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$106 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$107.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$107 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition. It appears to be a class definition for a specific component in the Hadoop framework.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$108.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$108 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$109.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$109 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class that extends another class and defines two methods. It is not a message definition and is not designed to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$11.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$11 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$110.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$110 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$111.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$111 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$112.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$112 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class definition for a particular class within the Apache HBase library, and it does not define a message that could be put on a message queue.;;;N;;;No, it is not a task definition. It is a class definition for a specific class in the Apache Hadoop HBase library.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$113.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$113 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$114.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$114 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$115.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$115 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a MasterCoprocessorHost$115 class that extends another class and has two methods. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$116.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$116 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$117.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$117 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$118.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$118 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$119.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$119 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No. This is a class definition for a method in the Apache HBase package. It is not a task definition that can be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$12.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$12 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$120.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$120 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue, as it does not define any specific task or instructions to be executed by a worker.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$121.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$121 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$122.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$122 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$123.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$123 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$124.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$124 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition. It is a class that extends a superclass and contains two methods that can throw an exception.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$125.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$125 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$126.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$126 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$127.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$127 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$128.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$128 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. \n\nThis class is a Java class that extends another class and has two methods, but it does not contain any message or message data that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$129.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$129 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$13.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$13 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.MasterCoprocessor, org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.client.TableDescriptor> {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$130.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$130 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$131.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$131 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$132.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$132 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a Java class definition that extends another Java class.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$133.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$133 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$134.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$134 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$135.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$135 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class definition for a specific class in the Apache HBase framework.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$136.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$136 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$137.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$137 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a MasterCoprocessorHost in Hadoop.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$138.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$138 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$139.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$139 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$14.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$14 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No. It is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$140.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$140 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$141.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$141 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$142.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$142 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$143.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$143 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a method call in the Hadoop HBase framework.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$144.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$144 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$145.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$145 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$146.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$146 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a Java class within the Apache HBase project, but it is not a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$147.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$147 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$148.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$148 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$149.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$149 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a specific implementation in the HBase framework.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$15.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$15 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class definition for a specific functionality within the Apache HBase software. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$150.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$150 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$151.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$151 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$152.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$152 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$153.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$153 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. The class is a Java class that extends another class and defines two methods. It does not represent a message or message definition that could be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$154.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$154 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$155.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$155 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class is not a message definition that might be put on a message queue. It is a class definition for a specific implementation of a method call in HBase's coprocessor framework.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$156.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$156 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No. This is a class definition for a specific implementation of a method call in a Hadoop HBase MasterCoprocessorHost. It is not a task definition that can be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$157.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$157 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$158.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$158 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$159.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$159 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$16.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$16 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$160.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$160 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$161.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$161 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$162.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$162 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class appears to be a Java class that extends another class and defines two methods. It does not define any message formats or data structures that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$163.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$163 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition for a specific functionality in HBase.;;;N;;;No, it is not a task definition. It appears to be a class definition for a specific observer operation in HBase.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$164.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$164 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It appears to be a class definition for a specific implementation in the Hadoop HBase codebase.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$165.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$165 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$166.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$166 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$167.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$167 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$168.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$168 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$169.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$169 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$17.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$17 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$170.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$170 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$171.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$171 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$172.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$172 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class definition and not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$173.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$173 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$174.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$174 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition for a specific component in the Hadoop HBase system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$175.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$175 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$176.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$176 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$177.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$177 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. It is a class definition for a specific implementation in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$178.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$178 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class that extends another class and contains two methods. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$179.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$179 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$18.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$18 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$180.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$180 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$181.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$181 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$182.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$182 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$183.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$183 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$19.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$19 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. \n\nThis is a Java class definition that appears to be a part of the HBase project. It defines two methods, but it is not a message definition that could be used to put a message on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$2.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$2 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$20.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$20 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a specific observer operation in Apache HBase's master coprocessor.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$21.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$21 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$22.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$22 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$23.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$23 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. It is a class definition, but it does not define a message that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$24.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$24 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$25.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$25 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$26.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$26 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.MasterCoprocessor, org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.MasterObserver, org.apache.hadoop.hbase.client.TableDescriptor> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$27.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$27 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a specific component in Apache HBase, and it does not represent a specific task that needs to be executed.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$28.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$28 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.MasterCoprocessor, org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.MasterObserver, java.lang.String> {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$29.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$29 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$3.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$3 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$30.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$30 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.MasterCoprocessor, org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.MasterObserver, java.lang.String> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$31.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$31 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class that extends another class and defines two methods, but it does not define a task to be executed on a queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$32.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$32 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$33.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$33 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a class that extends another class and defines two methods. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$34.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$34 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a Java class that extends another class and contains two method definitions. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$35.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$35 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class defining an operation in HBase's master coprocessor framework.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$36.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$36 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$37.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$37 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a Java class in the Apache HBase project. It does not define a specific message that can be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$38.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$38 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be a class definition for a component of the Apache Hadoop HBase system.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$39.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$39 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$4.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$4 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$40.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$40 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a specific implementation of a master coprocessor in Hadoop HBase. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$41.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$41 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$42.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$42 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, this is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$43.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$43 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$44.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$44 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$45.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$45 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$46.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$46 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition and it is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$47.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$47 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$48.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$48 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$49.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$49 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is defining a class and its methods, but it is not a task that can be executed on a queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$5.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$5 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class is actually an implementation of a method that belongs to a MasterCoprocessorHost$MasterObserverOperation class. It is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$50.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$50 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$51.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$51 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$52.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$52 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$53.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$53 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a Java class that extends another class, and includes two method declarations. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$54.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$54 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$55.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$55 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. It is a class definition for a MasterCoprocessorHost in the Apache HBase project. It does not define a message that could be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$56.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$56 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No. This is a class definition for a HBase master coprocessor host that extends a master observer operation class and contains two methods. It is not a task definition that can be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$57.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$57 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class that extends another class and defines two methods, but it is not clear what those methods do or how they relate to a specific task.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$58.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$58 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$59.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$59 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$6.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$6 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It seems to be a Java class that extends another class and contains two method definitions.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$60.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$60 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$61.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$61 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$62.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$62 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$63.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$63 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class definition and not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$64.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$64 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$65.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$65 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$66.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$66 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$67.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$67 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$68.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$68 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$69.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$69 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$7.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$7 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$70.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$70 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$71.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$71 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a Java class, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$72.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$72 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$73.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$73 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a class that extends another class and has two methods. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$74.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$74 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class definition, not a message definition. It extends another class and defines two methods, but it does not specify any message that might be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$75.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$75 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void postEnvCall();\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class definition and not a message definition. It is a subclass of another class and contains several method definitions. It is not clear how it would be used in the context of a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$76.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$76 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void postEnvCall();\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a method call in a Hadoop MapReduce application.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$77.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$77 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$78.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$78 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class is a method definition, not a message definition. It defines two methods, "call" which throws an IOException, and extends another class called "MasterCoprocessorHost$MasterObserverOperation".;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$79.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$79 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No. The given class does not define a task that can be put on a task queue. It appears to be a class definition for a specific HBase master coprocessor.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$8.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$8 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$80.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$80 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$81.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$81 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a class named org.apache.hadoop.hbase.master.MasterCoprocessorHost$81, which extends the class org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation and defines two methods. It is not a message definition that might be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$82.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$82 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$83.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$83 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a specific operation in the Hadoop HBase master coprocessor. It is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$84.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$84 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It appears to be a Java class definition for a MasterCoprocessorHost $84.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a MasterCoprocessorHost with two methods, but it does not define any specific task or work to be done.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$85.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$85 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$86.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$86 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. \n\nThis is a class definition. It defines a class that extends another class and contains two methods, but it does not define any message or data structure that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$87.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$87 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$88.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$88 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$89.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$89 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$9.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$9 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be a Java class that extends a parent class and defines two methods. There is no indication that it represents a specific task or operation that needs to be queued and executed asynchronously.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$90.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$90 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class definition for a specific class in the Hadoop ecosystem, but it is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$91.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$91 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be a class definition for a MasterCoprocessorHost.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$92.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$92 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$93.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$93 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$94.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$94 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class representing a method call, but not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$95.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$95 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a Java class in the Hadoop ecosystem, but it is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$96.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$96 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$97.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$97 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$98.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$98 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$99.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$99 extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.MasterObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$MasterEnvironment.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment extends org.apache.hadoop.hbase.coprocessor.BaseEnvironment<org.apache.hadoop.hbase.coprocessor.MasterCoprocessor> implements org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment {\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment(org.apache.hadoop.hbase.coprocessor.MasterCoprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.master.MasterServices);\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public org.apache.hadoop.hbase.client.Connection getConnection();\n  public org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.metrics.MetricRegistry getMetricRegistryForMaster();\n  public void shutdown();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$MasterEnvironmentForCoreCoprocessors.class;;;class org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironmentForCoreCoprocessors extends org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment implements org.apache.hadoop.hbase.coprocessor.HasMasterServices {\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironmentForCoreCoprocessors(org.apache.hadoop.hbase.coprocessor.MasterCoprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.master.MasterServices);\n  public org.apache.hadoop.hbase.master.MasterServices getMasterServices();\n}\n;;;No. This is not a message definition that might be put on a message queue. It is a Java class definition for a specific component in the Apache HBase system.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a Master Environment of HBase coprocessors.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost$MasterObserverOperation.class;;;abstract class org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.MasterCoprocessor, org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>.ObserverOperationWithoutResult<org.apache.hadoop.hbase.coprocessor.MasterObserver> {\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation(org.apache.hadoop.hbase.master.MasterCoprocessorHost);\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation(org.apache.hadoop.hbase.master.MasterCoprocessorHost, boolean);\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation(org.apache.hadoop.hbase.master.MasterCoprocessorHost, org.apache.hadoop.hbase.security.User);\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterObserverOperation(org.apache.hadoop.hbase.master.MasterCoprocessorHost, org.apache.hadoop.hbase.security.User, boolean);\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterCoprocessorHost.class;;;public class org.apache.hadoop.hbase.master.MasterCoprocessorHost extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.MasterCoprocessor, org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment> {\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost$MasterEnvironment createEnvironment(org.apache.hadoop.hbase.coprocessor.MasterCoprocessor, int, int, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.coprocessor.MasterCoprocessor checkAndGetInstance(java.lang.Class<?>) throws java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public void preCreateNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void postCreateNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void preDeleteNamespace(java.lang.String) throws java.io.IOException;\n  public void postDeleteNamespace(java.lang.String) throws java.io.IOException;\n  public void preModifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void postModifyNamespace(org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void preGetNamespaceDescriptor(java.lang.String) throws java.io.IOException;\n  public void postGetNamespaceDescriptor(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void preListNamespaces(java.util.List<java.lang.String>) throws java.io.IOException;\n  public void postListNamespaces(java.util.List<java.lang.String>) throws java.io.IOException;\n  public void preListNamespaceDescriptors(java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException;\n  public void postListNamespaceDescriptors(java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableDescriptor preCreateTableRegionsInfos(org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preCreateTable(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public void postCreateTable(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public void preCreateTableAction(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompletedCreateTableAction(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preDeleteTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postDeleteTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preDeleteTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompletedDeleteTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preTruncateTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postTruncateTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preTruncateTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompletedTruncateTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableDescriptor preModifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void postModifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public java.lang.String preModifyTableStoreFileTracker(org.apache.hadoop.hbase.TableName, java.lang.String) throws java.io.IOException;\n  public void postModifyTableStoreFileTracker(org.apache.hadoop.hbase.TableName, java.lang.String) throws java.io.IOException;\n  public java.lang.String preModifyColumnFamilyStoreFileTracker(org.apache.hadoop.hbase.TableName, byte[], java.lang.String) throws java.io.IOException;\n  public void postModifyColumnFamilyStoreFileTracker(org.apache.hadoop.hbase.TableName, byte[], java.lang.String) throws java.io.IOException;\n  public void preModifyTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompletedModifyTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preEnableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postEnableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preEnableTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompletedEnableTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preDisableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postDisableTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preDisableTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompletedDisableTableAction(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preAbortProcedure(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, long) throws java.io.IOException;\n  public void postAbortProcedure() throws java.io.IOException;\n  public void preGetProcedures() throws java.io.IOException;\n  public void postGetProcedures(java.util.List<org.apache.hadoop.hbase.procedure2.Procedure<?>>) throws java.io.IOException;\n  public void preGetLocks() throws java.io.IOException;\n  public void postGetLocks(java.util.List<org.apache.hadoop.hbase.procedure2.LockedResource>) throws java.io.IOException;\n  public void preMove(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public void postMove(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public void preAssign(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void postAssign(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void preUnassign(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void postUnassign(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void preRegionOffline(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void postRegionOffline(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void preMergeRegions(org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public void postMergeRegions(org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public boolean preBalance(org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public void postBalance(org.apache.hadoop.hbase.client.BalanceRequest, java.util.List<org.apache.hadoop.hbase.master.RegionPlan>) throws java.io.IOException;\n  public void preSetSplitOrMergeEnabled(boolean, org.apache.hadoop.hbase.client.MasterSwitchType) throws java.io.IOException;\n  public void postSetSplitOrMergeEnabled(boolean, org.apache.hadoop.hbase.client.MasterSwitchType) throws java.io.IOException;\n  public void preSplitRegion(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException;\n  public void preSplitRegionAction(org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompletedSplitRegionAction(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preSplitBeforeMETAAction(byte[], java.util.List<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preSplitAfterMETAAction(org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postRollBackSplitRegionAction(org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preMergeRegionsAction(org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompletedMergeRegionsAction(org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preMergeRegionsCommit(org.apache.hadoop.hbase.client.RegionInfo[], java.util.List<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postMergeRegionsCommit(org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postRollBackMergeRegionsAction(org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preBalanceSwitch(boolean) throws java.io.IOException;\n  public void postBalanceSwitch(boolean, boolean) throws java.io.IOException;\n  public void preShutdown() throws java.io.IOException;\n  public void preStopMaster() throws java.io.IOException;\n  public void preMasterInitialization() throws java.io.IOException;\n  public void postStartMaster() throws java.io.IOException;\n  public void preSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompletedSnapshotAction(org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preListSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public void postListSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public void preCloneSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void postCloneSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preRestoreSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void postRestoreSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preDeleteSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public void postDeleteSnapshot(org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public void preGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public void postGetTableDescriptors(java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public void preGetTableNames(java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public void postGetTableNames(java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public void preTableFlush(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postTableFlush(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preMasterStoreFlush() throws java.io.IOException;\n  public void postMasterStoreFlush() throws java.io.IOException;\n  public void preSetUserQuota(java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void postSetUserQuota(java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetUserQuota(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void postSetUserQuota(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetUserQuota(java.lang.String, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void postSetUserQuota(java.lang.String, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetTableQuota(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void postSetTableQuota(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetNamespaceQuota(java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void postSetNamespaceQuota(java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetRegionServerQuota(java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void postSetRegionServerQuota(java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preMoveServersAndTables(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public void postMoveServersAndTables(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public void preMoveServers(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String) throws java.io.IOException;\n  public void postMoveServers(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String) throws java.io.IOException;\n  public void preMoveTables(java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public void postMoveTables(java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public void preAddRSGroup(java.lang.String) throws java.io.IOException;\n  public void postAddRSGroup(java.lang.String) throws java.io.IOException;\n  public void preRemoveRSGroup(java.lang.String) throws java.io.IOException;\n  public void postRemoveRSGroup(java.lang.String) throws java.io.IOException;\n  public void preBalanceRSGroup(java.lang.String, org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public void postBalanceRSGroup(java.lang.String, org.apache.hadoop.hbase.client.BalanceRequest, org.apache.hadoop.hbase.client.BalanceResponse) throws java.io.IOException;\n  public void preRemoveServers(java.util.Set<org.apache.hadoop.hbase.net.Address>) throws java.io.IOException;\n  public void postRemoveServers(java.util.Set<org.apache.hadoop.hbase.net.Address>) throws java.io.IOException;\n  public void preGetRSGroupInfo(java.lang.String) throws java.io.IOException;\n  public void postGetRSGroupInfo(java.lang.String) throws java.io.IOException;\n  public void preGetRSGroupInfoOfTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postGetRSGroupInfoOfTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preListRSGroups() throws java.io.IOException;\n  public void postListRSGroups() throws java.io.IOException;\n  public void preListTablesInRSGroup(java.lang.String) throws java.io.IOException;\n  public void postListTablesInRSGroup(java.lang.String) throws java.io.IOException;\n  public void preRenameRSGroup(java.lang.String, java.lang.String) throws java.io.IOException;\n  public void postRenameRSGroup(java.lang.String, java.lang.String) throws java.io.IOException;\n  public void preUpdateRSGroupConfig(java.lang.String, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public void postUpdateRSGroupConfig(java.lang.String, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public void preGetConfiguredNamespacesAndTablesInRSGroup(java.lang.String) throws java.io.IOException;\n  public void postGetConfiguredNamespacesAndTablesInRSGroup(java.lang.String) throws java.io.IOException;\n  public void preGetRSGroupInfoOfServer(org.apache.hadoop.hbase.net.Address) throws java.io.IOException;\n  public void postGetRSGroupInfoOfServer(org.apache.hadoop.hbase.net.Address) throws java.io.IOException;\n  public void preAddReplicationPeer(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public void postAddReplicationPeer(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public void preRemoveReplicationPeer(java.lang.String) throws java.io.IOException;\n  public void postRemoveReplicationPeer(java.lang.String) throws java.io.IOException;\n  public void preEnableReplicationPeer(java.lang.String) throws java.io.IOException;\n  public void postEnableReplicationPeer(java.lang.String) throws java.io.IOException;\n  public void preDisableReplicationPeer(java.lang.String) throws java.io.IOException;\n  public void postDisableReplicationPeer(java.lang.String) throws java.io.IOException;\n  public void preGetReplicationPeerConfig(java.lang.String) throws java.io.IOException;\n  public void postGetReplicationPeerConfig(java.lang.String) throws java.io.IOException;\n  public void preUpdateReplicationPeerConfig(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public void postUpdateReplicationPeerConfig(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public void preListReplicationPeers(java.lang.String) throws java.io.IOException;\n  public void postListReplicationPeers(java.lang.String) throws java.io.IOException;\n  public void preTransitReplicationPeerSyncReplicationState(java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState) throws java.io.IOException;\n  public void postTransitReplicationPeerSyncReplicationState(java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState) throws java.io.IOException;\n  public void preRequestLock(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.procedure2.LockType, java.lang.String) throws java.io.IOException;\n  public void postRequestLock(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.procedure2.LockType, java.lang.String) throws java.io.IOException;\n  public void preLockHeartbeat(org.apache.hadoop.hbase.master.locking.LockProcedure, boolean) throws java.io.IOException;\n  public void postLockHeartbeat(org.apache.hadoop.hbase.master.locking.LockProcedure, boolean) throws java.io.IOException;\n  public void preGetClusterMetrics() throws java.io.IOException;\n  public void postGetClusterMetrics(org.apache.hadoop.hbase.ClusterMetrics) throws java.io.IOException;\n  public void preClearDeadServers() throws java.io.IOException;\n  public void postClearDeadServers(java.util.List<org.apache.hadoop.hbase.ServerName>, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException;\n  public void preDecommissionRegionServers(java.util.List<org.apache.hadoop.hbase.ServerName>, boolean) throws java.io.IOException;\n  public void postDecommissionRegionServers(java.util.List<org.apache.hadoop.hbase.ServerName>, boolean) throws java.io.IOException;\n  public void preListDecommissionedRegionServers() throws java.io.IOException;\n  public void postListDecommissionedRegionServers() throws java.io.IOException;\n  public void preRecommissionRegionServer(org.apache.hadoop.hbase.ServerName, java.util.List<byte[]>) throws java.io.IOException;\n  public void postRecommissionRegionServer(org.apache.hadoop.hbase.ServerName, java.util.List<byte[]>) throws java.io.IOException;\n  public void preSwitchRpcThrottle(boolean) throws java.io.IOException;\n  public void postSwitchRpcThrottle(boolean, boolean) throws java.io.IOException;\n  public void preIsRpcThrottleEnabled() throws java.io.IOException;\n  public void postIsRpcThrottleEnabled(boolean) throws java.io.IOException;\n  public void preSwitchExceedThrottleQuota(boolean) throws java.io.IOException;\n  public void postSwitchExceedThrottleQuota(boolean, boolean) throws java.io.IOException;\n  public void preGrant(org.apache.hadoop.hbase.security.access.UserPermission, boolean) throws java.io.IOException;\n  public void postGrant(org.apache.hadoop.hbase.security.access.UserPermission, boolean) throws java.io.IOException;\n  public void preRevoke(org.apache.hadoop.hbase.security.access.UserPermission) throws java.io.IOException;\n  public void postRevoke(org.apache.hadoop.hbase.security.access.UserPermission) throws java.io.IOException;\n  public void preGetUserPermissions(java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[]) throws java.io.IOException;\n  public void postGetUserPermissions(java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[]) throws java.io.IOException;\n  public void preHasUserPermissions(java.lang.String, java.util.List<org.apache.hadoop.hbase.security.access.Permission>) throws java.io.IOException;\n  public void postHasUserPermissions(java.lang.String, java.util.List<org.apache.hadoop.hbase.security.access.Permission>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Coprocessor checkAndGetInstance(java.lang.Class) throws java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public org.apache.hadoop.hbase.CoprocessorEnvironment createEnvironment(org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration);\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/MasterFileSystem.class;;;public class org.apache.hadoop.hbase.master.MasterFileSystem {\n  public static final java.lang.String HBASE_DIR_PERMS;\n  public static final java.lang.String HBASE_WAL_DIR_PERMS;\n  public org.apache.hadoop.hbase.master.MasterFileSystem(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n  public org.apache.hadoop.fs.FileSystem getWALFileSystem();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.fs.Path getRootDir();\n  public org.apache.hadoop.fs.Path getWALRootDir();\n  public org.apache.hadoop.fs.Path getRegionDir(org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.fs.Path getTempDir();\n  public org.apache.hadoop.hbase.ClusterId getClusterId();\n  public void deleteFamilyFromFS(org.apache.hadoop.hbase.client.RegionInfo, byte[]) throws java.io.IOException;\n  public void deleteFamilyFromFS(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, byte[]) throws java.io.IOException;\n  public void stop();\n  public void logFileSystemState(org.slf4j.Logger) throws java.io.IOException;\n}\n;;;No. It is a class that provides methods for managing the HBase file system and is not a definition for a message that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterInitializationMonitor.class;;;class org.apache.hadoop.hbase.master.MasterInitializationMonitor extends java.lang.Thread {\n  public static final java.lang.String TIMEOUT_KEY;\n  public static final long TIMEOUT_DEFAULT;\n  public static final java.lang.String HALT_KEY;\n  public static final boolean HALT_DEFAULT;\n  public void run();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterRegionServerList.class;;;public class org.apache.hadoop.hbase.master.MasterRegionServerList implements org.apache.hadoop.hbase.master.RegionServerList {\n  public org.apache.hadoop.hbase.master.MasterRegionServerList(org.apache.hadoop.hbase.master.region.MasterRegion, org.apache.hadoop.hbase.Abortable);\n  public void started(org.apache.hadoop.hbase.ServerName);\n  public void expired(org.apache.hadoop.hbase.ServerName);\n  public java.util.Set<org.apache.hadoop.hbase.ServerName> getAll() throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/master/MasterRpcServices$1.class;;;class org.apache.hadoop.hbase.master.MasterRpcServices$1 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterRpcServices$2.class;;;class org.apache.hadoop.hbase.master.MasterRpcServices$2 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterRpcServices$3.class;;;class org.apache.hadoop.hbase.master.MasterRpcServices$3 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterRpcServices$4.class;;;class org.apache.hadoop.hbase.master.MasterRpcServices$4 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterRpcServices$BalanceSwitchMode.class;;;final class org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode extends java.lang.Enum<org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode> {\n  public static final org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode SYNC;\n  public static final org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode ASYNC;\n  public static org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode[] values();\n  public static org.apache.hadoop.hbase.master.MasterRpcServices$BalanceSwitchMode valueOf(java.lang.String);\n}\n;;;No. This is an enum class definition and does not contain any message data.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterRpcServices.class;;;public class org.apache.hadoop.hbase.master.MasterRpcServices extends org.apache.hadoop.hbase.HBaseRpcServicesBase<org.apache.hadoop.hbase.master.HMaster> implements org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MasterService$BlockingInterface, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingInterface, org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockService$BlockingInterface, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$HbckService$BlockingInterface {\n  public static final java.lang.String MASTER_RPC_SCHEDULER_FACTORY_CLASS;\n  public org.apache.hadoop.hbase.master.MasterRpcServices(org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$GetLastFlushedSequenceIdResponse getLastFlushedSequenceId(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$GetLastFlushedSequenceIdRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerReportResponse regionServerReport(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerReportRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStartupResponse regionServerStartup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStartupRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRSFatalErrorResponse reportRSFatalError(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRSFatalErrorRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$AddColumnResponse addColumn(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$AddColumnRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$AssignRegionResponse assignRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$AssignRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$BalanceResponse balance(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$BalanceRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$CreateNamespaceResponse createNamespace(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$CreateNamespaceRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$CreateTableResponse createTable(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$CreateTableRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DeleteColumnResponse deleteColumn(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DeleteColumnRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DeleteNamespaceResponse deleteNamespace(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DeleteNamespaceRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DeleteSnapshotResponse deleteSnapshot(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DeleteSnapshotRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DeleteTableResponse deleteTable(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DeleteTableRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$TruncateTableResponse truncateTable(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$TruncateTableRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DisableTableResponse disableTable(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DisableTableRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$EnableCatalogJanitorResponse enableCatalogJanitor(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$EnableCatalogJanitorRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetCleanerChoreRunningResponse setCleanerChoreRunning(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetCleanerChoreRunningRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$EnableTableResponse enableTable(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$EnableTableRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MergeTableRegionsResponse mergeTableRegions(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MergeTableRegionsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SplitTableRegionResponse splitRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SplitTableRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CoprocessorServiceResponse execMasterService(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CoprocessorServiceRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ExecProcedureResponse execProcedure(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ExecProcedureRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ExecProcedureResponse execProcedureWithRet(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ExecProcedureRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetClusterStatusResponse getClusterStatus(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetClusterStatusRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetCompletedSnapshotsResponse getCompletedSnapshots(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetCompletedSnapshotsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListNamespacesResponse listNamespaces(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListNamespacesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetNamespaceDescriptorResponse getNamespaceDescriptor(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetNamespaceDescriptorRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetSchemaAlterStatusResponse getSchemaAlterStatus(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetSchemaAlterStatusRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetTableDescriptorsResponse getTableDescriptors(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetTableDescriptorsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetTableNamesResponse getTableNames(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetTableNamesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetTableStateResponse getTableState(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetTableStateRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsCatalogJanitorEnabledResponse isCatalogJanitorEnabled(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsCatalogJanitorEnabledRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsCleanerChoreEnabledResponse isCleanerChoreEnabled(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsCleanerChoreEnabledRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsMasterRunningResponse isMasterRunning(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsMasterRunningRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsProcedureDoneResponse isProcedureDone(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsProcedureDoneRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsSnapshotDoneResponse isSnapshotDone(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsSnapshotDoneRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetProcedureResultResponse getProcedureResult(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetProcedureResultRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$AbortProcedureResponse abortProcedure(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$AbortProcedureRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListNamespaceDescriptorsResponse listNamespaceDescriptors(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListNamespaceDescriptorsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetProceduresResponse getProcedures(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetProceduresRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetLocksResponse getLocks(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetLocksRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListTableDescriptorsByNamespaceResponse listTableDescriptorsByNamespace(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListTableDescriptorsByNamespaceRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListTableNamesByNamespaceResponse listTableNamesByNamespace(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListTableNamesByNamespaceRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyColumnResponse modifyColumn(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyColumnRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyColumnStoreFileTrackerResponse modifyColumnStoreFileTracker(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyColumnStoreFileTrackerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyNamespaceResponse modifyNamespace(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyNamespaceRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyTableResponse modifyTable(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyTableRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyTableStoreFileTrackerResponse modifyTableStoreFileTracker(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ModifyTableStoreFileTrackerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MoveRegionResponse moveRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MoveRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$OfflineRegionResponse offlineRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$OfflineRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RestoreSnapshotResponse restoreSnapshot(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RestoreSnapshotRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetSnapshotCleanupResponse switchSnapshotCleanup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetSnapshotCleanupRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsSnapshotCleanupEnabledResponse isSnapshotCleanupEnabled(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsSnapshotCleanupEnabledRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RunCatalogScanResponse runCatalogScan(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RunCatalogScanRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RunCleanerChoreResponse runCleanerChore(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RunCleanerChoreRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetBalancerRunningResponse setBalancerRunning(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetBalancerRunningRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ShutdownResponse shutdown(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ShutdownRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SnapshotResponse snapshot(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SnapshotRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$StopMasterResponse stopMaster(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$StopMasterRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsInMaintenanceModeResponse isMasterInMaintenanceMode(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsInMaintenanceModeRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$UnassignRegionResponse unassignRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$UnassignRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionResponse reportRegionStateTransition(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaResponse setQuota(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MajorCompactionTimestampResponse getLastMajorCompactionTimestamp(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MajorCompactionTimestampRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MajorCompactionTimestampResponse getLastMajorCompactionTimestampForRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$MajorCompactionTimestampForRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsBalancerEnabledResponse isBalancerEnabled(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsBalancerEnabledRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetSplitOrMergeEnabledResponse setSplitOrMergeEnabled(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetSplitOrMergeEnabledRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsSplitOrMergeEnabledResponse isSplitOrMergeEnabled(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsSplitOrMergeEnabledRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$NormalizeResponse normalize(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$NormalizeRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetNormalizerRunningResponse setNormalizerRunning(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetNormalizerRunningRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsNormalizerEnabledResponse isNormalizerEnabled(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsNormalizerEnabledRequest);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SecurityCapabilitiesResponse getSecurityCapabilities(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SecurityCapabilitiesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$AddReplicationPeerResponse addReplicationPeer(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$AddReplicationPeerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$RemoveReplicationPeerResponse removeReplicationPeer(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$RemoveReplicationPeerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$EnableReplicationPeerResponse enableReplicationPeer(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$EnableReplicationPeerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$DisableReplicationPeerResponse disableReplicationPeer(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$DisableReplicationPeerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$GetReplicationPeerConfigResponse getReplicationPeerConfig(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$GetReplicationPeerConfigRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$UpdateReplicationPeerConfigResponse updateReplicationPeerConfig(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$UpdateReplicationPeerConfigRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$TransitReplicationPeerSyncReplicationStateResponse transitReplicationPeerSyncReplicationState(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$TransitReplicationPeerSyncReplicationStateRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$ListReplicationPeersResponse listReplicationPeers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ReplicationProtos$ListReplicationPeersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListDecommissionedRegionServersResponse listDecommissionedRegionServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ListDecommissionedRegionServersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DecommissionRegionServersResponse decommissionRegionServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$DecommissionRegionServersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RecommissionRegionServerResponse recommissionRegionServer(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RecommissionRegionServerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockResponse requestLock(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockHeartbeatResponse lockHeartbeat(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockHeartbeatRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionSpaceUseReportResponse reportRegionSpaceUse(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionSpaceUseReportRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaRegionSizesResponse getSpaceQuotaRegionSizes(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaRegionSizesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetQuotaStatesResponse getQuotaStates(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetQuotaStatesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ClearDeadServersResponse clearDeadServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ClearDeadServersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportProcedureDoneResponse reportProcedureDone(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportProcedureDoneRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$FileArchiveNotificationResponse reportFileArchival(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$FileArchiveNotificationRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RunHbckChoreResponse runHbckChore(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$RunHbckChoreRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$GetTableStateResponse setTableStateInMeta(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetTableStateInMetaRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetRegionStateInMetaResponse setRegionStateInMeta(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetRegionStateInMetaRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$AssignsResponse assigns(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$AssignsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$UnassignsResponse unassigns(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$UnassignsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$BypassProcedureResponse bypassProcedure(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$BypassProcedureRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ScheduleServerCrashProcedureResponse scheduleServerCrashProcedure(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ScheduleServerCrashProcedureRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ScheduleSCPsForUnknownServersResponse scheduleSCPsForUnknownServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$ScheduleSCPsForUnknownServersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$FixMetaResponse fixMeta(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$FixMetaRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SwitchRpcThrottleResponse switchRpcThrottle(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SwitchRpcThrottleRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsRpcThrottleEnabledResponse isRpcThrottleEnabled(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsRpcThrottleEnabledRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SwitchExceedThrottleQuotaResponse switchExceedThrottleQuota(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SwitchExceedThrottleQuotaRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$GrantResponse grant(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$GrantRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$RevokeResponse revoke(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$RevokeRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$GetUserPermissionsResponse getUserPermissions(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$GetUserPermissionsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$HasUserPermissionsResponse hasUserPermissions(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$HasUserPermissionsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoResponse getRSGroupInfo(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoOfTableResponse getRSGroupInfoOfTable(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoOfTableRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoOfServerResponse getRSGroupInfoOfServer(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoOfServerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$MoveServersResponse moveServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$MoveServersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$AddRSGroupResponse addRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$AddRSGroupRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RemoveRSGroupResponse removeRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RemoveRSGroupRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$BalanceRSGroupResponse balanceRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$BalanceRSGroupRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$ListRSGroupInfosResponse listRSGroupInfos(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$ListRSGroupInfosRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RemoveServersResponse removeServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RemoveServersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$ListTablesInRSGroupResponse listTablesInRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$ListTablesInRSGroupRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetConfiguredNamespacesAndTablesInRSGroupResponse getConfiguredNamespacesAndTablesInRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetConfiguredNamespacesAndTablesInRSGroupRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RenameRSGroupResponse renameRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RenameRSGroupRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$UpdateRSGroupConfigResponse updateRSGroupConfig(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$UpdateRSGroupConfigRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$LogEntry getLogEntries(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$LogRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionInfoResponse getRegionInfo(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionInfoRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetStoreFileResponse getStoreFile(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetStoreFileRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetOnlineRegionResponse getOnlineRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetOnlineRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$OpenRegionResponse openRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$OpenRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WarmupRegionResponse warmupRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WarmupRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CloseRegionResponse closeRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CloseRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionResponse flushRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactionSwitchResponse compactionSwitch(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactionSwitchRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactRegionResponse compactRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse replicateWALEntry(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse replay(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$RollWALWriterResponse rollWALWriter(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$RollWALWriterRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetServerInfoResponse getServerInfo(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetServerInfoRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$StopServerResponse stopServer(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$StopServerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateFavoredNodesResponse updateFavoredNodes(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateFavoredNodesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionLoadResponse getRegionLoad(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionLoadRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearCompactionQueuesResponse clearCompactionQueues(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearCompactionQueuesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearRegionBlockCacheResponse clearRegionBlockCache(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearRegionBlockCacheRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaSnapshotsResponse getSpaceQuotaSnapshots(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaSnapshotsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ExecuteProceduresResponse executeProcedures(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ExecuteProceduresRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$GetLiveRegionServersResponse getLiveRegionServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$GetLiveRegionServersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse replicateToReplica(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$FlushMasterStoreResponse flushMasterStore(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$FlushMasterStoreRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n}\n;;;No. This is an enum class definition and does not contain any message data.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterRpcServicesVersionWrapper$ServiceCallFunction.class;;;public interface org.apache.hadoop.hbase.master.MasterRpcServicesVersionWrapper$ServiceCallFunction<Req, Resp> extends org.apache.hadoop.hbase.client.VersionInfoUtil$ServiceCallFunction<org.apache.hbase.thirdparty.com.google.protobuf.RpcController, Req, Resp, org.apache.hbase.thirdparty.com.google.protobuf.ServiceException> {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface definition related to RPC services and versioning in the Hadoop ecosystem.;;;N
org/apache/hadoop/hbase/master/MasterRpcServicesVersionWrapper.class;;;public class org.apache.hadoop.hbase.master.MasterRpcServicesVersionWrapper implements org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStatusService$BlockingInterface {\n  public org.apache.hadoop.hbase.master.MasterRpcServicesVersionWrapper(org.apache.hadoop.hbase.master.MasterRpcServices);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStartupResponse regionServerStartup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerStartupRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerReportResponse regionServerReport(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionServerReportRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRSFatalErrorResponse reportRSFatalError(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRSFatalErrorRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$GetLastFlushedSequenceIdResponse getLastFlushedSequenceId(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$GetLastFlushedSequenceIdRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionResponse reportRegionStateTransition(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionSpaceUseReportResponse reportRegionSpaceUse(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionSpaceUseReportRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportProcedureDoneResponse reportProcedureDone(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportProcedureDoneRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$FileArchiveNotificationResponse reportFileArchival(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$FileArchiveNotificationRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$GetLiveRegionServersResponse getLiveRegionServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$GetLiveRegionServersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n}\n;;;Yes, the class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/MasterServices.class;;;public interface org.apache.hadoop.hbase.master.MasterServices extends org.apache.hadoop.hbase.Server {\n  public abstract org.apache.hadoop.hbase.master.snapshot.SnapshotManager getSnapshotManager();\n  public abstract org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost getMasterProcedureManagerHost();\n  public abstract org.apache.hadoop.hbase.master.ClusterSchema getClusterSchema();\n  public abstract org.apache.hadoop.hbase.master.assignment.AssignmentManager getAssignmentManager();\n  public abstract org.apache.hadoop.hbase.master.MasterFileSystem getMasterFileSystem();\n  public abstract org.apache.hadoop.hbase.master.MasterWalManager getMasterWalManager();\n  public abstract org.apache.hadoop.hbase.master.ServerManager getServerManager();\n  public abstract org.apache.hadoop.hbase.executor.ExecutorService getExecutorService();\n  public abstract org.apache.hadoop.hbase.master.TableStateManager getTableStateManager();\n  public abstract org.apache.hadoop.hbase.master.MasterCoprocessorHost getMasterCoprocessorHost();\n  public abstract org.apache.hadoop.hbase.quotas.MasterQuotaManager getMasterQuotaManager();\n  public abstract org.apache.hadoop.hbase.master.normalizer.RegionNormalizerManager getRegionNormalizerManager();\n  public abstract org.apache.hadoop.hbase.master.janitor.CatalogJanitor getCatalogJanitor();\n  public abstract org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> getMasterProcedureExecutor();\n  public abstract org.apache.hadoop.hbase.procedure2.ProcedureEvent<?> getInitializedEvent();\n  public abstract org.apache.hadoop.hbase.master.MetricsMaster getMasterMetrics();\n  public abstract void checkTableModifiable(org.apache.hadoop.hbase.TableName) throws java.io.IOException, org.apache.hadoop.hbase.TableNotFoundException, org.apache.hadoop.hbase.TableNotDisabledException;\n  public abstract long createTable(org.apache.hadoop.hbase.client.TableDescriptor, byte[][], long, long) throws java.io.IOException;\n  public abstract long createSystemTable(org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public abstract long deleteTable(org.apache.hadoop.hbase.TableName, long, long) throws java.io.IOException;\n  public abstract long truncateTable(org.apache.hadoop.hbase.TableName, boolean, long, long) throws java.io.IOException;\n  public abstract long modifyTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, long, long) throws java.io.IOException;\n  public abstract long modifyTableStoreFileTracker(org.apache.hadoop.hbase.TableName, java.lang.String, long, long) throws java.io.IOException;\n  public abstract long enableTable(org.apache.hadoop.hbase.TableName, long, long) throws java.io.IOException;\n  public abstract long disableTable(org.apache.hadoop.hbase.TableName, long, long) throws java.io.IOException;\n  public abstract long addColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, long, long) throws java.io.IOException;\n  public abstract long modifyColumn(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, long, long) throws java.io.IOException;\n  public abstract long modifyColumnStoreFileTracker(org.apache.hadoop.hbase.TableName, byte[], java.lang.String, long, long) throws java.io.IOException;\n  public abstract long deleteColumn(org.apache.hadoop.hbase.TableName, byte[], long, long) throws java.io.IOException;\n  public abstract long mergeRegions(org.apache.hadoop.hbase.client.RegionInfo[], boolean, long, long) throws java.io.IOException;\n  public abstract long splitRegion(org.apache.hadoop.hbase.client.RegionInfo, byte[], long, long) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.TableDescriptors getTableDescriptors();\n  public abstract boolean registerService(org.apache.hbase.thirdparty.com.google.protobuf.Service);\n  public abstract boolean isActiveMaster();\n  public abstract boolean isInitialized();\n  public abstract boolean isInMaintenanceMode();\n  public abstract boolean skipRegionManagementAction(java.lang.String);\n  public abstract boolean abortProcedure(long, boolean) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.procedure2.Procedure<?>> getProcedures() throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.procedure2.LockedResource> getLocks() throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.client.TableDescriptor> listTableDescriptorsByNamespace(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.TableName> listTableNamesByNamespace(java.lang.String) throws java.io.IOException;\n  public abstract long getLastMajorCompactionTimestamp(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public abstract long getLastMajorCompactionTimestampForRegion(byte[]) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.master.LoadBalancer getLoadBalancer();\n  public abstract boolean isSplitOrMergeEnabled(org.apache.hadoop.hbase.client.MasterSwitchType);\n  public abstract org.apache.hadoop.hbase.favored.FavoredNodesManager getFavoredNodesManager();\n  public abstract long addReplicationPeer(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig, boolean) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract long removeReplicationPeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract long enableReplicationPeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract long disableReplicationPeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract org.apache.hadoop.hbase.replication.ReplicationPeerConfig getReplicationPeerConfig(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract org.apache.hadoop.hbase.master.replication.ReplicationPeerManager getReplicationPeerManager();\n  public abstract org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager getSyncReplicationReplayWALManager();\n  public abstract long updateReplicationPeerConfig(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.replication.ReplicationPeerDescription> listReplicationPeers(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract long transitReplicationPeerSyncReplicationState(java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract org.apache.hadoop.hbase.master.locking.LockManager getLockManager();\n  public abstract java.lang.String getRegionServerVersion(org.apache.hadoop.hbase.ServerName);\n  public abstract void checkIfShouldMoveSystemRegionAsync();\n  public abstract java.lang.String getClientIdAuditPrefix();\n  public abstract boolean isClusterUp();\n  public default org.apache.hadoop.hbase.master.SplitWALManager getSplitWALManager();\n  public abstract org.apache.hadoop.hbase.security.access.AccessChecker getAccessChecker();\n  public abstract org.apache.hadoop.hbase.security.access.ZKPermissionWatcher getZKPermissionWatcher();\n  public abstract java.util.List<org.apache.hadoop.hbase.master.RegionPlan> executeRegionPlansWithThrottling(java.util.List<org.apache.hadoop.hbase.master.RegionPlan>);\n  public abstract void runReplicationBarrierCleaner();\n  public abstract org.apache.hadoop.hbase.rsgroup.RSGroupInfoManager getRSGroupInfoManager();\n  public abstract boolean isBalancerOn();\n  public abstract boolean normalizeRegions(org.apache.hadoop.hbase.client.NormalizeTableFilterParams, boolean) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.master.zksyncer.MetaLocationSyncer getMetaLocationSyncer();\n  public abstract void flushMasterStore() throws java.io.IOException;\n}\n;;;No. This is an interface that defines the methods that the MasterServices class should have. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterWalManager$1.class;;;final class org.apache.hadoop.hbase.master.MasterWalManager$1 implements org.apache.hadoop.fs.PathFilter {\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/MasterWalManager$2.class;;;final class org.apache.hadoop.hbase.master.MasterWalManager$2 implements org.apache.hadoop.fs.PathFilter {\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MasterWalManager.class;;;public class org.apache.hadoop.hbase.master.MasterWalManager {\n  public static final org.apache.hadoop.fs.PathFilter NON_META_FILTER;\n  public org.apache.hadoop.hbase.master.MasterWalManager(org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.MasterWalManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException;\n  public void stop();\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n  public java.util.Set<org.apache.hadoop.hbase.ServerName> getSplittingServersFromWALDir() throws java.io.IOException;\n  public java.util.Set<org.apache.hadoop.hbase.ServerName> getLiveServersFromWALDir() throws java.io.IOException;\n  public java.util.Set<org.apache.hadoop.hbase.ServerName> getServerNamesFromWALDirPath(org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus[] getWALDirPaths(org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public java.util.Set<org.apache.hadoop.hbase.ServerName> getFailedServersFromLogFolders() throws java.io.IOException;\n  public void splitLog(org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public void splitMetaLog(org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public void splitMetaLog(java.util.Set<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException;\n  public void splitLog(java.util.Set<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException;\n  public void splitLog(java.util.Set<org.apache.hadoop.hbase.ServerName>, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public void archiveMetaLog(org.apache.hadoop.hbase.ServerName);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MetricsAssignmentManager.class;;;public class org.apache.hadoop.hbase.master.MetricsAssignmentManager {\n  public org.apache.hadoop.hbase.master.MetricsAssignmentManager();\n  public org.apache.hadoop.hbase.master.MetricsAssignmentManagerSource getMetricsProcSource();\n  public void updateRITCount(int);\n  public void updateRITCountOverThreshold(int);\n  public void updateRITOldestAge(long);\n  public void updateRitDuration(long);\n  public void incrementOperationCounter();\n  public void updateDeadServerOpenRegions(int);\n  public void updateUnknownServerOpenRegions(int);\n  public void updateOrphanRegionsOnRs(int);\n  public void updateOrphanRegionsOnFs(int);\n  public void updateInconsistentRegions(int);\n  public void updateHoles(int);\n  public void updateOverlaps(int);\n  public void updateUnknownServerRegions(int);\n  public void updateEmptyRegionInfoRegions(int);\n  public org.apache.hadoop.hbase.procedure2.ProcedureMetrics getAssignProcMetrics();\n  public org.apache.hadoop.hbase.procedure2.ProcedureMetrics getUnassignProcMetrics();\n  public org.apache.hadoop.hbase.procedure2.ProcedureMetrics getMoveProcMetrics();\n  public org.apache.hadoop.hbase.procedure2.ProcedureMetrics getReopenProcMetrics();\n  public org.apache.hadoop.hbase.procedure2.ProcedureMetrics getOpenProcMetrics();\n  public org.apache.hadoop.hbase.procedure2.ProcedureMetrics getCloseProcMetrics();\n  public org.apache.hadoop.hbase.procedure2.ProcedureMetrics getSplitProcMetrics();\n  public org.apache.hadoop.hbase.procedure2.ProcedureMetrics getMergeProcMetrics();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MetricsMaster$1.class;;;final class org.apache.hadoop.hbase.master.MetricsMaster$1 implements org.apache.hadoop.hbase.procedure2.ProcedureMetrics {\n  public org.apache.hadoop.hbase.metrics.Counter getSubmittedCounter();\n  public org.apache.hadoop.hbase.metrics.Histogram getTimeHisto();\n  public org.apache.hadoop.hbase.metrics.Counter getFailedCounter();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MetricsMaster.class;;;public class org.apache.hadoop.hbase.master.MetricsMaster {\n  public org.apache.hadoop.hbase.master.MetricsMaster(org.apache.hadoop.hbase.master.MetricsMasterWrapper);\n  public org.apache.hadoop.hbase.master.MetricsMasterSource getMetricsSource();\n  public org.apache.hadoop.hbase.master.MetricsMasterProcSource getMetricsProcSource();\n  public org.apache.hadoop.hbase.master.MetricsMasterQuotaSource getMetricsQuotaSource();\n  public void incrementRequests(long);\n  public void incrementReadRequests(long);\n  public void incrementWriteRequests(long);\n  public void setNumSpaceQuotas(long);\n  public void setNumTableInSpaceQuotaViolation(long);\n  public void setNumNamespacesInSpaceQuotaViolation(long);\n  public void setNumRegionSizeReports(long);\n  public void incrementQuotaObserverTime(long);\n  public org.apache.hadoop.hbase.procedure2.ProcedureMetrics getServerCrashProcMetrics();\n  public static org.apache.hadoop.hbase.procedure2.ProcedureMetrics convertToProcedureMetrics(org.apache.hadoop.hbase.metrics.OperationMetrics);\n  public void incrementSnapshotObserverTime(long);\n  public void incrementSnapshotSizeComputationTime(long);\n  public void incrementSnapshotFetchTime(long);\n}\n;;;No. This class represents metrics and methods to collect and manipulate them within the Apache HBase master, but it is not a message definition intended for a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MetricsMasterFileSystem.class;;;public class org.apache.hadoop.hbase.master.MetricsMasterFileSystem {\n  public org.apache.hadoop.hbase.master.MetricsMasterFileSystem();\n  public synchronized void addSplit(long, long);\n  public synchronized void addMetaWALSplit(long, long);\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MetricsMasterWrapperImpl.class;;;public class org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl implements org.apache.hadoop.hbase.master.MetricsMasterWrapper {\n  public org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl(org.apache.hadoop.hbase.master.HMaster);\n  public double getAverageLoad();\n  public long getSplitPlanCount();\n  public long getMergePlanCount();\n  public long getMasterInitializationTime();\n  public java.lang.String getClusterId();\n  public java.lang.String getZookeeperQuorum();\n  public java.lang.String[] getCoprocessors();\n  public long getStartTime();\n  public long getActiveTime();\n  public java.lang.String getRegionServers();\n  public int getNumRegionServers();\n  public java.lang.String getDeadRegionServers();\n  public int getNumDeadRegionServers();\n  public boolean isRunning();\n  public java.lang.String getDrainingRegionServers();\n  public int getNumDrainingRegionServers();\n  public java.lang.String getServerName();\n  public boolean getIsActiveMaster();\n  public long getNumWALFiles();\n  public java.util.Map<java.lang.String, java.util.Map$Entry<java.lang.Long, java.lang.Long>> getTableSpaceUtilization();\n  public java.util.Map<java.lang.String, java.util.Map$Entry<java.lang.Long, java.lang.Long>> getNamespaceSpaceUtilization();\n  public org.apache.hadoop.hbase.util.PairOfSameType<java.lang.Integer> getRegionCounts();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/MetricsSnapshot.class;;;public class org.apache.hadoop.hbase.master.MetricsSnapshot {\n  public org.apache.hadoop.hbase.master.MetricsSnapshot();\n  public void addSnapshot(long);\n  public void addSnapshotRestore(long);\n  public void addSnapshotClone(long);\n}\n;;;No. This class contains methods but there is no clear indication of the data that needs to be included in a message.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/NoSuchProcedureException.class;;;public class org.apache.hadoop.hbase.master.NoSuchProcedureException extends org.apache.hadoop.hbase.HBaseIOException {\n  public org.apache.hadoop.hbase.master.NoSuchProcedureException();\n  public org.apache.hadoop.hbase.master.NoSuchProcedureException(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/RegionPlacementMaintainer$RandomizedMatrix.class;;;public class org.apache.hadoop.hbase.master.RegionPlacementMaintainer$RandomizedMatrix {\n  public org.apache.hadoop.hbase.master.RegionPlacementMaintainer$RandomizedMatrix(int, int);\n  public float[][] transform(float[][]);\n  public float[][] invert(float[][]);\n  public int[] invertIndices(int[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/RegionPlacementMaintainer.class;;;public class org.apache.hadoop.hbase.master.RegionPlacementMaintainer implements java.io.Closeable {\n  public org.apache.hadoop.hbase.master.RegionPlacementMaintainer(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.RegionPlacementMaintainer(org.apache.hadoop.conf.Configuration, boolean, boolean);\n  public void setTargetTableName(java.lang.String[]);\n  public org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta getRegionAssignmentSnapshot() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.master.AssignmentVerificationReport> verifyRegionPlacement(boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.favored.FavoredNodesPlan getNewAssignmentPlan() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public static void printAssignmentPlan(org.apache.hadoop.hbase.favored.FavoredNodesPlan);\n  public void updateAssignmentPlanToMeta(org.apache.hadoop.hbase.favored.FavoredNodesPlan) throws java.io.IOException;\n  public void updateAssignmentPlan(org.apache.hadoop.hbase.favored.FavoredNodesPlan) throws java.io.IOException;\n  public java.util.Map<org.apache.hadoop.hbase.TableName, java.lang.Integer> getRegionsMovement(org.apache.hadoop.hbase.favored.FavoredNodesPlan) throws java.io.IOException;\n  public void checkDifferencesWithOldPlan(java.util.Map<org.apache.hadoop.hbase.TableName, java.lang.Integer>, java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>, org.apache.hadoop.hbase.favored.FavoredNodesPlan) throws java.io.IOException;\n  public void printDispersionScores(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta, int, org.apache.hadoop.hbase.favored.FavoredNodesPlan, boolean);\n  public void printLocalityAndDispersionForCurrentPlan(java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>>) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.ServerName> getFavoredNodeList(java.lang.String);\n  public static void main(java.lang.String[]) throws java.io.IOException;\n}\n;;;No, this is not a message definition. It is a class definition for a region placement maintainer in the Hadoop HBase framework.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/RegionServerList.class;;;public interface org.apache.hadoop.hbase.master.RegionServerList {\n  public abstract void started(org.apache.hadoop.hbase.ServerName);\n  public abstract void expired(org.apache.hadoop.hbase.ServerName);\n  public abstract java.util.Set<org.apache.hadoop.hbase.ServerName> getAll() throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition. It defines an interface for managing master region servers in Hadoop, but there is no indication that it involves any asynchronous processing or task scheduling.;;;N
org/apache/hadoop/hbase/master/RegionServerTracker.class;;;public class org.apache.hadoop.hbase.master.RegionServerTracker extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public org.apache.hadoop.hbase.master.RegionServerTracker(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.master.MasterServices);\n  public void upgrade(java.util.Set<org.apache.hadoop.hbase.ServerName>, java.util.Set<org.apache.hadoop.hbase.ServerName>, java.util.Set<org.apache.hadoop.hbase.ServerName>) throws org.apache.zookeeper.KeeperException, java.io.IOException;\n  public void stop();\n  public java.util.Set<org.apache.hadoop.hbase.ServerName> getRegionServers();\n  public void nodeChildrenChanged(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/RegionsRecoveryChore.class;;;public class org.apache.hadoop.hbase.master.RegionsRecoveryChore extends org.apache.hadoop.hbase.ScheduledChore {\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/RegionsRecoveryConfigManager.class;;;public class org.apache.hadoop.hbase.master.RegionsRecoveryConfigManager implements org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/ServerListener.class;;;public interface org.apache.hadoop.hbase.master.ServerListener {\n  public default void waiting();\n  public default void serverAdded(org.apache.hadoop.hbase.ServerName);\n  public default void serverRemoved(org.apache.hadoop.hbase.ServerName);\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an interface defining methods that can be implemented by the listener of the HBase master server.;;;N;;;No. This is an interface that defines the methods to listen for events related to HBase servers. It is not a task definition that can be put on a task queue.;;;N
org/apache/hadoop/hbase/master/ServerManager$FlushedSequenceIdFlusher.class;;;class org.apache.hadoop.hbase.master.ServerManager$FlushedSequenceIdFlusher extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.master.ServerManager$FlushedSequenceIdFlusher(org.apache.hadoop.hbase.master.ServerManager, java.lang.String, int);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/master/ServerManager$ServerLiveState.class;;;public final class org.apache.hadoop.hbase.master.ServerManager$ServerLiveState extends java.lang.Enum<org.apache.hadoop.hbase.master.ServerManager$ServerLiveState> {\n  public static final org.apache.hadoop.hbase.master.ServerManager$ServerLiveState LIVE;\n  public static final org.apache.hadoop.hbase.master.ServerManager$ServerLiveState DEAD;\n  public static final org.apache.hadoop.hbase.master.ServerManager$ServerLiveState UNKNOWN;\n  public static org.apache.hadoop.hbase.master.ServerManager$ServerLiveState[] values();\n  public static org.apache.hadoop.hbase.master.ServerManager$ServerLiveState valueOf(java.lang.String);\n}\n;;;No. This is an enum definition, not a message definition. It is used to define a set of possible values for a variable, not to define the contents of a message.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/ServerManager.class;;;public class org.apache.hadoop.hbase.master.ServerManager {\n  public static final java.lang.String WAIT_ON_REGIONSERVERS_MAXTOSTART;\n  public static final java.lang.String WAIT_ON_REGIONSERVERS_MINTOSTART;\n  public static final java.lang.String WAIT_ON_REGIONSERVERS_TIMEOUT;\n  public static final java.lang.String WAIT_ON_REGIONSERVERS_INTERVAL;\n  public static final java.lang.String PERSIST_FLUSHEDSEQUENCEID;\n  public static final boolean PERSIST_FLUSHEDSEQUENCEID_DEFAULT;\n  public static final java.lang.String FLUSHEDSEQUENCEID_FLUSHER_INTERVAL;\n  public static final int FLUSHEDSEQUENCEID_FLUSHER_INTERVAL_DEFAULT;\n  public static final java.lang.String MAX_CLOCK_SKEW_MS;\n  public org.apache.hadoop.hbase.master.ServerManager(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.RegionServerList);\n  public void registerListener(org.apache.hadoop.hbase.master.ServerListener);\n  public boolean unregisterListener(org.apache.hadoop.hbase.master.ServerListener);\n  public void regionServerReport(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerMetrics) throws org.apache.hadoop.hbase.YouAreDeadException;\n  public java.util.concurrent.ConcurrentNavigableMap<byte[], java.lang.Long> getFlushedSequenceIdByRegion();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds getLastFlushedSequenceId(byte[]);\n  public org.apache.hadoop.hbase.ServerMetrics getLoad(org.apache.hadoop.hbase.ServerName);\n  public double getAverageLoad();\n  public int countOfRegionServers();\n  public java.util.Map<org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerMetrics> getOnlineServers();\n  public org.apache.hadoop.hbase.master.DeadServer getDeadServers();\n  public boolean areDeadServersInProgress() throws java.io.IOException;\n  public synchronized long expireServer(org.apache.hadoop.hbase.ServerName);\n  public synchronized void moveFromOnlineToDeadServers(org.apache.hadoop.hbase.ServerName);\n  public synchronized boolean removeServerFromDrainList(org.apache.hadoop.hbase.ServerName);\n  public synchronized boolean addServerToDrainList(org.apache.hadoop.hbase.ServerName);\n  public static void closeRegionSilentlyAndWait(org.apache.hadoop.hbase.client.AsyncClusterConnection, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo, long) throws java.io.IOException, java.lang.InterruptedException;\n  public void waitForRegionServers(org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.lang.InterruptedException;\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getOnlineServersList();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getOnlineServersListWithPredicator(java.util.List<org.apache.hadoop.hbase.ServerName>, java.util.function.Predicate<org.apache.hadoop.hbase.ServerMetrics>);\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getDrainingServersList();\n  public boolean isServerOnline(org.apache.hadoop.hbase.ServerName);\n  public synchronized org.apache.hadoop.hbase.master.ServerManager$ServerLiveState isServerKnownAndOnline(org.apache.hadoop.hbase.ServerName);\n  public synchronized boolean isServerDead(org.apache.hadoop.hbase.ServerName);\n  public boolean isServerUnknown(org.apache.hadoop.hbase.ServerName);\n  public void shutdownCluster();\n  public boolean isClusterShutdown();\n  public void startChore();\n  public void stop();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> createDestinationServersList(java.util.List<org.apache.hadoop.hbase.ServerName>);\n  public java.util.List<org.apache.hadoop.hbase.ServerName> createDestinationServersList();\n  public void removeRegion(org.apache.hadoop.hbase.client.RegionInfo);\n  public boolean isRegionInServerManagerStates(org.apache.hadoop.hbase.client.RegionInfo);\n  public void removeRegions(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>);\n  public int getVersionNumber(org.apache.hadoop.hbase.ServerName);\n  public java.lang.String getVersion(org.apache.hadoop.hbase.ServerName);\n  public int getInfoPort(org.apache.hadoop.hbase.ServerName);\n  public void loadLastFlushedSequenceIds() throws java.io.IOException;\n  public void removeDeletedRegionFromLoadedFlushedSequenceIds();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a definition of a Java class that provides functionality related to managing servers and regions in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/SnapshotSentinel.class;;;public interface org.apache.hadoop.hbase.master.SnapshotSentinel {\n  public abstract boolean isFinished();\n  public abstract long getCompletionTimestamp();\n  public abstract void cancel(java.lang.String);\n  public abstract org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription getSnapshot();\n  public abstract org.apache.hadoop.hbase.errorhandling.ForeignException getExceptionIfFailed();\n  public abstract void rethrowExceptionIfFailed() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n}\n;;;No, it is an interface for a class in the Apache HBase libraries. It is not a complete message definition that can be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/SplitLogManager$ResubmitDirective.class;;;public final class org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective extends java.lang.Enum<org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective> {\n  public static final org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective CHECK;\n  public static final org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective FORCE;\n  public static org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective[] values();\n  public static org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective valueOf(java.lang.String);\n}\n;;;No. This class doesn't contain any fields or methods that could represent a message. It is simply an enum class with two constants and some standard enum methods.;;;N;;;No, it is not a task definition. It is an enum class definition that is used for managing split logs in Hadoop HBase.;;;N
org/apache/hadoop/hbase/master/SplitLogManager$Task.class;;;public class org.apache.hadoop.hbase.master.SplitLogManager$Task {\n  public volatile long last_update;\n  public volatile int last_version;\n  public volatile org.apache.hadoop.hbase.ServerName cur_worker_name;\n  public volatile org.apache.hadoop.hbase.master.SplitLogManager$TaskBatch batch;\n  public volatile org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus status;\n  public volatile java.util.concurrent.atomic.AtomicInteger incarnation;\n  public final java.util.concurrent.atomic.AtomicInteger unforcedResubmits;\n  public volatile boolean resubmitThresholdReached;\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.master.SplitLogManager$Task();\n  public boolean isOrphan();\n  public boolean isUnassigned();\n  public void heartbeatNoDetails(long);\n  public void heartbeat(long, int, org.apache.hadoop.hbase.ServerName);\n  public void setUnassigned();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/SplitLogManager$TaskBatch.class;;;public class org.apache.hadoop.hbase.master.SplitLogManager$TaskBatch {\n  public int installed;\n  public int done;\n  public int error;\n  public volatile boolean isDead;\n  public org.apache.hadoop.hbase.master.SplitLogManager$TaskBatch();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/SplitLogManager$TerminationStatus.class;;;public final class org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus extends java.lang.Enum<org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus> {\n  public static final org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus IN_PROGRESS;\n  public static final org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus SUCCESS;\n  public static final org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus FAILURE;\n  public static final org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus DELETED;\n  public static org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus[] values();\n  public static org.apache.hadoop.hbase.master.SplitLogManager$TerminationStatus valueOf(java.lang.String);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/SplitLogManager$TimeoutMonitor.class;;;class org.apache.hadoop.hbase.master.SplitLogManager$TimeoutMonitor extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.master.SplitLogManager$TimeoutMonitor(org.apache.hadoop.hbase.master.SplitLogManager, int, org.apache.hadoop.hbase.Stoppable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/SplitLogManager.class;;;public class org.apache.hadoop.hbase.master.SplitLogManager {\n  public static final int DEFAULT_UNASSIGNED_TIMEOUT;\n  public org.apache.hadoop.hbase.master.SplitLogManager(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.fs.FileStatus> getFileList(org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public long splitLogDistributed(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public long splitLogDistributed(java.util.List<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n  public long splitLogDistributed(java.util.Set<org.apache.hadoop.hbase.ServerName>, java.util.List<org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.PathFilter) throws java.io.IOException;\n  public void stop();\n}\n;;;No, it is a class definition for a class in the Apache HBase system. It contains methods and variables, but it is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/SplitOrMergeTracker$1.class;;;class org.apache.hadoop.hbase.master.SplitOrMergeTracker$1 {\n}\n;;;No, this is not a message definition, it is a class definition.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class within the Hadoop HBase codebase, and as such, it cannot be executed on its own as a task.;;;N
org/apache/hadoop/hbase/master/SplitOrMergeTracker$SwitchStateTracker.class;;;class org.apache.hadoop.hbase.master.SplitOrMergeTracker$SwitchStateTracker extends org.apache.hadoop.hbase.zookeeper.ZKNodeTracker {\n  public org.apache.hadoop.hbase.master.SplitOrMergeTracker$SwitchStateTracker(org.apache.hadoop.hbase.zookeeper.ZKWatcher, java.lang.String, org.apache.hadoop.hbase.Abortable);\n  public boolean isSwitchEnabled();\n  public void setSwitchEnabled(boolean) throws org.apache.zookeeper.KeeperException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/SplitOrMergeTracker.class;;;public class org.apache.hadoop.hbase.master.SplitOrMergeTracker {\n  public org.apache.hadoop.hbase.master.SplitOrMergeTracker(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Abortable);\n  public void start();\n  public boolean isSplitOrMergeEnabled(org.apache.hadoop.hbase.client.MasterSwitchType);\n  public void setSplitOrMergeEnabled(boolean, org.apache.hadoop.hbase.client.MasterSwitchType) throws org.apache.zookeeper.KeeperException;\n}\n;;;No. It is a class definition for the SplitOrMergeTracker object in the Hadoop HBase library. It is not a message definition that would typically be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/SplitWALManager.class;;;public class org.apache.hadoop.hbase.master.SplitWALManager {\n  public org.apache.hadoop.hbase.master.SplitWALManager(org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.procedure2.Procedure> splitWALs(org.apache.hadoop.hbase.ServerName, boolean) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.FileStatus> getWALsToSplit(org.apache.hadoop.hbase.ServerName, boolean) throws java.io.IOException;\n  public void archive(java.lang.String) throws java.io.IOException;\n  public void deleteWALDir(org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public boolean isSplitWALFinished(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.ServerName acquireSplitWALWorker(org.apache.hadoop.hbase.procedure2.Procedure<?>) throws org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;\n  public void releaseSplitWALWorker(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler);\n  public void addUsedSplitWALWorker(org.apache.hadoop.hbase.ServerName);\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/TableNamespaceManager.class;;;public class org.apache.hadoop.hbase.master.TableNamespaceManager {\n  public static final java.lang.String KEY_MAX_REGIONS;\n  public static final java.lang.String KEY_MAX_TABLES;\n  public void start() throws java.io.IOException;\n  public boolean doesNamespaceExist(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.NamespaceDescriptor get(java.lang.String) throws java.io.IOException;\n  public void addOrUpdateNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public static void insertNamespaceToMeta(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void deleteNamespace(java.lang.String) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor> list() throws java.io.IOException;\n  public void validateTableAndRegionCount(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public static long getMaxTables(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public static long getMaxRegions(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n}\n;;;No, the class is not a message definition that might be put on a message queue. It represents a manager for managing namespaces and tables in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/TableStateManager$1.class;;;class org.apache.hadoop.hbase.master.TableStateManager$1 implements org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor {\n  public boolean visit(org.apache.hadoop.hbase.client.Result) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/TableStateManager.class;;;public class org.apache.hadoop.hbase.master.TableStateManager {\n  public void setTableState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableState$State) throws java.io.IOException;\n  public boolean isTableState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableState$State...);\n  public void setDeletedTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public boolean isTablePresent(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableState getTableState(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n}\n;;;No. This class contains methods for managing the state of tables in HBase, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/WorkerAssigner.class;;;public class org.apache.hadoop.hbase.master.WorkerAssigner implements org.apache.hadoop.hbase.master.ServerListener {\n  public org.apache.hadoop.hbase.master.WorkerAssigner(org.apache.hadoop.hbase.master.MasterServices, int, org.apache.hadoop.hbase.procedure2.ProcedureEvent<?>);\n  public synchronized java.util.Optional<org.apache.hadoop.hbase.ServerName> acquire();\n  public synchronized void release(org.apache.hadoop.hbase.ServerName);\n  public void suspend(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public void wake(org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler);\n  public void serverAdded(org.apache.hadoop.hbase.ServerName);\n  public synchronized void addUsedWorker(org.apache.hadoop.hbase.ServerName);\n  public java.lang.Integer getAvailableWorker(org.apache.hadoop.hbase.ServerName);\n}\n;;;No, this class is not a message definition, it is a class definition for a worker assigner in Apache HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/AssignProcedure$1.class;;;class org.apache.hadoop.hbase.master.assignment.AssignProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/assignment/AssignProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.AssignProcedure extends org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure {\n  public org.apache.hadoop.hbase.master.assignment.AssignProcedure();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public void setProcId(long);\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/AssignmentManager$1.class;;;class org.apache.hadoop.hbase.master.assignment.AssignmentManager$1 extends java.lang.Thread {\n  public void run();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/AssignmentManager$2.class;;;class org.apache.hadoop.hbase.master.assignment.AssignmentManager$2 {\n}\n;;;No.;;;N;;;No information is provided to determine if it is a task definition.;;;N
org/apache/hadoop/hbase/master/assignment/AssignmentManager$DeadServerMetricRegionChore.class;;;class org.apache.hadoop.hbase.master.assignment.AssignmentManager$DeadServerMetricRegionChore extends org.apache.hadoop.hbase.procedure2.ProcedureInMemoryChore<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> {\n  public org.apache.hadoop.hbase.master.assignment.AssignmentManager$DeadServerMetricRegionChore(int);\n}\n;;;No (but almost yes). \n\nThe class itself is not a message definition, as it is a subclass of `ProcedureInMemoryChore` and not `Message`. However, the constructor `public org.apache.hadoop.hbase.master.assignment.AssignmentManager$DeadServerMetricRegionChore(int)` can be used to create an instance of the class and potentially put it on a message queue as a message to be consumed by other parts of the system.;;;N;;;No, it is not a task definition that might be put on a task queue as there is no specific task being defined in this class. It is a chore that extends a ProcedureInMemoryChore.;;;N
org/apache/hadoop/hbase/master/assignment/AssignmentManager$RegionInTransitionChore.class;;;class org.apache.hadoop.hbase.master.assignment.AssignmentManager$RegionInTransitionChore extends org.apache.hadoop.hbase.procedure2.ProcedureInMemoryChore<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> {\n  public org.apache.hadoop.hbase.master.assignment.AssignmentManager$RegionInTransitionChore(int);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/AssignmentManager$RegionInTransitionStat.class;;;public class org.apache.hadoop.hbase.master.assignment.AssignmentManager$RegionInTransitionStat {\n  public org.apache.hadoop.hbase.master.assignment.AssignmentManager$RegionInTransitionStat(org.apache.hadoop.conf.Configuration);\n  public int getRITThreshold();\n  public long getTimestamp();\n  public int getTotalRITs();\n  public long getOldestRITTime();\n  public int getTotalRITsOverThreshold();\n  public boolean hasRegionsTwiceOverThreshold();\n  public boolean hasRegionsOverThreshold();\n  public java.util.Collection<org.apache.hadoop.hbase.master.RegionState> getRegionOverThreshold();\n  public boolean isRegionOverThreshold(org.apache.hadoop.hbase.client.RegionInfo);\n  public boolean isRegionTwiceOverThreshold(org.apache.hadoop.hbase.client.RegionInfo);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It provides methods to retrieve information about a RegionInTransitionStat object, but it does not represent a specific task to be performed.;;;N
org/apache/hadoop/hbase/master/assignment/AssignmentManager$RegionMetaLoadingVisitor.class;;;class org.apache.hadoop.hbase.master.assignment.AssignmentManager$RegionMetaLoadingVisitor implements org.apache.hadoop.hbase.master.assignment.RegionStateStore$RegionStateVisitor {\n  public void visitRegionState(org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.master.RegionState$State, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName, long);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/assignment/AssignmentManager.class;;;public class org.apache.hadoop.hbase.master.assignment.AssignmentManager {\n  public static final java.lang.String BOOTSTRAP_THREAD_POOL_SIZE_CONF_KEY;\n  public static final java.lang.String ASSIGN_DISPATCH_WAIT_MSEC_CONF_KEY;\n  public static final java.lang.String ASSIGN_DISPATCH_WAITQ_MAX_CONF_KEY;\n  public static final java.lang.String RIT_CHORE_INTERVAL_MSEC_CONF_KEY;\n  public static final java.lang.String DEAD_REGION_METRIC_CHORE_INTERVAL_MSEC_CONF_KEY;\n  public static final java.lang.String ASSIGN_MAX_ATTEMPTS;\n  public static final java.lang.String ASSIGN_RETRY_IMMEDIATELY_MAX_ATTEMPTS;\n  public static final java.lang.String METRICS_RIT_STUCK_WARNING_THRESHOLD;\n  public static final java.lang.String UNEXPECTED_STATE_REGION;\n  public org.apache.hadoop.hbase.master.assignment.AssignmentManager(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.region.MasterRegion);\n  public void start() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public void setupRIT(java.util.List<org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure>);\n  public void stop();\n  public boolean isRunning();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.master.MetricsAssignmentManager getAssignmentManagerMetrics();\n  public org.apache.hadoop.hbase.master.assignment.RegionStates getRegionStates();\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getRegionsOnServer(org.apache.hadoop.hbase.ServerName);\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getTableRegions(org.apache.hadoop.hbase.TableName, boolean);\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName>> getTableRegionsAndLocations(org.apache.hadoop.hbase.TableName, boolean);\n  public org.apache.hadoop.hbase.master.assignment.RegionStateStore getRegionStateStore();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getFavoredNodes(org.apache.hadoop.hbase.client.RegionInfo);\n  public boolean isMetaRegion(byte[]);\n  public org.apache.hadoop.hbase.client.RegionInfo getMetaRegionFromName(byte[]);\n  public boolean isCarryingMeta(org.apache.hadoop.hbase.ServerName);\n  public java.util.Set<org.apache.hadoop.hbase.client.RegionInfo> getMetaRegionSet();\n  public boolean isMetaAssigned();\n  public boolean isMetaRegionInTransition();\n  public boolean waitMetaAssigned(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.client.RegionInfo);\n  public boolean waitMetaLoaded(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public void wakeMetaLoadedEvent();\n  public boolean isMetaLoaded();\n  public void checkIfShouldMoveSystemRegionAsync();\n  public long assign(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public long assign(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public java.util.concurrent.Future<byte[]> assignAsync(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public java.util.concurrent.Future<byte[]> assignAsync(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public long unassign(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure createMoveRegionProcedure(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName) throws org.apache.hadoop.hbase.HBaseIOException;\n  public void move(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public java.util.concurrent.Future<byte[]> moveAsync(org.apache.hadoop.hbase.master.RegionPlan) throws org.apache.hadoop.hbase.HBaseIOException;\n  public java.util.concurrent.Future<byte[]> balance(org.apache.hadoop.hbase.master.RegionPlan) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure[] createRoundRobinAssignProcedures(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, java.util.List<org.apache.hadoop.hbase.ServerName>);\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure[] createRoundRobinAssignProcedures(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>);\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure createOneAssignProcedure(org.apache.hadoop.hbase.client.RegionInfo, boolean);\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure createOneUnassignProcedure(org.apache.hadoop.hbase.client.RegionInfo, boolean);\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure[] createAssignProcedures(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>);\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure[] createUnassignProceduresForDisabling(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure[] createUnassignProceduresForClosingExcessRegionReplicas(org.apache.hadoop.hbase.TableName, int);\n  public org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure createSplitProcedure(org.apache.hadoop.hbase.client.RegionInfo, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure createMergeProcedure(org.apache.hadoop.hbase.client.RegionInfo...) throws java.io.IOException;\n  public void deleteTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionResponse reportRegionStateTransition(org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$ReportRegionStateTransitionRequest) throws org.apache.hadoop.hbase.PleaseHoldException;\n  public void reportOnlineRegions(org.apache.hadoop.hbase.ServerName, java.util.Set<byte[]>);\n  public org.apache.hadoop.hbase.master.assignment.AssignmentManager$RegionInTransitionStat computeRegionInTransitionStat();\n  public void joinCluster() throws java.io.IOException;\n  public void processOfflineRegions();\n  public org.apache.hadoop.hbase.client.RegionInfo loadRegionFromMeta(java.lang.String) throws org.apache.hadoop.hbase.UnknownRegionException;\n  public int getNumRegionsOpened();\n  public long submitServerCrash(org.apache.hadoop.hbase.ServerName, boolean, boolean);\n  public void offlineRegion(org.apache.hadoop.hbase.client.RegionInfo);\n  public void onlineRegion(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName);\n  public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>> getSnapShotOfAssignment(java.util.Collection<org.apache.hadoop.hbase.client.RegionInfo>);\n  public org.apache.hadoop.hbase.util.Pair<java.lang.Integer, java.lang.Integer> getReopenStatus(org.apache.hadoop.hbase.TableName);\n  public boolean hasRegionsInTransition();\n  public java.util.List<org.apache.hadoop.hbase.master.assignment.RegionStateNode> getRegionsInTransition();\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getAssignedRegions();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo(byte[]);\n  public void regionClosedAbnormally(org.apache.hadoop.hbase.master.assignment.RegionStateNode) throws java.io.IOException;\n  public void markRegionAsSplit(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void markRegionAsMerged(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getExcludedServersForSystemTable();\n  public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.Set<byte[]>> getRSReports();\n  public org.apache.hadoop.hbase.client.RegionStatesCount getRegionStatesCount(org.apache.hadoop.hbase.TableName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/AssignmentManagerUtil.class;;;final class org.apache.hadoop.hbase.master.assignment.AssignmentManagerUtil {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/CloseRegionProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.CloseRegionProcedure extends org.apache.hadoop.hbase.master.assignment.RegionRemoteProcedureBase {\n  public org.apache.hadoop.hbase.master.assignment.CloseRegionProcedure();\n  public org.apache.hadoop.hbase.master.assignment.CloseRegionProcedure(org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation newRemoteOperation();\n}\n;;;No. This is a Java class definition for a procedure used in Apache HBase, but it is not a message definition that can be placed on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/GCMergedRegionsProcedure$1.class;;;class org.apache.hadoop.hbase.master.assignment.GCMergedRegionsProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/assignment/GCMergedRegionsProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.GCMergedRegionsProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$GCMergedRegionsState> {\n  public org.apache.hadoop.hbase.master.assignment.GCMergedRegionsProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.master.assignment.GCMergedRegionsProcedure();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public org.apache.hadoop.hbase.TableName getTableName();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a procedure class in HBase, used for garbage collecting merged regions.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/GCMultipleMergedRegionsProcedure$1.class;;;class org.apache.hadoop.hbase.master.assignment.GCMultipleMergedRegionsProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/GCMultipleMergedRegionsProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.GCMultipleMergedRegionsProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$GCMergedRegionsState> {\n  public org.apache.hadoop.hbase.master.assignment.GCMultipleMergedRegionsProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>);\n  public org.apache.hadoop.hbase.master.assignment.GCMultipleMergedRegionsProcedure();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public org.apache.hadoop.hbase.TableName getTableName();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/GCRegionProcedure$1.class;;;class org.apache.hadoop.hbase.master.assignment.GCRegionProcedure$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/GCRegionProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.GCRegionProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineRegionProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$GCRegionState> {\n  public org.apache.hadoop.hbase.master.assignment.GCRegionProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.master.assignment.GCRegionProcedure();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure$1.class;;;class org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$MergeTableRegionsState> {\n  public org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure();\n  public org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo[], boolean) throws java.io.IOException;\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/MoveRegionProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.MoveRegionProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineRegionProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$MoveRegionState> {\n  public org.apache.hadoop.hbase.master.assignment.MoveRegionProcedure();\n  public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public boolean abort(java.lang.Object);\n}\n;;;No, this class appears to be a procedure definition for the HBase master, not a message definition for a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/assignment/OpenRegionProcedure$1.class;;;class org.apache.hadoop.hbase.master.assignment.OpenRegionProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/OpenRegionProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.OpenRegionProcedure extends org.apache.hadoop.hbase.master.assignment.RegionRemoteProcedureBase {\n  public org.apache.hadoop.hbase.master.assignment.OpenRegionProcedure();\n  public org.apache.hadoop.hbase.master.assignment.OpenRegionProcedure(org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation newRemoteOperation();\n}\n;;;No, this class is not a message definition. It does not include any fields that would define the content of a message that could be put on a message queue. It appears to be a Java class that extends another class and includes several public methods with various return types.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/RegionRemoteProcedureBase$1.class;;;class org.apache.hadoop.hbase.master.assignment.RegionRemoteProcedureBase$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/RegionRemoteProcedureBase.class;;;public abstract class org.apache.hadoop.hbase.master.assignment.RegionRemoteProcedureBase extends org.apache.hadoop.hbase.procedure2.Procedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> implements org.apache.hadoop.hbase.master.procedure.TableProcedureInterface, org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName> {\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public void remoteOperationCompleted(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public void remoteOperationFailed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.procedure2.RemoteProcedureException);\n  public void remoteCallFailed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName, java.io.IOException);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public boolean storeInDispatchedQueue();\n  public java.lang.String getProcName();\n  public void remoteOperationFailed(java.lang.Object, org.apache.hadoop.hbase.procedure2.RemoteProcedureException);\n  public void remoteOperationCompleted(java.lang.Object);\n  public void remoteCallFailed(java.lang.Object, java.lang.Object, java.io.IOException);\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/RegionStateNode$AssignmentProcedureEvent.class;;;final class org.apache.hadoop.hbase.master.assignment.RegionStateNode$AssignmentProcedureEvent extends org.apache.hadoop.hbase.procedure2.ProcedureEvent<org.apache.hadoop.hbase.client.RegionInfo> {\n  public org.apache.hadoop.hbase.master.assignment.RegionStateNode$AssignmentProcedureEvent(org.apache.hadoop.hbase.client.RegionInfo);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/RegionStateNode.class;;;public class org.apache.hadoop.hbase.master.assignment.RegionStateNode implements java.lang.Comparable<org.apache.hadoop.hbase.master.assignment.RegionStateNode> {\n  public boolean setState(org.apache.hadoop.hbase.master.RegionState$State, org.apache.hadoop.hbase.master.RegionState$State...);\n  public org.apache.hadoop.hbase.ServerName offline();\n  public void transitionState(org.apache.hadoop.hbase.master.RegionState$State, org.apache.hadoop.hbase.master.RegionState$State...) throws org.apache.hadoop.hbase.exceptions.UnexpectedStateException;\n  public boolean isInState(org.apache.hadoop.hbase.master.RegionState$State...);\n  public boolean isStuck();\n  public boolean isInTransition();\n  public boolean isSplit();\n  public long getLastUpdate();\n  public void setLastHost(org.apache.hadoop.hbase.ServerName);\n  public void setOpenSeqNum(long);\n  public org.apache.hadoop.hbase.ServerName setRegionLocation(org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure setProcedure(org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure);\n  public void unsetProcedure(org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure);\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure getProcedure();\n  public org.apache.hadoop.hbase.procedure2.ProcedureEvent<?> getProcedureEvent();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public org.apache.hadoop.hbase.TableName getTable();\n  public boolean isSystemTable();\n  public org.apache.hadoop.hbase.ServerName getLastHost();\n  public org.apache.hadoop.hbase.ServerName getRegionLocation();\n  public org.apache.hadoop.hbase.master.RegionState$State getState();\n  public long getOpenSeqNum();\n  public int getFormatVersion();\n  public org.apache.hadoop.hbase.master.RegionState toRegionState();\n  public int compareTo(org.apache.hadoop.hbase.master.assignment.RegionStateNode);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n  public java.lang.String toShortString();\n  public java.lang.String toDescriptiveString();\n  public void checkOnline() throws org.apache.hadoop.hbase.client.DoNotRetryRegionException;\n  public void lock();\n  public void unlock();\n  public int compareTo(java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/RegionStateStore$1.class;;;class org.apache.hadoop.hbase.master.assignment.RegionStateStore$1 implements org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor {\n  public boolean visit(org.apache.hadoop.hbase.client.Result) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/RegionStateStore$RegionStateVisitor.class;;;public interface org.apache.hadoop.hbase.master.assignment.RegionStateStore$RegionStateVisitor {\n  public abstract void visitRegionState(org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.master.RegionState$State, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName, long);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/RegionStateStore.class;;;public class org.apache.hadoop.hbase.master.assignment.RegionStateStore {\n  public org.apache.hadoop.hbase.master.assignment.RegionStateStore(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.region.MasterRegion);\n  public void visitMeta(org.apache.hadoop.hbase.master.assignment.RegionStateStore$RegionStateVisitor) throws java.io.IOException;\n  public void visitMetaForRegion(java.lang.String, org.apache.hadoop.hbase.master.assignment.RegionStateStore$RegionStateVisitor) throws java.io.IOException;\n  public static void visitMetaEntry(org.apache.hadoop.hbase.master.assignment.RegionStateStore$RegionStateVisitor, org.apache.hadoop.hbase.client.Result) throws java.io.IOException;\n  public void splitRegion(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void mergeRegions(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public boolean hasMergeRegions(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getMergeRegions(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void deleteMergeQualifiers(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void deleteRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void deleteRegions(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>) throws java.io.IOException;\n  public void overwriteRegions(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, int) throws java.io.IOException;\n  public void removeRegionReplicas(org.apache.hadoop.hbase.TableName, int, int) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.master.RegionState$State getRegionState(org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.client.RegionInfo);\n  public static byte[] getStateColumn(int);\n}\n;;;No. This is a class definition for a Java class and does not represent a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/RegionStates$1.class;;;class org.apache.hadoop.hbase.master.assignment.RegionStates$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/RegionStates$RegionFailedOpen.class;;;public final class org.apache.hadoop.hbase.master.assignment.RegionStates$RegionFailedOpen {\n  public org.apache.hadoop.hbase.master.assignment.RegionStates$RegionFailedOpen(org.apache.hadoop.hbase.master.assignment.RegionStateNode);\n  public org.apache.hadoop.hbase.master.assignment.RegionStateNode getRegionStateNode();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public int incrementAndGetRetries();\n  public int getRetries();\n  public void setException(java.lang.Exception);\n  public java.lang.Exception getException();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/RegionStates$RegionStateStampComparator.class;;;class org.apache.hadoop.hbase.master.assignment.RegionStates$RegionStateStampComparator implements java.util.Comparator<org.apache.hadoop.hbase.master.RegionState> {\n  public int compare(org.apache.hadoop.hbase.master.RegionState, org.apache.hadoop.hbase.master.RegionState);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/RegionStates.class;;;public class org.apache.hadoop.hbase.master.assignment.RegionStates {\n  public static final org.apache.hadoop.hbase.master.assignment.RegionStates$RegionStateStampComparator REGION_STATE_STAMP_COMPARATOR;\n  public org.apache.hadoop.hbase.master.assignment.RegionStates();\n  public void clear();\n  public boolean isRegionInRegionStates(org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.master.assignment.RegionStateNode getOrCreateRegionStateNode(org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.master.assignment.RegionStateNode getRegionStateNodeFromName(byte[]);\n  public org.apache.hadoop.hbase.master.assignment.RegionStateNode getRegionStateNode(org.apache.hadoop.hbase.client.RegionInfo);\n  public void deleteRegion(org.apache.hadoop.hbase.client.RegionInfo);\n  public void deleteRegions(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>);\n  public java.util.Collection<org.apache.hadoop.hbase.master.assignment.RegionStateNode> getRegionStateNodes();\n  public java.util.ArrayList<org.apache.hadoop.hbase.master.RegionState> getRegionStates();\n  public org.apache.hadoop.hbase.master.RegionState getRegionState(org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.master.RegionState getRegionState(java.lang.String);\n  public boolean hasTableRegionStates(org.apache.hadoop.hbase.TableName);\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getRegionsOfTable(org.apache.hadoop.hbase.TableName);\n  public java.util.List<org.apache.hadoop.hbase.HRegionLocation> getRegionsOfTableForReopen(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.HRegionLocation checkReopened(org.apache.hadoop.hbase.HRegionLocation);\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getRegionsOfTableForEnabling(org.apache.hadoop.hbase.TableName);\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getRegionsOfTableForDeleting(org.apache.hadoop.hbase.TableName);\n  public void metaLogSplitting(org.apache.hadoop.hbase.ServerName);\n  public void metaLogSplit(org.apache.hadoop.hbase.ServerName);\n  public void logSplitting(org.apache.hadoop.hbase.ServerName);\n  public void logSplit(org.apache.hadoop.hbase.ServerName);\n  public void updateRegionState(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.master.RegionState$State);\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getAssignedRegions();\n  public boolean isRegionInState(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.master.RegionState$State...);\n  public boolean isRegionOnline(org.apache.hadoop.hbase.client.RegionInfo);\n  public boolean isRegionOffline(org.apache.hadoop.hbase.client.RegionInfo);\n  public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>> getSnapShotOfAssignment(java.util.Collection<org.apache.hadoop.hbase.client.RegionInfo>);\n  public java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName> getRegionAssignments();\n  public java.util.Map<org.apache.hadoop.hbase.master.RegionState$State, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>> getRegionByStateOfTable(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.ServerName getRegionServerOfRegion(org.apache.hadoop.hbase.client.RegionInfo);\n  public java.util.Map<org.apache.hadoop.hbase.TableName, java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>>> getAssignmentsForBalancer(org.apache.hadoop.hbase.master.TableStateManager, java.util.List<org.apache.hadoop.hbase.ServerName>);\n  public boolean hasRegionsInTransition();\n  public boolean isRegionInTransition(org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.master.RegionState getRegionTransitionState(org.apache.hadoop.hbase.client.RegionInfo);\n  public java.util.List<org.apache.hadoop.hbase.master.assignment.RegionStateNode> getRegionsInTransition();\n  public int getRegionsInTransitionCount();\n  public java.util.List<org.apache.hadoop.hbase.master.RegionState> getRegionsStateInTransition();\n  public java.util.SortedSet<org.apache.hadoop.hbase.master.RegionState> getRegionsInTransitionOrderedByTimestamp();\n  public void addToOfflineRegions(org.apache.hadoop.hbase.master.assignment.RegionStateNode);\n  public void removeFromOfflineRegions(org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.master.assignment.RegionStates$RegionFailedOpen addToFailedOpen(org.apache.hadoop.hbase.master.assignment.RegionStateNode);\n  public org.apache.hadoop.hbase.master.assignment.RegionStates$RegionFailedOpen getFailedOpen(org.apache.hadoop.hbase.client.RegionInfo);\n  public void removeFromFailedOpen(org.apache.hadoop.hbase.client.RegionInfo);\n  public java.util.List<org.apache.hadoop.hbase.master.RegionState> getRegionFailedOpen();\n  public org.apache.hadoop.hbase.master.assignment.ServerStateNode getOrCreateServer(org.apache.hadoop.hbase.ServerName);\n  public void removeServer(org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.master.assignment.ServerStateNode getServerNode(org.apache.hadoop.hbase.ServerName);\n  public double getAverageLoad();\n  public org.apache.hadoop.hbase.master.assignment.ServerStateNode addRegionToServer(org.apache.hadoop.hbase.master.assignment.RegionStateNode);\n  public org.apache.hadoop.hbase.master.assignment.ServerStateNode removeRegionFromServer(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.master.assignment.RegionStateNode);\n  public static java.lang.String regionNamesToString(java.util.Collection<byte[]>);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/RegionTransitionProcedure.class;;;public abstract class org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure extends org.apache.hadoop.hbase.procedure2.Procedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> implements org.apache.hadoop.hbase.master.procedure.TableProcedureInterface, org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName> {\n  public org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure();\n  public org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure(org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public void setRegionInfo(org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public boolean isMeta();\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public java.lang.String getProcName();\n  public org.apache.hadoop.hbase.master.assignment.RegionStateNode getRegionState(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public abstract java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public synchronized void remoteCallFailed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName, java.io.IOException);\n  public void remoteOperationCompleted(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public void remoteOperationFailed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.procedure2.RemoteProcedureException);\n  public void remoteOperationFailed(java.lang.Object, org.apache.hadoop.hbase.procedure2.RemoteProcedureException);\n  public void remoteOperationCompleted(java.lang.Object);\n  public void remoteCallFailed(java.lang.Object, java.lang.Object, java.io.IOException);\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;Yes, the class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/ServerStateNode.class;;;public class org.apache.hadoop.hbase.master.assignment.ServerStateNode implements java.lang.Comparable<org.apache.hadoop.hbase.master.assignment.ServerStateNode> {\n  public org.apache.hadoop.hbase.master.assignment.ServerStateNode(org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public org.apache.hadoop.hbase.master.assignment.ServerState getState();\n  public boolean isInState(org.apache.hadoop.hbase.master.assignment.ServerState...);\n  public int getRegionCount();\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getRegionInfoList();\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getSystemRegionInfoList();\n  public void addRegion(org.apache.hadoop.hbase.master.assignment.RegionStateNode);\n  public void removeRegion(org.apache.hadoop.hbase.master.assignment.RegionStateNode);\n  public java.util.concurrent.locks.Lock readLock();\n  public java.util.concurrent.locks.Lock writeLock();\n  public int compareTo(org.apache.hadoop.hbase.master.assignment.ServerStateNode);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n  public int compareTo(java.lang.Object);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition that defines the behavior of a certain object type.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure$1.class;;;class org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure$StoreFileSplitter.class;;;class org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure$StoreFileSplitter implements java.util.concurrent.Callable<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path>> {\n  public org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure$StoreFileSplitter(org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure, org.apache.hadoop.hbase.regionserver.HRegionFileSystem, byte[], org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path> call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No, this class is a callable that might be used in a concurrent execution context. It's not a message definition that can be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/assignment/SplitTableRegionProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineRegionProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$SplitTableRegionState> {\n  public org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure();\n  public org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo, byte[]) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.RegionInfo getDaughterOneRI();\n  public org.apache.hadoop.hbase.client.RegionInfo getDaughterTwoRI();\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public boolean prepareSplitRegion(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv) throws java.io.IOException;\n  public void createDaughterRegions(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv) throws java.io.IOException;\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/TransitRegionStateProcedure$1.class;;;class org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/TransitRegionStateProcedure$TransitionType.class;;;public final class org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure$TransitionType extends java.lang.Enum<org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure$TransitionType> {\n  public static final org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure$TransitionType ASSIGN;\n  public static final org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure$TransitionType UNASSIGN;\n  public static final org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure$TransitionType MOVE;\n  public static final org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure$TransitionType REOPEN;\n  public static org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure$TransitionType[] values();\n  public static org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure$TransitionType valueOf(java.lang.String);\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is an enumeration class that defines the different types of transition that can occur in the process of assigning, unassigning, moving, and reopening region states in HBase.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an enumeration class that defines different types of transitions for a region state in Apache HBase.;;;N
org/apache/hadoop/hbase/master/assignment/TransitRegionStateProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineRegionProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$RegionStateTransitionState> {\n  public org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void reportTransition(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.master.assignment.RegionStateNode, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode, long, long) throws java.io.IOException;\n  public void serverCrashed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.master.assignment.RegionStateNode, org.apache.hadoop.hbase.ServerName, boolean) throws java.io.IOException;\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public static org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure assign(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName);\n  public static org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure assign(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo, boolean, org.apache.hadoop.hbase.ServerName);\n  public static org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure unassign(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo);\n  public static org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure reopen(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo);\n  public static org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure move(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/assignment/UnassignProcedure$1.class;;;class org.apache.hadoop.hbase.master.assignment.UnassignProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/assignment/UnassignProcedure.class;;;public class org.apache.hadoop.hbase.master.assignment.UnassignProcedure extends org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure {\n  public org.apache.hadoop.hbase.master.assignment.UnassignProcedure();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/balancer/BalancerChore.class;;;public class org.apache.hadoop.hbase.master.balancer.BalancerChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.master.balancer.BalancerChore(org.apache.hadoop.hbase.master.HMaster);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/balancer/ClusterStatusChore.class;;;public class org.apache.hadoop.hbase.master.balancer.ClusterStatusChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.master.balancer.ClusterStatusChore(org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.hbase.master.LoadBalancer);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/balancer/MaintenanceLoadBalancer.class;;;public class org.apache.hadoop.hbase.master.balancer.MaintenanceLoadBalancer implements org.apache.hadoop.hbase.master.LoadBalancer {\n  public org.apache.hadoop.hbase.master.balancer.MaintenanceLoadBalancer();\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public void updateClusterMetrics(org.apache.hadoop.hbase.ClusterMetrics);\n  public void setClusterInfoProvider(org.apache.hadoop.hbase.master.balancer.ClusterInfoProvider);\n  public java.util.List<org.apache.hadoop.hbase.master.RegionPlan> balanceCluster(java.util.Map<org.apache.hadoop.hbase.TableName, java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>>>) throws java.io.IOException;\n  public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>> roundRobinAssignment(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException;\n  public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>> retainAssignment(java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName>, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.ServerName randomAssignment(org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException;\n  public void initialize();\n  public void regionOnline(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName);\n  public void regionOffline(org.apache.hadoop.hbase.client.RegionInfo);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public void postMasterStartupInitialize();\n  public void updateBalancerStatus(boolean);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/balancer/MasterClusterInfoProvider.class;;;public class org.apache.hadoop.hbase.master.balancer.MasterClusterInfoProvider implements org.apache.hadoop.hbase.master.balancer.ClusterInfoProvider {\n  public org.apache.hadoop.hbase.master.balancer.MasterClusterInfoProvider(org.apache.hadoop.hbase.master.MasterServices);\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.client.Connection getConnection();\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getAssignedRegions();\n  public void unassign(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableDescriptor getTableDescriptor(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public boolean hasRegionReplica(java.util.Collection<org.apache.hadoop.hbase.client.RegionInfo>) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getOnlineServersList();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getOnlineServersListWithPredicator(java.util.List<org.apache.hadoop.hbase.ServerName>, java.util.function.Predicate<org.apache.hadoop.hbase.ServerMetrics>);\n  public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>> getSnapShotOfAssignment(java.util.Collection<org.apache.hadoop.hbase.client.RegionInfo>);\n  public int getNumberOfTables() throws java.io.IOException;\n  public boolean isOffPeakHour();\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public void recordBalancerDecision(java.util.function.Supplier<org.apache.hadoop.hbase.client.BalancerDecision>);\n  public void recordBalancerRejection(java.util.function.Supplier<org.apache.hadoop.hbase.client.BalancerRejection>);\n  public org.apache.hadoop.hbase.ServerMetrics getLoad(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/master/cleaner/BaseFileCleanerDelegate.class;;;public abstract class org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate extends org.apache.hadoop.hbase.BaseConfigurable implements org.apache.hadoop.hbase.master.cleaner.FileCleanerDelegate {\n  public org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate();\n  public java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>);\n  public void init(java.util.Map<java.lang.String, java.lang.Object>);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/cleaner/BaseHFileCleanerDelegate.class;;;public abstract class org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate extends org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate {\n  public org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate();\n  public void stop(java.lang.String);\n  public boolean isStopped();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/cleaner/BaseLogCleanerDelegate.class;;;public abstract class org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate extends org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate {\n  public org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate();\n  public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/cleaner/BaseTimeToLiveFileCleaner.class;;;public abstract class org.apache.hadoop.hbase.master.cleaner.BaseTimeToLiveFileCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate {\n  public org.apache.hadoop.hbase.master.cleaner.BaseTimeToLiveFileCleaner();\n  public final void setConf(org.apache.hadoop.conf.Configuration);\n  public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus);\n  public void stop(java.lang.String);\n  public boolean isStopped();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/cleaner/CleanerChore$1.class;;;class org.apache.hadoop.hbase.master.cleaner.CleanerChore$1 implements java.util.Comparator<org.apache.hadoop.fs.FileStatus> {\n  public int compare(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileStatus);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/cleaner/CleanerChore$Action.class;;;interface org.apache.hadoop.hbase.master.cleaner.CleanerChore$Action<T> {\n  public abstract T act() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/cleaner/CleanerChore.class;;;public abstract class org.apache.hadoop.hbase.master.cleaner.CleanerChore<T extends org.apache.hadoop.hbase.master.cleaner.FileCleanerDelegate> extends org.apache.hadoop.hbase.ScheduledChore {\n  public static final java.lang.String CHORE_POOL_SIZE;\n  public static final java.lang.String LOG_CLEANER_CHORE_SIZE;\n  public org.apache.hadoop.hbase.master.cleaner.CleanerChore(java.lang.String, int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.master.cleaner.DirScanPool);\n  public org.apache.hadoop.hbase.master.cleaner.CleanerChore(java.lang.String, int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.master.cleaner.DirScanPool, java.util.Map<java.lang.String, java.lang.Object>, java.util.List<org.apache.hadoop.fs.Path>);\n  public boolean runCleaner();\n  public synchronized void cleanup();\n  public boolean setEnabled(boolean);\n  public boolean getEnabled();\n}\n;;;No. This class is not a message definition. It appears to be a utility class for cleaning files in Hadoop. It extends a scheduled chore and contains methods for running the cleaner, cleaning up, and enabling/disabling the cleaner.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/DirScanPool$Type.class;;;final class org.apache.hadoop.hbase.master.cleaner.DirScanPool$Type extends java.lang.Enum<org.apache.hadoop.hbase.master.cleaner.DirScanPool$Type> {\n  public static final org.apache.hadoop.hbase.master.cleaner.DirScanPool$Type LOG_CLEANER;\n  public static final org.apache.hadoop.hbase.master.cleaner.DirScanPool$Type HFILE_CLEANER;\n  public static org.apache.hadoop.hbase.master.cleaner.DirScanPool$Type[] values();\n  public static org.apache.hadoop.hbase.master.cleaner.DirScanPool$Type valueOf(java.lang.String);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is an enumeration class in the Apache HBase codebase.;;;N;;;No. It is an enumeration class and not a task definition.;;;N
org/apache/hadoop/hbase/master/cleaner/DirScanPool.class;;;public class org.apache.hadoop.hbase.master.cleaner.DirScanPool implements org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public synchronized void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public synchronized void shutdownNow();\n  public int getSize();\n  public static org.apache.hadoop.hbase.master.cleaner.DirScanPool getHFileCleanerScanPool(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.hbase.master.cleaner.DirScanPool getHFileCleanerScanPool(java.lang.String);\n  public static org.apache.hadoop.hbase.master.cleaner.DirScanPool getLogCleanerScanPool(org.apache.hadoop.conf.Configuration);\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a Java class that defines methods for managing a thread pool for scanning directories in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/FileCleanerDelegate.class;;;public interface org.apache.hadoop.hbase.master.cleaner.FileCleanerDelegate extends org.apache.hadoop.conf.Configurable,org.apache.hadoop.hbase.Stoppable {\n  public abstract java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>);\n  public abstract void init(java.util.Map<java.lang.String, java.lang.Object>);\n  public default void preClean();\n  public default boolean isEmptyDirDeletable(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/HFileCleaner$1.class;;;class org.apache.hadoop.hbase.master.cleaner.HFileCleaner$1 extends java.lang.Thread {\n  public void run();\n}\n;;;No. This is a class definition for a thread and not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/HFileCleaner$2.class;;;class org.apache.hadoop.hbase.master.cleaner.HFileCleaner$2 extends java.lang.Thread {\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/HFileCleaner$3.class;;;final class org.apache.hadoop.hbase.master.cleaner.HFileCleaner$3 implements java.util.Comparator<org.apache.hadoop.hbase.master.cleaner.HFileCleaner$HFileDeleteTask> {\n  public int compare(org.apache.hadoop.hbase.master.cleaner.HFileCleaner$HFileDeleteTask, org.apache.hadoop.hbase.master.cleaner.HFileCleaner$HFileDeleteTask);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No. This class is a Java class that implements the Comparator interface and is not related to message definitions or message queues directly.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/HFileCleaner$HFileDeleteTask.class;;;final class org.apache.hadoop.hbase.master.cleaner.HFileCleaner$HFileDeleteTask {\n  public org.apache.hadoop.hbase.master.cleaner.HFileCleaner$HFileDeleteTask(org.apache.hadoop.fs.FileStatus, long);\n  public synchronized void setResult(boolean);\n  public synchronized boolean getResult(long);\n}\n;;;No.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/master/cleaner/HFileCleaner.class;;;public class org.apache.hadoop.hbase.master.cleaner.HFileCleaner extends org.apache.hadoop.hbase.master.cleaner.CleanerChore<org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate> implements org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public static final java.lang.String MASTER_HFILE_CLEANER_PLUGINS;\n  public static final java.lang.String HFILE_DELETE_THROTTLE_THRESHOLD;\n  public static final int DEFAULT_HFILE_DELETE_THROTTLE_THRESHOLD;\n  public static final java.lang.String LARGE_HFILE_QUEUE_INIT_SIZE;\n  public static final int DEFAULT_LARGE_HFILE_QUEUE_INIT_SIZE;\n  public static final java.lang.String SMALL_HFILE_QUEUE_INIT_SIZE;\n  public static final int DEFAULT_SMALL_HFILE_QUEUE_INIT_SIZE;\n  public static final java.lang.String LARGE_HFILE_DELETE_THREAD_NUMBER;\n  public static final int DEFAULT_LARGE_HFILE_DELETE_THREAD_NUMBER;\n  public static final java.lang.String SMALL_HFILE_DELETE_THREAD_NUMBER;\n  public static final int DEFAULT_SMALL_HFILE_DELETE_THREAD_NUMBER;\n  public static final java.lang.String HFILE_DELETE_THREAD_TIMEOUT_MSEC;\n  public static final java.lang.String HFILE_DELETE_THREAD_CHECK_INTERVAL_MSEC;\n  public static final java.lang.String HFILE_CLEANER_CUSTOM_PATHS;\n  public static final java.lang.String HFILE_CLEANER_CUSTOM_PATHS_PLUGINS;\n  public static final java.lang.String CUSTOM_POOL_SIZE;\n  public org.apache.hadoop.hbase.master.cleaner.HFileCleaner(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.master.cleaner.DirScanPool);\n  public org.apache.hadoop.hbase.master.cleaner.HFileCleaner(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.master.cleaner.DirScanPool, java.util.Map<java.lang.String, java.lang.Object>);\n  public org.apache.hadoop.hbase.master.cleaner.HFileCleaner(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.master.cleaner.DirScanPool, java.util.Map<java.lang.String, java.lang.Object>, java.util.List<org.apache.hadoop.fs.Path>);\n  public org.apache.hadoop.hbase.master.cleaner.HFileCleaner(java.lang.String, int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.master.cleaner.DirScanPool, java.util.Map<java.lang.String, java.lang.Object>, java.util.List<org.apache.hadoop.fs.Path>);\n  public java.util.List<org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate> getDelegatesForTesting();\n  public int deleteFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>);\n  public synchronized void cleanup();\n  public java.util.List<java.lang.Thread> getCleanerThreads();\n  public long getNumOfDeletedLargeFiles();\n  public long getNumOfDeletedSmallFiles();\n  public long getLargeQueueInitSize();\n  public long getSmallQueueInitSize();\n  public long getThrottlePoint();\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public synchronized void cancel(boolean);\n}\n;;;Yes, it is a message definition.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/cleaner/HFileLinkCleaner.class;;;public class org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate {\n  public org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner();\n  public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n}\n;;;No. This is a class definition for a cleaner that is part of the HBase system. It does have methods that take input parameters and return output, but it is not a message definition in the sense of being a description of a message that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/LogCleaner$1.class;;;class org.apache.hadoop.hbase.master.cleaner.LogCleaner$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/LogCleaner$CleanerContext.class;;;final class org.apache.hadoop.hbase.master.cleaner.LogCleaner$CleanerContext {\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be a class for defining the context of the LogCleaner used in the Apache HBase master.;;;N
org/apache/hadoop/hbase/master/cleaner/LogCleaner.class;;;public class org.apache.hadoop.hbase.master.cleaner.LogCleaner extends org.apache.hadoop.hbase.master.cleaner.CleanerChore<org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate> implements org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public static final java.lang.String OLD_WALS_CLEANER_THREAD_SIZE;\n  public static final int DEFAULT_OLD_WALS_CLEANER_THREAD_SIZE;\n  public static final java.lang.String OLD_WALS_CLEANER_THREAD_TIMEOUT_MSEC;\n  public org.apache.hadoop.hbase.master.cleaner.LogCleaner(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.master.cleaner.DirScanPool, java.util.Map<java.lang.String, java.lang.Object>);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public synchronized void cleanup();\n  public synchronized void cancel(boolean);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/ReplicationBarrierCleaner.class;;;public class org.apache.hadoop.hbase.master.cleaner.ReplicationBarrierCleaner extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.master.cleaner.ReplicationBarrierCleaner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.master.replication.ReplicationPeerManager);\n  public synchronized void chore();\n}\n;;;No. This is a class definition for a scheduled chore in the HBase database management system. It does not define a message that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/SnapshotCleanerChore.class;;;public class org.apache.hadoop.hbase.master.cleaner.SnapshotCleanerChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.master.cleaner.SnapshotCleanerChore(org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.master.snapshot.SnapshotManager);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/TimeToLiveHFileCleaner.class;;;public class org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseTimeToLiveFileCleaner {\n  public static final java.lang.String TTL_CONF_KEY;\n  public static final long DEFAULT_TTL;\n  public org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/TimeToLiveLogCleaner.class;;;public class org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseTimeToLiveFileCleaner {\n  public static final java.lang.String TTL_CONF_KEY;\n  public static final long DEFAULT_TTL;\n  public org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner();\n}\n;;;No. The class itself is not a message definition. It is a class that extends another class and has some constants and a constructor.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/TimeToLiveMasterLocalStoreHFileCleaner.class;;;public class org.apache.hadoop.hbase.master.cleaner.TimeToLiveMasterLocalStoreHFileCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseTimeToLiveFileCleaner {\n  public static final java.lang.String TTL_CONF_KEY;\n  public static final long DEFAULT_TTL;\n  public org.apache.hadoop.hbase.master.cleaner.TimeToLiveMasterLocalStoreHFileCleaner();\n}\n;;;No.;;;N;;;No, it is not a task definition for a task queue.;;;N
org/apache/hadoop/hbase/master/cleaner/TimeToLiveMasterLocalStoreWALCleaner.class;;;public class org.apache.hadoop.hbase.master.cleaner.TimeToLiveMasterLocalStoreWALCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseTimeToLiveFileCleaner {\n  public static final java.lang.String TTL_CONF_KEY;\n  public static final long DEFAULT_TTL;\n  public org.apache.hadoop.hbase.master.cleaner.TimeToLiveMasterLocalStoreWALCleaner();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/cleaner/TimeToLiveProcedureWALCleaner.class;;;public class org.apache.hadoop.hbase.master.cleaner.TimeToLiveProcedureWALCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseTimeToLiveFileCleaner {\n  public static final java.lang.String TTL_CONF_KEY;\n  public static final long DEFAULT_TTL;\n  public org.apache.hadoop.hbase.master.cleaner.TimeToLiveProcedureWALCleaner();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/hbck/HbckChore.class;;;public class org.apache.hadoop.hbase.master.hbck.HbckChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.master.hbck.HbckChore(org.apache.hadoop.hbase.master.MasterServices);\n  public org.apache.hadoop.hbase.master.hbck.HbckReport getLastReport();\n  public boolean runChore();\n  public boolean isDisabled();\n  public boolean isRunning();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/hbck/HbckReport.class;;;public class org.apache.hadoop.hbase.master.hbck.HbckReport {\n  public org.apache.hadoop.hbase.master.hbck.HbckReport();\n  public java.time.Instant getCheckingStartTimestamp();\n  public void setCheckingStartTimestamp(java.time.Instant);\n  public java.time.Instant getCheckingEndTimestamp();\n  public void setCheckingEndTimestamp(java.time.Instant);\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.util.HbckRegionInfo> getRegionInfoMap();\n  public java.util.Set<java.lang.String> getDisabledTableRegions();\n  public java.util.Set<java.lang.String> getSplitParentRegions();\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.ServerName> getOrphanRegionsOnRS();\n  public java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getOrphanRegionsOnFS();\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.ServerName>>> getInconsistentRegions();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/http/MasterDumpServlet.class;;;public class org.apache.hadoop.hbase.master.http.MasterDumpServlet extends org.apache.hadoop.hbase.monitoring.StateDumpServlet {\n  public org.apache.hadoop.hbase.master.http.MasterDumpServlet();\n  public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n}\n;;;No. This is a servlet class that handles HTTP requests and responses in a Hadoop HBase master node. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/MasterRedirectServlet.class;;;public class org.apache.hadoop.hbase.master.http.MasterRedirectServlet extends javax.servlet.http.HttpServlet {\n  public org.apache.hadoop.hbase.master.http.MasterRedirectServlet(org.apache.hadoop.hbase.http.InfoServer, java.lang.String);\n  public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws javax.servlet.ServletException, java.io.IOException;\n}\n;;;No. This class is a servlet, not a message definition, and is used for handling HTTP requests and responses in the Hadoop HBase cluster.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/MasterStatusServlet.class;;;public class org.apache.hadoop.hbase.master.http.MasterStatusServlet extends javax.servlet.http.HttpServlet {\n  public org.apache.hadoop.hbase.master.http.MasterStatusServlet();\n  public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/MetaBrowser$1.class;;;class org.apache.hadoop.hbase.master.http.MetaBrowser$1 {\n}\n;;;No, this class is not a message definition that might be put on a message queue. It appears to be an inner class within the org.apache.hadoop.hbase.master.http.MetaBrowser class.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/MetaBrowser$Results.class;;;public final class org.apache.hadoop.hbase.master.http.MetaBrowser$Results implements java.lang.AutoCloseable, java.lang.Iterable<org.apache.hadoop.hbase.master.http.RegionReplicaInfo> {\n  public boolean hasMoreResults();\n  public void close();\n  public java.util.Iterator<org.apache.hadoop.hbase.master.http.RegionReplicaInfo> iterator();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/MetaBrowser.class;;;public class org.apache.hadoop.hbase.master.http.MetaBrowser {\n  public static final java.lang.String NAME_PARAM;\n  public static final java.lang.String SCAN_LIMIT_PARAM;\n  public static final java.lang.String SCAN_REGION_STATE_PARAM;\n  public static final java.lang.String SCAN_START_PARAM;\n  public static final java.lang.String SCAN_TABLE_PARAM;\n  public static final int SCAN_LIMIT_DEFAULT;\n  public static final int SCAN_LIMIT_MAX;\n  public org.apache.hadoop.hbase.master.http.MetaBrowser(org.apache.hadoop.hbase.client.AsyncConnection, javax.servlet.http.HttpServletRequest);\n  public java.util.List<java.lang.String> getErrorMessages();\n  public java.lang.String getName();\n  public java.lang.Integer getScanLimit();\n  public byte[] getScanStart();\n  public org.apache.hadoop.hbase.master.RegionState$State getScanRegionState();\n  public org.apache.hadoop.hbase.TableName getScanTable();\n  public org.apache.hadoop.hbase.master.http.MetaBrowser$Results getResults();\n  public java.lang.String toString();\n  public java.lang.String buildFirstPageUrl();\n  public java.lang.String buildNextPageUrl(byte[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/RegionReplicaInfo.class;;;public final class org.apache.hadoop.hbase.master.http.RegionReplicaInfo {\n  public static java.util.List<org.apache.hadoop.hbase.master.http.RegionReplicaInfo> from(org.apache.hadoop.hbase.client.Result);\n  public byte[] getRow();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public byte[] getRegionName();\n  public byte[] getStartKey();\n  public byte[] getEndKey();\n  public java.lang.Integer getReplicaId();\n  public org.apache.hadoop.hbase.master.RegionState$State getRegionState();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public long getSeqNum();\n  public org.apache.hadoop.hbase.ServerName getTargetServerName();\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.client.RegionInfo> getMergeRegionInfo();\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.client.RegionInfo> getSplitRegionInfo();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It appears to be a utility class providing various methods for working with RegionReplicaInfo objects used in the HBase Master HTTP API.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/RegionVisualizer$1.class;;;class org.apache.hadoop.hbase.master.http.RegionVisualizer$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be an anonymous inner class within the RegionVisualizer class in the Apache Hadoop HBase project.;;;N
org/apache/hadoop/hbase/master/http/RegionVisualizer$ByteArraySerializer.class;;;final class org.apache.hadoop.hbase.master.http.RegionVisualizer$ByteArraySerializer implements org.apache.hbase.thirdparty.com.google.gson.JsonSerializer<byte[]> {\n  public org.apache.hbase.thirdparty.com.google.gson.JsonElement serialize(byte[], java.lang.reflect.Type, org.apache.hbase.thirdparty.com.google.gson.JsonSerializationContext);\n  public org.apache.hbase.thirdparty.com.google.gson.JsonElement serialize(java.lang.Object, java.lang.reflect.Type, org.apache.hbase.thirdparty.com.google.gson.JsonSerializationContext);\n}\n;;;No. This is a Java class definition that implements a JSON serializer for byte arrays. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/RegionVisualizer$RegionDetails.class;;;public final class org.apache.hadoop.hbase.master.http.RegionVisualizer$RegionDetails {\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.RegionMetrics getRegionMetrics();\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/http/RegionVisualizer$RegionDetailsSerializer.class;;;final class org.apache.hadoop.hbase.master.http.RegionVisualizer$RegionDetailsSerializer implements org.apache.hbase.thirdparty.com.google.gson.JsonSerializer<org.apache.hadoop.hbase.master.http.RegionVisualizer$RegionDetails> {\n  public org.apache.hbase.thirdparty.com.google.gson.JsonElement serialize(org.apache.hadoop.hbase.master.http.RegionVisualizer$RegionDetails, java.lang.reflect.Type, org.apache.hbase.thirdparty.com.google.gson.JsonSerializationContext);\n  public org.apache.hbase.thirdparty.com.google.gson.JsonElement serialize(java.lang.Object, java.lang.reflect.Type, org.apache.hbase.thirdparty.com.google.gson.JsonSerializationContext);\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class definition for a serializer used to convert a specific type of object (RegionDetails) to JSON format.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/RegionVisualizer$SizeAsBytesSerializer.class;;;final class org.apache.hadoop.hbase.master.http.RegionVisualizer$SizeAsBytesSerializer implements org.apache.hbase.thirdparty.com.google.gson.JsonSerializer<org.apache.hadoop.hbase.Size> {\n  public org.apache.hbase.thirdparty.com.google.gson.JsonElement serialize(org.apache.hadoop.hbase.Size, java.lang.reflect.Type, org.apache.hbase.thirdparty.com.google.gson.JsonSerializationContext);\n  public org.apache.hbase.thirdparty.com.google.gson.JsonElement serialize(java.lang.Object, java.lang.reflect.Type, org.apache.hbase.thirdparty.com.google.gson.JsonSerializationContext);\n}\n;;;No;;;N;;;No, it is not a task definition, but rather a Java class implementing a JSON serializer interface.;;;N
org/apache/hadoop/hbase/master/http/RegionVisualizer.class;;;public class org.apache.hadoop.hbase.master.http.RegionVisualizer extends org.apache.hadoop.hbase.util.AbstractHBaseTool {\n  public org.apache.hadoop.hbase.master.http.RegionVisualizer();\n  public org.apache.hadoop.hbase.master.http.RegionVisualizer(org.apache.hadoop.hbase.client.Admin);\n  public org.apache.hadoop.hbase.master.http.RegionVisualizer(org.apache.hadoop.hbase.client.AsyncAdmin);\n  public java.lang.String renderRegionDetails();\n  public static void main(java.lang.String[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/api_v1/ResourceConfigFactory.class;;;public final class org.apache.hadoop.hbase.master.http.api_v1.ResourceConfigFactory {\n  public static org.apache.hbase.thirdparty.org.glassfish.jersey.server.ResourceConfig createResourceConfig(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.master.HMaster);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/api_v1/cluster_metrics/model/ClusterMetrics.class;;;public final class org.apache.hadoop.hbase.master.http.api_v1.cluster_metrics.model.ClusterMetrics {\n  public static org.apache.hadoop.hbase.master.http.api_v1.cluster_metrics.model.ClusterMetrics from(org.apache.hadoop.hbase.ClusterMetrics);\n  public java.lang.String getHBaseVersion();\n  public java.lang.String getClusterId();\n  public org.apache.hadoop.hbase.ServerName getMasterName();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getBackupMasterNames();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/http/api_v1/cluster_metrics/package-info.class;;;interface org.apache.hadoop.hbase.master.http.api_v1.cluster_metrics.package-info {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/api_v1/cluster_metrics/resource/ClusterMetricsResource.class;;;public class org.apache.hadoop.hbase.master.http.api_v1.cluster_metrics.resource.ClusterMetricsResource {\n  public org.apache.hadoop.hbase.master.http.api_v1.cluster_metrics.resource.ClusterMetricsResource(org.apache.hadoop.hbase.master.MasterServices);\n  public org.apache.hadoop.hbase.master.http.api_v1.cluster_metrics.model.ClusterMetrics getBaseMetrics() throws java.util.concurrent.ExecutionException, java.lang.InterruptedException;\n  public java.util.Collection<org.apache.hadoop.hbase.ServerMetrics> getLiveServers() throws java.util.concurrent.ExecutionException, java.lang.InterruptedException;\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getDeadServers() throws java.util.concurrent.ExecutionException, java.lang.InterruptedException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/gson/GsonFactory.class;;;public final class org.apache.hadoop.hbase.master.http.gson.GsonFactory {\n  public static org.apache.hbase.thirdparty.com.google.gson.Gson buildGson();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/gson/GsonSerializationFeature$1.class;;;class org.apache.hadoop.hbase.master.http.gson.GsonSerializationFeature$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/gson/GsonSerializationFeature$Binder.class;;;class org.apache.hadoop.hbase.master.http.gson.GsonSerializationFeature$Binder extends org.apache.hbase.thirdparty.org.glassfish.hk2.utilities.binding.AbstractBinder {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/gson/GsonSerializationFeature.class;;;public class org.apache.hadoop.hbase.master.http.gson.GsonSerializationFeature implements org.apache.hbase.thirdparty.javax.ws.rs.core.Feature {\n  public org.apache.hadoop.hbase.master.http.gson.GsonSerializationFeature();\n  public boolean configure(org.apache.hbase.thirdparty.javax.ws.rs.core.FeatureContext);\n}\n;;;No. This class appears to be a configuration feature for a library and does not define a message format that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/gson/SizeAsBytesSerializer.class;;;final class org.apache.hadoop.hbase.master.http.gson.SizeAsBytesSerializer implements org.apache.hbase.thirdparty.com.google.gson.JsonSerializer<org.apache.hadoop.hbase.Size> {\n  public org.apache.hbase.thirdparty.com.google.gson.JsonElement serialize(org.apache.hadoop.hbase.Size, java.lang.reflect.Type, org.apache.hbase.thirdparty.com.google.gson.JsonSerializationContext);\n  public org.apache.hbase.thirdparty.com.google.gson.JsonElement serialize(java.lang.Object, java.lang.reflect.Type, org.apache.hbase.thirdparty.com.google.gson.JsonSerializationContext);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/jersey/MasterFeature$1.class;;;class org.apache.hadoop.hbase.master.http.jersey.MasterFeature$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/jersey/MasterFeature$Binder.class;;;class org.apache.hadoop.hbase.master.http.jersey.MasterFeature$Binder extends org.apache.hbase.thirdparty.org.glassfish.hk2.utilities.binding.AbstractBinder {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/http/jersey/MasterFeature.class;;;public class org.apache.hadoop.hbase.master.http.jersey.MasterFeature implements org.apache.hbase.thirdparty.javax.ws.rs.core.Feature {\n  public org.apache.hadoop.hbase.master.http.jersey.MasterFeature(org.apache.hadoop.hbase.master.HMaster);\n  public boolean configure(org.apache.hbase.thirdparty.javax.ws.rs.core.FeatureContext);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/janitor/CatalogJanitor$SplitParentFirstComparator.class;;;class org.apache.hadoop.hbase.master.janitor.CatalogJanitor$SplitParentFirstComparator implements java.util.Comparator<org.apache.hadoop.hbase.client.RegionInfo> {\n  public int compare(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/janitor/CatalogJanitor.class;;;public class org.apache.hadoop.hbase.master.janitor.CatalogJanitor extends org.apache.hadoop.hbase.ScheduledChore {\n  public static final int DEFAULT_HBASE_CATALOGJANITOR_INTERVAL;\n  public org.apache.hadoop.hbase.master.janitor.CatalogJanitor(org.apache.hadoop.hbase.master.MasterServices);\n  public boolean setEnabled(boolean);\n  public boolean getEnabled();\n  public int scan() throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.janitor.CatalogJanitorReport getLastReport();\n  public static void main(java.lang.String[]) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/janitor/CatalogJanitorReport.class;;;public class org.apache.hadoop.hbase.master.janitor.CatalogJanitorReport {\n  public org.apache.hadoop.hbase.master.janitor.CatalogJanitorReport();\n  public long getCreateTime();\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo>> getHoles();\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo>> getOverlaps();\n  public java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.Result> getMergedRegions();\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName>> getUnknownServers();\n  public java.util.List<byte[]> getEmptyRegionInfo();\n  public boolean isEmpty();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/janitor/MetaFixer$Either.class;;;class org.apache.hadoop.hbase.master.janitor.MetaFixer$Either<L, R> {\n  public static <L, R> org.apache.hadoop.hbase.master.janitor.MetaFixer$Either<L, R> ofLeft(L);\n  public static <L, R> org.apache.hadoop.hbase.master.janitor.MetaFixer$Either<L, R> ofRight(R);\n  public boolean hasLeft();\n  public L getLeft();\n  public boolean hasRight();\n  public R getRight();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/janitor/MetaFixer.class;;;public class org.apache.hadoop.hbase.master.janitor.MetaFixer {\n  public org.apache.hadoop.hbase.master.janitor.MetaFixer(org.apache.hadoop.hbase.master.MasterServices);\n  public void fix() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/janitor/ReportMakingVisitor$1.class;;;class org.apache.hadoop.hbase.master.janitor.ReportMakingVisitor$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/janitor/ReportMakingVisitor.class;;;class org.apache.hadoop.hbase.master.janitor.ReportMakingVisitor implements org.apache.hadoop.hbase.ClientMetaTableAccessor$CloseableVisitor {\n  public boolean visit(org.apache.hadoop.hbase.client.Result);\n  public void close() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/locking/LockManager$MasterLock.class;;;public class org.apache.hadoop.hbase.master.locking.LockManager$MasterLock {\n  public org.apache.hadoop.hbase.master.locking.LockManager$MasterLock(org.apache.hadoop.hbase.master.locking.LockManager, java.lang.String, org.apache.hadoop.hbase.procedure2.LockType, java.lang.String);\n  public org.apache.hadoop.hbase.master.locking.LockManager$MasterLock(org.apache.hadoop.hbase.master.locking.LockManager, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.procedure2.LockType, java.lang.String);\n  public org.apache.hadoop.hbase.master.locking.LockManager$MasterLock(org.apache.hadoop.hbase.master.locking.LockManager, org.apache.hadoop.hbase.client.RegionInfo[], java.lang.String);\n  public boolean acquire() throws java.lang.InterruptedException;\n  public boolean tryAcquire(long) throws java.lang.InterruptedException;\n  public void release();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/locking/LockManager$RemoteLocks.class;;;public class org.apache.hadoop.hbase.master.locking.LockManager$RemoteLocks {\n  public org.apache.hadoop.hbase.master.locking.LockManager$RemoteLocks(org.apache.hadoop.hbase.master.locking.LockManager);\n  public long requestNamespaceLock(java.lang.String, org.apache.hadoop.hbase.procedure2.LockType, java.lang.String, org.apache.hadoop.hbase.util.NonceKey) throws java.lang.IllegalArgumentException, java.io.IOException;\n  public long requestTableLock(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.procedure2.LockType, java.lang.String, org.apache.hadoop.hbase.util.NonceKey) throws java.lang.IllegalArgumentException, java.io.IOException;\n  public long requestRegionsLock(org.apache.hadoop.hbase.client.RegionInfo[], java.lang.String, org.apache.hadoop.hbase.util.NonceKey) throws java.lang.IllegalArgumentException, java.io.IOException;\n  public boolean lockHeartbeat(long, boolean) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/locking/LockManager.class;;;public final class org.apache.hadoop.hbase.master.locking.LockManager {\n  public org.apache.hadoop.hbase.master.locking.LockManager(org.apache.hadoop.hbase.master.HMaster);\n  public org.apache.hadoop.hbase.master.locking.LockManager$RemoteLocks remoteLocks();\n  public org.apache.hadoop.hbase.master.locking.LockManager$MasterLock createMasterLock(java.lang.String, org.apache.hadoop.hbase.procedure2.LockType, java.lang.String);\n  public org.apache.hadoop.hbase.master.locking.LockManager$MasterLock createMasterLock(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.procedure2.LockType, java.lang.String);\n  public org.apache.hadoop.hbase.master.locking.LockManager$MasterLock createMasterLock(org.apache.hadoop.hbase.client.RegionInfo[], java.lang.String);\n}\n;;;No, this is not a message definition. It is a class definition for a lock manager in HBase.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class that provides methods for acquiring and releasing locks on regions of an HBase cluster.;;;N
org/apache/hadoop/hbase/master/locking/LockProcedure$1.class;;;class org.apache.hadoop.hbase.master.locking.LockProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a nested class called `1` inside the `LockProcedure` class of the `org.apache.hadoop.hbase.master.locking` package in Hadoop.;;;N
org/apache/hadoop/hbase/master/locking/LockProcedure$LockInterface.class;;;interface org.apache.hadoop.hbase.master.locking.LockProcedure$LockInterface {\n  public abstract boolean acquireLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public abstract void releaseLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/locking/LockProcedure$NamespaceExclusiveLock.class;;;class org.apache.hadoop.hbase.master.locking.LockProcedure$NamespaceExclusiveLock implements org.apache.hadoop.hbase.master.locking.LockProcedure$LockInterface {\n  public boolean acquireLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public void releaseLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/locking/LockProcedure$RegionExclusiveLock.class;;;class org.apache.hadoop.hbase.master.locking.LockProcedure$RegionExclusiveLock implements org.apache.hadoop.hbase.master.locking.LockProcedure$LockInterface {\n  public boolean acquireLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public void releaseLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/master/locking/LockProcedure$TableExclusiveLock.class;;;class org.apache.hadoop.hbase.master.locking.LockProcedure$TableExclusiveLock implements org.apache.hadoop.hbase.master.locking.LockProcedure$LockInterface {\n  public boolean acquireLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public void releaseLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/locking/LockProcedure$TableSharedLock.class;;;class org.apache.hadoop.hbase.master.locking.LockProcedure$TableSharedLock implements org.apache.hadoop.hbase.master.locking.LockProcedure$LockInterface {\n  public boolean acquireLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public void releaseLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/locking/LockProcedure.class;;;public final class org.apache.hadoop.hbase.master.locking.LockProcedure extends org.apache.hadoop.hbase.procedure2.Procedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> implements org.apache.hadoop.hbase.master.procedure.TableProcedureInterface {\n  public static final int DEFAULT_REMOTE_LOCKS_TIMEOUT_MS;\n  public static final java.lang.String REMOTE_LOCKS_TIMEOUT_MS_CONF;\n  public static final int DEFAULT_LOCAL_MASTER_LOCKS_TIMEOUT_MS;\n  public static final java.lang.String LOCAL_MASTER_LOCKS_TIMEOUT_MS_CONF;\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public org.apache.hadoop.hbase.master.locking.LockProcedure();\n  public org.apache.hadoop.hbase.master.locking.LockProcedure(org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.procedure2.LockType, java.lang.String, java.util.concurrent.CountDownLatch) throws java.lang.IllegalArgumentException;\n  public org.apache.hadoop.hbase.master.locking.LockProcedure(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.procedure2.LockType, java.lang.String, java.util.concurrent.CountDownLatch) throws java.lang.IllegalArgumentException;\n  public org.apache.hadoop.hbase.master.locking.LockProcedure(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.procedure2.LockType, java.lang.String, java.util.concurrent.CountDownLatch) throws java.lang.IllegalArgumentException;\n  public void updateHeartBeat();\n  public void unlock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public org.apache.hadoop.hbase.procedure2.LockType getType();\n  public java.lang.String getDescription();\n  public boolean isLocked();\n  public boolean holdLock(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public boolean holdLock(java.lang.Object);\n}\n;;;Yes, the class org.apache.hadoop.hbase.master.locking.LockProcedure appears to be a message definition as it extends the Procedure class and implements the TableProcedureInterface.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/migrate/RollingUpgradeChore.class;;;public class org.apache.hadoop.hbase.master.migrate.RollingUpgradeChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.master.migrate.RollingUpgradeChore(org.apache.hadoop.hbase.master.MasterServices);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/normalizer/MergeNormalizationPlan$1.class;;;class org.apache.hadoop.hbase.master.normalizer.MergeNormalizationPlan$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/MergeNormalizationPlan$Builder.class;;;class org.apache.hadoop.hbase.master.normalizer.MergeNormalizationPlan$Builder {\n  public org.apache.hadoop.hbase.master.normalizer.MergeNormalizationPlan$Builder setTargets(java.util.List<org.apache.hadoop.hbase.master.normalizer.NormalizationTarget>);\n  public org.apache.hadoop.hbase.master.normalizer.MergeNormalizationPlan$Builder addTarget(org.apache.hadoop.hbase.client.RegionInfo, long);\n  public org.apache.hadoop.hbase.master.normalizer.MergeNormalizationPlan build();\n}\n;;;No. This is a builder class for creating instances of the MergeNormalizationPlan class in the org.apache.hadoop.hbase.master.normalizer package. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/MergeNormalizationPlan.class;;;final class org.apache.hadoop.hbase.master.normalizer.MergeNormalizationPlan implements org.apache.hadoop.hbase.master.normalizer.NormalizationPlan {\n  public org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType getType();\n  public java.util.List<org.apache.hadoop.hbase.master.normalizer.NormalizationTarget> getNormalizationTargets();\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/normalizer/NormalizationPlan$PlanType.class;;;public final class org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType extends java.lang.Enum<org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType> {\n  public static final org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType SPLIT;\n  public static final org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType MERGE;\n  public static final org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType NONE;\n  public static org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType[] values();\n  public static org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType valueOf(java.lang.String);\n}\n;;;No. This is a Java class definition for an enum type, not a message definition.;;;N;;;No, it is not a task definition, but rather an enum class that defines the different types of normalization plans that can be generated by a normalizer component in Apache HBase.;;;N
org/apache/hadoop/hbase/master/normalizer/NormalizationPlan.class;;;public interface org.apache.hadoop.hbase.master.normalizer.NormalizationPlan {\n  public abstract org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType getType();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/normalizer/NormalizationTarget.class;;;class org.apache.hadoop.hbase.master.normalizer.NormalizationTarget {\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public long getRegionSizeMb();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/RegionNormalizer.class;;;interface org.apache.hadoop.hbase.master.normalizer.RegionNormalizer extends org.apache.hadoop.conf.Configurable {\n  public abstract void setMasterServices(org.apache.hadoop.hbase.master.MasterServices);\n  public abstract java.util.List<org.apache.hadoop.hbase.master.normalizer.NormalizationPlan> computePlansForTable(org.apache.hadoop.hbase.client.TableDescriptor);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/normalizer/RegionNormalizerChore.class;;;class org.apache.hadoop.hbase.master.normalizer.RegionNormalizerChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.master.normalizer.RegionNormalizerChore(org.apache.hadoop.hbase.master.MasterServices);\n}\n;;;No. This is a class definition for a component in the Hadoop ecosystem, but it is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/RegionNormalizerFactory.class;;;public final class org.apache.hadoop.hbase.master.normalizer.RegionNormalizerFactory {\n  public static org.apache.hadoop.hbase.master.normalizer.RegionNormalizerManager createNormalizerManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.master.HMaster);\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/master/normalizer/RegionNormalizerManager.class;;;public class org.apache.hadoop.hbase.master.normalizer.RegionNormalizerManager implements org.apache.hadoop.hbase.conf.PropagatingConfigurationObserver {\n  public void registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public void deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public void start();\n  public void stop();\n  public org.apache.hadoop.hbase.ScheduledChore getRegionNormalizerChore();\n  public boolean isNormalizerOn();\n  public void setNormalizerOn(boolean);\n  public void planSkipped(org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType);\n  public long getSkippedCount(org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType);\n  public long getSplitPlanCount();\n  public long getMergePlanCount();\n  public boolean normalizeRegions(java.util.List<org.apache.hadoop.hbase.TableName>, boolean);\n}\n;;;yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorkQueue.class;;;class org.apache.hadoop.hbase.master.normalizer.RegionNormalizerWorkQueue<E> {\n  public void put(E);\n  public void putFirst(E);\n  public void putAll(java.util.Collection<? extends E>);\n  public void putAllFirst(java.util.Collection<? extends E>);\n  public E take() throws java.lang.InterruptedException;\n  public void clear();\n  public int size();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker$1.class;;;class org.apache.hadoop.hbase.master.normalizer.RegionNormalizerWorker$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/RegionNormalizerWorker.class;;;class org.apache.hadoop.hbase.master.normalizer.RegionNormalizerWorker implements org.apache.hadoop.hbase.conf.PropagatingConfigurationObserver,java.lang.Runnable {\n  public static final java.lang.String HBASE_TABLE_NORMALIZATION_ENABLED;\n  public void registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public void deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public void run();\n}\n;;;No. It is a class definition for a RegionNormalizerWorker in HBase, but it does not define any specific message that could be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer$1.class;;;class org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer$NormalizeContext.class;;;class org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer$NormalizeContext {\n  public org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer$NormalizeContext(org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer, org.apache.hadoop.hbase.client.TableDescriptor);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.assignment.RegionStates getRegionStates();\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getTableRegions();\n  public double getAverageRegionSizeMb();\n  public <T> T getOrDefault(java.lang.String, java.util.function.Function<java.lang.String, T>, T);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer$NormalizerConfiguration.class;;;final class org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer$NormalizerConfiguration {\n  public org.apache.hadoop.conf.Configuration getConf();\n  public boolean isSplitEnabled();\n  public boolean isMergeEnabled();\n  public int getMergeMinRegionCount();\n  public int getMergeMinRegionCount(org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer$NormalizeContext);\n  public java.time.Period getMergeMinRegionAge();\n  public java.time.Period getMergeMinRegionAge(org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer$NormalizeContext);\n  public long getMergeMinRegionSizeMb();\n  public long getMergeMinRegionSizeMb(org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer$NormalizeContext);\n}\n;;;No. This class provides configuration options and getter methods, but it does not define a message that would be passed around on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.class;;;class org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer implements org.apache.hadoop.hbase.master.normalizer.RegionNormalizer,org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public boolean isSplitEnabled();\n  public boolean isMergeEnabled();\n  public int getMergeMinRegionCount();\n  public java.time.Period getMergeMinRegionAge();\n  public long getMergeMinRegionSizeMb();\n  public void setMasterServices(org.apache.hadoop.hbase.master.MasterServices);\n  public java.util.List<org.apache.hadoop.hbase.master.normalizer.NormalizationPlan> computePlansForTable(org.apache.hadoop.hbase.client.TableDescriptor);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/normalizer/SplitNormalizationPlan.class;;;final class org.apache.hadoop.hbase.master.normalizer.SplitNormalizationPlan implements org.apache.hadoop.hbase.master.normalizer.NormalizationPlan {\n  public org.apache.hadoop.hbase.master.normalizer.NormalizationPlan$PlanType getType();\n  public org.apache.hadoop.hbase.master.normalizer.NormalizationTarget getSplitTarget();\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/AbstractStateMachineNamespaceProcedure.class;;;public abstract class org.apache.hadoop.hbase.master.procedure.AbstractStateMachineNamespaceProcedure<TState> extends org.apache.hadoop.hbase.procedure2.StateMachineProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, TState> implements org.apache.hadoop.hbase.master.procedure.TableProcedureInterface {\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public abstract org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public static void createDirectory(org.apache.hadoop.hbase.master.MasterFileSystem, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n}\n;;;No, it is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/AbstractStateMachineRegionProcedure.class;;;public abstract class org.apache.hadoop.hbase.master.procedure.AbstractStateMachineRegionProcedure<TState> extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<TState> {\n  public org.apache.hadoop.hbase.client.RegionInfo getRegion();\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public abstract org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void toStringClassDetails(java.lang.StringBuilder);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/AbstractStateMachineTableProcedure.class;;;public abstract class org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<TState> extends org.apache.hadoop.hbase.procedure2.StateMachineProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, TState> implements org.apache.hadoop.hbase.master.procedure.TableProcedureInterface {\n  public abstract org.apache.hadoop.hbase.TableName getTableName();\n  public abstract org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void toStringClassDetails(java.lang.StringBuilder);\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure$1 implements org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$CreateHdfsRegions {\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> createHdfsRegions(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure$2.class;;;class org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure$2 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/CloneSnapshotProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$CloneSnapshotState> {\n  public org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure();\n  public org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription);\n  public org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, boolean);\n  public org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, boolean, java.lang.String);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public boolean getRestoreAcl();\n}\n;;;No. This is a Java class definition for a procedure used in the Apache HBase database, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/CreateNamespaceProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/CreateNamespaceProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineNamespaceProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$CreateNamespaceState> {\n  public org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure();\n  public org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.NamespaceDescriptor);\n  public org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch);\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/CreateTableProcedure$1.class;;;final class org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$1 implements org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$CreateHdfsRegions {\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> createHdfsRegions(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/CreateTableProcedure$2.class;;;class org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$2 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/CreateTableProcedure$CreateHdfsRegions.class;;;public interface org.apache.hadoop.hbase.master.procedure.CreateTableProcedure$CreateHdfsRegions {\n  public abstract java.util.List<org.apache.hadoop.hbase.client.RegionInfo> createHdfsRegions(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/CreateTableProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.CreateTableProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$CreateTableState> {\n  public org.apache.hadoop.hbase.master.procedure.CreateTableProcedure();\n  public org.apache.hadoop.hbase.master.procedure.CreateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]);\n  public org.apache.hadoop.hbase.master.procedure.CreateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/DeleteNamespaceProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/DeleteNamespaceProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineNamespaceProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$DeleteNamespaceState> {\n  public org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure();\n  public org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.lang.String);\n  public org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.lang.String, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch);\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It seems to be an anonymous inner class within a procedure definition in Hadoop HBase.;;;N
org/apache/hadoop/hbase/master/procedure/DeleteTableProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$DeleteTableState> {\n  public org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure();\n  public org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/DisableTableProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.DisableTableProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a nested class defined within the `DisableTableProcedure` class in the HBase framework. It is not a standalone entity that can be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/DisableTableProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.DisableTableProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$DisableTableState> {\n  public org.apache.hadoop.hbase.master.procedure.DisableTableProcedure();\n  public org.apache.hadoop.hbase.master.procedure.DisableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.master.procedure.DisableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/EnableTableProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.EnableTableProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/EnableTableProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.EnableTableProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$EnableTableState> {\n  public org.apache.hadoop.hbase.master.procedure.EnableTableProcedure();\n  public org.apache.hadoop.hbase.master.procedure.EnableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.master.procedure.EnableTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/FairQueue.class;;;public class org.apache.hadoop.hbase.master.procedure.FairQueue<T extends java.lang.Comparable<T>> {\n  public org.apache.hadoop.hbase.master.procedure.FairQueue();\n  public boolean hasRunnables();\n  public void add(org.apache.hadoop.hbase.master.procedure.Queue<T>);\n  public void remove(org.apache.hadoop.hbase.master.procedure.Queue<T>);\n  public org.apache.hadoop.hbase.master.procedure.Queue<T> poll();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/HBCKServerCrashProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.HBCKServerCrashProcedure$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/HBCKServerCrashProcedure$UnknownServerVisitor.class;;;final class org.apache.hadoop.hbase.master.procedure.HBCKServerCrashProcedure$UnknownServerVisitor implements org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor {\n  public boolean visit(org.apache.hadoop.hbase.client.Result) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/HBCKServerCrashProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.HBCKServerCrashProcedure extends org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure {\n  public org.apache.hadoop.hbase.master.procedure.HBCKServerCrashProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName, boolean, boolean);\n  public org.apache.hadoop.hbase.master.procedure.HBCKServerCrashProcedure();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/InitMetaProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.InitMetaProcedure$1 {\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/InitMetaProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.InitMetaProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$InitMetaState> {\n  public org.apache.hadoop.hbase.master.procedure.InitMetaProcedure();\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void await() throws java.lang.InterruptedException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/MasterDDLOperationHelper.class;;;public final class org.apache.hadoop.hbase.master.procedure.MasterDDLOperationHelper {\n  public static void deleteColumnFamilyFromFileSystem(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, byte[], boolean) throws java.io.IOException;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/procedure/MasterProcedureConstants.class;;;public final class org.apache.hadoop.hbase.master.procedure.MasterProcedureConstants {\n  public static final java.lang.String MASTER_PROCEDURE_THREADS;\n  public static final int DEFAULT_MIN_MASTER_PROCEDURE_THREADS;\n  public static final java.lang.String EXECUTOR_ABORT_ON_CORRUPTION;\n  public static final boolean DEFAULT_EXECUTOR_ABORT_ON_CORRUPTION;\n}\n;;;No. This class contains constants related to the configuration of the Apache HBase master procedure, but it is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/MasterProcedureEnv$FsUtilsLeaseRecovery$1.class;;;class org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$FsUtilsLeaseRecovery$1 implements org.apache.hadoop.hbase.util.CancelableProgressable {\n  public boolean progress();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/MasterProcedureEnv$FsUtilsLeaseRecovery.class;;;public class org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$FsUtilsLeaseRecovery implements org.apache.hadoop.hbase.procedure2.store.LeaseRecovery {\n  public org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv$FsUtilsLeaseRecovery(org.apache.hadoop.hbase.master.MasterServices);\n  public void recoverFileLease(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/MasterProcedureEnv.class;;;public class org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv implements org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv(org.apache.hadoop.hbase.master.MasterServices);\n  public org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher);\n  public org.apache.hadoop.hbase.security.User getRequestUser();\n  public org.apache.hadoop.hbase.master.MasterServices getMasterServices();\n  public org.apache.hadoop.conf.Configuration getMasterConfiguration();\n  public org.apache.hadoop.hbase.master.assignment.AssignmentManager getAssignmentManager();\n  public org.apache.hadoop.hbase.master.MasterCoprocessorHost getMasterCoprocessorHost();\n  public org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler getProcedureScheduler();\n  public org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher getRemoteDispatcher();\n  public org.apache.hadoop.hbase.master.replication.ReplicationPeerManager getReplicationPeerManager();\n  public org.apache.hadoop.hbase.master.MasterFileSystem getMasterFileSystem();\n  public boolean isRunning();\n  public boolean isInitialized();\n  public boolean waitInitialized(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public void setEventReady(org.apache.hadoop.hbase.procedure2.ProcedureEvent<?>, boolean);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/MasterProcedureScheduler.class;;;public class org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler extends org.apache.hadoop.hbase.procedure2.AbstractProcedureScheduler {\n  public org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler(java.util.function.Function<java.lang.Long, org.apache.hadoop.hbase.procedure2.Procedure<?>>);\n  public void yield(org.apache.hadoop.hbase.procedure2.Procedure);\n  public java.util.List<org.apache.hadoop.hbase.procedure2.LockedResource> getLocks();\n  public org.apache.hadoop.hbase.procedure2.LockedResource getLockResource(org.apache.hadoop.hbase.procedure2.LockedResourceType, java.lang.String);\n  public void clear();\n  public void completionCleanup(org.apache.hadoop.hbase.procedure2.Procedure);\n  public boolean waitTableExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.TableName);\n  public void wakeTableExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.TableName);\n  public boolean waitTableSharedLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.TableName);\n  public void wakeTableSharedLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.TableName);\n  public boolean waitRegion(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.client.RegionInfo);\n  public boolean waitRegions(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo...);\n  public void wakeRegion(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.client.RegionInfo);\n  public void wakeRegions(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo...);\n  public boolean waitNamespaceExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, java.lang.String);\n  public void wakeNamespaceExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, java.lang.String);\n  public boolean waitServerExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.ServerName);\n  public void wakeServerExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.ServerName);\n  public boolean waitPeerExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, java.lang.String);\n  public void wakePeerExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>, java.lang.String);\n  public boolean waitMetaExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public void wakeMetaExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public java.lang.String dumpLocks() throws java.io.IOException;\n}\n;;;No, this class is not a message definition. It defines a scheduler for HBase master procedures and provides various methods for managing locks and resources. It can be used as part of a larger system, but it is not directly related to message passing or message queuing.;;;N;;;No. This class defines a scheduler for procedures in HBase and is not a task that can be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil$NonceProcedureRunnable.class;;;public abstract class org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n  public org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable(org.apache.hadoop.hbase.master.MasterServices, long, long);\n}\n;;;No.;;;N;;;It is not a task definition, as it does not contain any instructions or actions to be performed.;;;?
org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil$RSGroupGetter.class;;;public interface org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$RSGroupGetter {\n  public abstract org.apache.hadoop.hbase.rsgroup.RSGroupInfo get(java.lang.String) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/MasterProcedureUtil.class;;;public final class org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil {\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$UserInformation toProtoUserInfo(org.apache.hadoop.hbase.security.User);\n  public static org.apache.hadoop.hbase.security.User toUserInfo(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$UserInformation);\n  public static long submitProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable) throws java.io.IOException;\n  public static boolean validateProcedureWALFilename(java.lang.String);\n  public static int getTablePriority(org.apache.hadoop.hbase.TableName);\n  public static int getServerPriority(org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface);\n  public static java.io.IOException unwrapRemoteIOException(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public static void checkGroupNotEmpty(org.apache.hadoop.hbase.rsgroup.RSGroupInfo, java.util.function.Supplier<java.lang.String>) throws org.apache.hadoop.hbase.constraint.ConstraintException;\n  public static org.apache.hadoop.hbase.rsgroup.RSGroupInfo checkGroupExists(org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$RSGroupGetter, java.util.Optional<java.lang.String>, java.util.function.Supplier<java.lang.String>) throws java.io.IOException;\n  public static java.util.Optional<java.lang.String> getNamespaceGroup(org.apache.hadoop.hbase.NamespaceDescriptor);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/MetaProcedureInterface$MetaOperationType.class;;;public final class org.apache.hadoop.hbase.master.procedure.MetaProcedureInterface$MetaOperationType extends java.lang.Enum<org.apache.hadoop.hbase.master.procedure.MetaProcedureInterface$MetaOperationType> {\n  public static final org.apache.hadoop.hbase.master.procedure.MetaProcedureInterface$MetaOperationType RECOVER;\n  public static org.apache.hadoop.hbase.master.procedure.MetaProcedureInterface$MetaOperationType[] values();\n  public static org.apache.hadoop.hbase.master.procedure.MetaProcedureInterface$MetaOperationType valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/MetaProcedureInterface.class;;;public interface org.apache.hadoop.hbase.master.procedure.MetaProcedureInterface {\n  public default org.apache.hadoop.hbase.master.procedure.MetaProcedureInterface$MetaOperationType getMetaOperationType();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/MetaQueue.class;;;class org.apache.hadoop.hbase.master.procedure.MetaQueue extends org.apache.hadoop.hbase.master.procedure.Queue<org.apache.hadoop.hbase.TableName> {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/ModifyNamespaceProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ModifyNamespaceProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineNamespaceProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$ModifyNamespaceState> {\n  public org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure();\n  public org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.NamespaceDescriptor);\n  public org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch);\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a procedure definition that might be put on a procedure queue.;;;N
org/apache/hadoop/hbase/master/procedure/ModifyTableDescriptorProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.ModifyTableDescriptorProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ModifyTableDescriptorProcedure.class;;;public abstract class org.apache.hadoop.hbase.master.procedure.ModifyTableDescriptorProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$ModifyTableDescriptorState> {\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be an anonymous inner class within the ModifyTableProcedure class in the Hadoop HBase library.;;;N
org/apache/hadoop/hbase/master/procedure/ModifyTableProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$ModifyTableState> {\n  public org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure();\n  public org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch, org.apache.hadoop.hbase.client.TableDescriptor, boolean) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No. This class defines a procedure for modifying a table in HBase, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface$PeerOperationType.class;;;public final class org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType extends java.lang.Enum<org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType> {\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType ADD;\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType REMOVE;\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType ENABLE;\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType DISABLE;\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType UPDATE_CONFIG;\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType REFRESH;\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType TRANSIT_SYNC_REPLICATION_STATE;\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType RECOVER_STANDBY;\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType SYNC_REPLICATION_REPLAY_WAL;\n  public static final org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType SYNC_REPLICATION_REPLAY_WAL_REMOTE;\n  public static org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType[] values();\n  public static org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType valueOf(java.lang.String);\n}\n;;;No. This is an enum definition for the various types of peer operation, but it does not define a message that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/PeerProcedureInterface.class;;;public interface org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface {\n  public abstract java.lang.String getPeerId();\n  public abstract org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/PeerQueue.class;;;class org.apache.hadoop.hbase.master.procedure.PeerQueue extends org.apache.hadoop.hbase.master.procedure.Queue<java.lang.String> {\n  public org.apache.hadoop.hbase.master.procedure.PeerQueue(java.lang.String, org.apache.hadoop.hbase.procedure2.LockStatus);\n  public boolean requireExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n}\n;;;No. The class is a definition of a queue used in HBase master procedure processing, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedureDescriber.class;;;public class org.apache.hadoop.hbase.master.procedure.ProcedureDescriber {\n  public static java.lang.String describe(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public static java.lang.String describeParameters(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n}\n;;;No. This class provides utility methods for describing procedures in Hadoop HBase, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedurePrepareLatch$1.class;;;class org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedurePrepareLatch$CompatibilityLatch.class;;;public class org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch$CompatibilityLatch extends org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch {\n  public void await() throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedurePrepareLatch$NoopLatch.class;;;class org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch$NoopLatch extends org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch {\n  public void await() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedurePrepareLatch.class;;;public abstract class org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch {\n  public org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch();\n  public static org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch createLatch();\n  public static org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch createLatch(int, int);\n  public static org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch createBlockingLatch();\n  public static org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch getNoopLatch();\n  public abstract void await() throws java.io.IOException;\n  public static void releaseLatch(org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch, org.apache.hadoop.hbase.procedure2.Procedure);\n}\n;;;No, this class does not contain any message definitions that might be put on a message queue. Instead, it provides methods for creating and working with procedure prepare latches in Hadoop HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait$1.class;;;final class org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$1 implements org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<java.lang.Boolean> {\n  public java.lang.Boolean evaluate() throws java.io.IOException;\n  public java.lang.Object evaluate() throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait$2.class;;;final class org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$2 implements org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<java.lang.Boolean> {\n  public java.lang.Boolean evaluate() throws java.io.IOException;\n  public java.lang.Object evaluate() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait$3.class;;;final class org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$3 implements org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<org.apache.hadoop.hbase.quotas.MasterQuotaManager> {\n  public org.apache.hadoop.hbase.quotas.MasterQuotaManager evaluate() throws java.io.IOException;\n  public java.lang.Object evaluate() throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be a class implementation that defines a method for evaluating a MasterQuotaManager in the context of the ProcedureSyncWait class.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait$Predicate.class;;;public interface org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<T> {\n  public abstract T evaluate() throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface definition for a predicate method that throws an IOException, used in the HBase procedure framework.;;;N
org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait$ProcedureFuture.class;;;class org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$ProcedureFuture implements java.util.concurrent.Future<byte[]> {\n  public org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$ProcedureFuture(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public boolean cancel(boolean);\n  public boolean isCancelled();\n  public boolean isDone();\n  public byte[] get() throws java.lang.InterruptedException, java.util.concurrent.ExecutionException;\n  public byte[] get(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException, java.util.concurrent.ExecutionException, java.util.concurrent.TimeoutException;\n  public java.lang.Object get(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException, java.util.concurrent.ExecutionException, java.util.concurrent.TimeoutException;\n  public java.lang.Object get() throws java.lang.InterruptedException, java.util.concurrent.ExecutionException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/ProcedureSyncWait.class;;;public final class org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait {\n  public static java.util.concurrent.Future<byte[]> submitProcedure(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, org.apache.hadoop.hbase.procedure2.Procedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>);\n  public static byte[] submitAndWaitProcedure(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, org.apache.hadoop.hbase.procedure2.Procedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>) throws java.io.IOException;\n  public static byte[] waitForProcedureToCompleteIOE(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, org.apache.hadoop.hbase.procedure2.Procedure<?>, long) throws java.io.IOException;\n  public static byte[] waitForProcedureToComplete(org.apache.hadoop.hbase.procedure2.ProcedureExecutor<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv>, org.apache.hadoop.hbase.procedure2.Procedure<?>, long) throws java.io.IOException;\n  public static <T> T waitFor(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.lang.String, org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<T>) throws java.io.IOException;\n  public static <T> T waitFor(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, long, java.lang.String, org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<T>) throws java.io.IOException;\n  public static <T> T waitFor(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, long, long, java.lang.String, org.apache.hadoop.hbase.master.procedure.ProcedureSyncWait$Predicate<T>) throws java.io.IOException;\n}\n;;;No. This class provides methods for synchronously waiting for the completion of procedures in HBase, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/Queue.class;;;abstract class org.apache.hadoop.hbase.master.procedure.Queue<TKey extends java.lang.Comparable<TKey>> extends org.apache.hadoop.hbase.util.AvlUtil$AvlLinkedNode<org.apache.hadoop.hbase.master.procedure.Queue<TKey>> {\n  public int getPriority();\n  public boolean isAvailable();\n  public void add(org.apache.hadoop.hbase.procedure2.Procedure<?>, boolean);\n  public org.apache.hadoop.hbase.procedure2.Procedure<?> peek();\n  public org.apache.hadoop.hbase.procedure2.Procedure<?> poll();\n  public boolean isEmpty();\n  public int size();\n  public int compareKey(TKey);\n  public int compareTo(org.apache.hadoop.hbase.master.procedure.Queue<TKey>);\n  public java.lang.String toString();\n  public int compareTo(org.apache.hadoop.hbase.util.AvlUtil$AvlNode);\n}\n;;;No, it is not a message definition. It is a class definition for a queue implementation.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher$1.class;;;class org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$1 implements java.lang.Thread$UncaughtExceptionHandler {\n  public void uncaughtException(java.lang.Thread, java.lang.Throwable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher$DeadRSRemoteCall.class;;;class org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$DeadRSRemoteCall extends org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$ExecuteProceduresRemoteCall {\n  public org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$DeadRSRemoteCall(org.apache.hadoop.hbase.ServerName, java.util.Set<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure>);\n  public void run();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher$ExecuteProceduresRemoteCall.class;;;public class org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$ExecuteProceduresRemoteCall implements org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RemoteProcedureResolver,java.lang.Runnable {\n  public org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$ExecuteProceduresRemoteCall(org.apache.hadoop.hbase.ServerName, java.util.Set<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure>);\n  public void run();\n  public void dispatchOpenRequests(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.util.List<org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionOpenOperation>);\n  public void dispatchCloseRequests(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.util.List<org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionCloseOperation>);\n  public void dispatchServerOperations(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.util.List<org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$ServerOperation>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher$RegionCloseOperation.class;;;public class org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionCloseOperation extends org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionOperation {\n  public org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionCloseOperation(org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure, org.apache.hadoop.hbase.client.RegionInfo, long, org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.ServerName getDestinationServer();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CloseRegionRequest buildCloseRegionRequest(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher$RegionOpenOperation.class;;;public class org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionOpenOperation extends org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionOperation {\n  public org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionOpenOperation(org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure, org.apache.hadoop.hbase.client.RegionInfo, long);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$OpenRegionRequest$RegionOpenInfo buildRegionOpenInfoRequest(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher$RegionOperation.class;;;public abstract class org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionOperation extends org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation {\n}\n;;;No.;;;N;;;No, it is not a task definition, but rather a class definition for a region operation dispatcher in HBase.;;;N
org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher$RemoteProcedureResolver.class;;;interface org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RemoteProcedureResolver {\n  public abstract void dispatchOpenRequests(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.util.List<org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionOpenOperation>);\n  public abstract void dispatchCloseRequests(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.util.List<org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RegionCloseOperation>);\n  public abstract void dispatchServerOperations(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, java.util.List<org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$ServerOperation>);\n}\n;;;No. It is an interface that defines three methods related to dispatching requests and operations in HBase. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher$ServerOperation.class;;;public final class org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$ServerOperation extends org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation {\n  public org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$ServerOperation(org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure, long, java.lang.Class<?>, byte[]);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$RemoteProcedureRequest buildRequest();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/RSProcedureDispatcher.class;;;public class org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher extends org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName> implements org.apache.hadoop.hbase.master.ServerListener {\n  public static final java.lang.String RS_RPC_STARTUP_WAIT_TIME_CONF_KEY;\n  public org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher(org.apache.hadoop.hbase.master.MasterServices);\n  public boolean start();\n  public boolean stop();\n  public void serverAdded(org.apache.hadoop.hbase.ServerName);\n  public void serverRemoved(org.apache.hadoop.hbase.ServerName);\n  public void splitAndResolveOperation(org.apache.hadoop.hbase.ServerName, java.util.Set<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure>, org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher$RemoteProcedureResolver);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/RecoverMetaProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.RecoverMetaProcedure extends org.apache.hadoop.hbase.procedure2.StateMachineProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$RecoverMetaState> implements org.apache.hadoop.hbase.master.procedure.MetaProcedureInterface {\n  public org.apache.hadoop.hbase.master.procedure.RecoverMetaProcedure();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.ReopenTableRegionsProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/ReopenTableRegionsProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.ReopenTableRegionsProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$ReopenTableRegionsState> {\n  public org.apache.hadoop.hbase.master.procedure.ReopenTableRegionsProcedure();\n  public org.apache.hadoop.hbase.master.procedure.ReopenTableRegionsProcedure(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.master.procedure.ReopenTableRegionsProcedure(org.apache.hadoop.hbase.TableName, java.util.List<byte[]>);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/RestoreSnapshotProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$RestoreSnapshotState> {\n  public org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure();\n  public org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, boolean) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public boolean abort(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public boolean getRestoreAcl();\n  public boolean abort(java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/SchemaLocking$1.class;;;class org.apache.hadoop.hbase.master.procedure.SchemaLocking$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/procedure/SchemaLocking.class;;;class org.apache.hadoop.hbase.master.procedure.SchemaLocking {\n  public org.apache.hadoop.hbase.master.procedure.SchemaLocking(java.util.function.Function<java.lang.Long, org.apache.hadoop.hbase.procedure2.Procedure<?>>);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/ServerCrashException.class;;;public class org.apache.hadoop.hbase.master.procedure.ServerCrashException extends org.apache.hadoop.hbase.HBaseIOException {\n  public org.apache.hadoop.hbase.master.procedure.ServerCrashException(long, org.apache.hadoop.hbase.ServerName);\n  public java.lang.String getMessage();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a nested class within `ServerCrashProcedure` class in the HBase Master Procedure framework for handling server crashes.;;;N
org/apache/hadoop/hbase/master/procedure/ServerCrashProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure extends org.apache.hadoop.hbase.procedure2.StateMachineProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$ServerCrashState> implements org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface {\n  public static final java.lang.String MASTER_SCP_RETAIN_ASSIGNMENT;\n  public static final boolean DEFAULT_MASTER_SCP_RETAIN_ASSIGNMENT;\n  public org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName, boolean, boolean);\n  public org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure();\n  public boolean isInRecoverMetaState();\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public java.lang.String getProcName();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public boolean hasMetaTableRegion();\n  public org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType getServerOperationType();\n  public static void updateProgress(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, long);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/ServerProcedureInterface$ServerOperationType.class;;;public final class org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType extends java.lang.Enum<org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType> {\n  public static final org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType CRASH_HANDLER;\n  public static final org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType SWITCH_RPC_THROTTLE;\n  public static final org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType SPLIT_WAL;\n  public static final org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType SPLIT_WAL_REMOTE;\n  public static final org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType CLAIM_REPLICATION_QUEUES;\n  public static final org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType CLAIM_REPLICATION_QUEUE_REMOTE;\n  public static final org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType VERIFY_SNAPSHOT;\n  public static org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType[] values();\n  public static org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType valueOf(java.lang.String);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ServerProcedureInterface.class;;;public interface org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface {\n  public abstract org.apache.hadoop.hbase.ServerName getServerName();\n  public abstract boolean hasMetaTableRegion();\n  public abstract org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType getServerOperationType();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/ServerQueue$1.class;;;class org.apache.hadoop.hbase.master.procedure.ServerQueue$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ServerQueue.class;;;class org.apache.hadoop.hbase.master.procedure.ServerQueue extends org.apache.hadoop.hbase.master.procedure.Queue<org.apache.hadoop.hbase.ServerName> {\n  public org.apache.hadoop.hbase.master.procedure.ServerQueue(org.apache.hadoop.hbase.ServerName, int, org.apache.hadoop.hbase.procedure2.LockStatus);\n  public boolean requireExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/ServerRemoteProcedure.class;;;public abstract class org.apache.hadoop.hbase.master.procedure.ServerRemoteProcedure extends org.apache.hadoop.hbase.procedure2.Procedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> implements org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName> {\n  public org.apache.hadoop.hbase.master.procedure.ServerRemoteProcedure();\n  public synchronized void remoteCallFailed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName, java.io.IOException);\n  public synchronized void remoteOperationCompleted(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public synchronized void remoteOperationFailed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.procedure2.RemoteProcedureException);\n  public void remoteOperationFailed(java.lang.Object, org.apache.hadoop.hbase.procedure2.RemoteProcedureException);\n  public void remoteOperationCompleted(java.lang.Object);\n  public void remoteCallFailed(java.lang.Object, java.lang.Object, java.io.IOException);\n}\n;;;No, it is a class definition. It is not a specific message that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/SnapshotProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.SnapshotProcedure$1 implements org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask {\n  public void editRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/procedure/SnapshotProcedure$2.class;;;class org.apache.hadoop.hbase.master.procedure.SnapshotProcedure$2 implements org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask {\n  public void editRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/procedure/SnapshotProcedure$3.class;;;class org.apache.hadoop.hbase.master.procedure.SnapshotProcedure$3 {\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class definition for a specific procedure related to Apache HBase master snapshot functionality. It may be used within a larger codebase to implement some functionality related to HBase snapshots, but it is not a standalone task that can be put on a task queue.;;;N
org/apache/hadoop/hbase/master/procedure/SnapshotProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.SnapshotProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$SnapshotState> {\n  public org.apache.hadoop.hbase.master.procedure.SnapshotProcedure();\n  public org.apache.hadoop.hbase.master.procedure.SnapshotProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription getSnapshotDesc();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription getSnapshot();\n  public synchronized void markSnapshotCorrupted() throws java.io.IOException;\n  public boolean isSnapshotCorrupted() throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/SnapshotRegionProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.SnapshotRegionProcedure extends org.apache.hadoop.hbase.procedure2.Procedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> implements org.apache.hadoop.hbase.master.procedure.TableProcedureInterface, org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName> {\n  public org.apache.hadoop.hbase.master.procedure.SnapshotRegionProcedure();\n  public org.apache.hadoop.hbase.master.procedure.SnapshotRegionProcedure(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.client.RegionInfo);\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public void remoteCallFailed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName, java.io.IOException);\n  public void remoteOperationCompleted(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv);\n  public void remoteOperationFailed(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.procedure2.RemoteProcedureException);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public java.lang.String getProcName();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegion();\n  public void remoteOperationFailed(java.lang.Object, org.apache.hadoop.hbase.procedure2.RemoteProcedureException);\n  public void remoteOperationCompleted(java.lang.Object);\n  public void remoteCallFailed(java.lang.Object, java.lang.Object, java.io.IOException);\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;Yes. This class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/SnapshotVerifyProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.SnapshotVerifyProcedure extends org.apache.hadoop.hbase.master.procedure.ServerRemoteProcedure implements org.apache.hadoop.hbase.master.procedure.TableProcedureInterface {\n  public org.apache.hadoop.hbase.master.procedure.SnapshotVerifyProcedure();\n  public org.apache.hadoop.hbase.master.procedure.SnapshotVerifyProcedure(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.client.RegionInfo);\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;Yes, this class might be a message definition that could be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/SplitWALProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.SplitWALProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/SplitWALProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.SplitWALProcedure extends org.apache.hadoop.hbase.procedure2.StateMachineProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$SplitWALState> implements org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface {\n  public org.apache.hadoop.hbase.master.procedure.SplitWALProcedure();\n  public org.apache.hadoop.hbase.master.procedure.SplitWALProcedure(java.lang.String, org.apache.hadoop.hbase.ServerName);\n  public java.lang.String getWAL();\n  public org.apache.hadoop.hbase.ServerName getWorker();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public boolean hasMetaTableRegion();\n  public org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType getServerOperationType();\n  public java.lang.String getProcName();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/procedure/SplitWALRemoteProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.SplitWALRemoteProcedure extends org.apache.hadoop.hbase.master.procedure.ServerRemoteProcedure implements org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface {\n  public org.apache.hadoop.hbase.master.procedure.SplitWALRemoteProcedure();\n  public org.apache.hadoop.hbase.master.procedure.SplitWALRemoteProcedure(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName, java.lang.String);\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public java.lang.String getWAL();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public boolean hasMetaTableRegion();\n  public org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType getServerOperationType();\n  public java.lang.String getProcName();\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/SwitchRpcThrottleProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.SwitchRpcThrottleProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/SwitchRpcThrottleProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.SwitchRpcThrottleProcedure extends org.apache.hadoop.hbase.procedure2.StateMachineProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$SwitchRpcThrottleState> implements org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface {\n  public org.apache.hadoop.hbase.master.procedure.SwitchRpcThrottleProcedure();\n  public org.apache.hadoop.hbase.master.procedure.SwitchRpcThrottleProcedure(org.apache.hadoop.hbase.quotas.RpcThrottleStorage, boolean, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch);\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public boolean hasMetaTableRegion();\n  public org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType getServerOperationType();\n  public void switchThrottleState(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, boolean) throws java.io.IOException;\n  public void toStringClassDetails(java.lang.StringBuilder);\n}\n;;;No, it is not a message definition. It is a class definition for a procedure in Apache HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/SwitchRpcThrottleRemoteProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.SwitchRpcThrottleRemoteProcedure extends org.apache.hadoop.hbase.master.procedure.ServerRemoteProcedure implements org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface {\n  public org.apache.hadoop.hbase.master.procedure.SwitchRpcThrottleRemoteProcedure();\n  public org.apache.hadoop.hbase.master.procedure.SwitchRpcThrottleRemoteProcedure(org.apache.hadoop.hbase.ServerName, boolean);\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public boolean hasMetaTableRegion();\n  public org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType getServerOperationType();\n  public void toStringClassDetails(java.lang.StringBuilder);\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/TableProcedureInterface$TableOperationType.class;;;public final class org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType extends java.lang.Enum<org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType> {\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType CREATE;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType DELETE;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType DISABLE;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType EDIT;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType ENABLE;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType READ;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType SNAPSHOT;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType REGION_SNAPSHOT;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType REGION_EDIT;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType REGION_SPLIT;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType REGION_MERGE;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType REGION_ASSIGN;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType REGION_UNASSIGN;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType REGION_GC;\n  public static final org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType MERGED_REGIONS_GC;\n  public static org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType[] values();\n  public static org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType valueOf(java.lang.String);\n}\n;;;No, this is not a message definition. It is an enumeration class in Java that defines a set of named constants.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/TableProcedureInterface.class;;;public interface org.apache.hadoop.hbase.master.procedure.TableProcedureInterface {\n  public static final org.apache.hadoop.hbase.TableName DUMMY_NAMESPACE_TABLE_NAME;\n  public abstract org.apache.hadoop.hbase.TableName getTableName();\n  public abstract org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/TableQueue$1.class;;;class org.apache.hadoop.hbase.master.procedure.TableQueue$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/TableQueue.class;;;class org.apache.hadoop.hbase.master.procedure.TableQueue extends org.apache.hadoop.hbase.master.procedure.Queue<org.apache.hadoop.hbase.TableName> {\n  public org.apache.hadoop.hbase.master.procedure.TableQueue(org.apache.hadoop.hbase.TableName, int, org.apache.hadoop.hbase.procedure2.LockStatus, org.apache.hadoop.hbase.procedure2.LockStatus);\n  public boolean isAvailable();\n  public boolean requireExclusiveLock(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure$1.class;;;class org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/procedure/TruncateTableProcedure.class;;;public class org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$TruncateTableState> {\n  public org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure();\n  public org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, boolean, org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n  public void toStringClassDetails(java.lang.StringBuilder);\n}\n;;;No.;;;N;;;No, it is a procedure definition for the Apache HBase master. It is not a task definition that can be put on a task queue.;;;N
org/apache/hadoop/hbase/master/region/MasterRegion.class;;;public final class org.apache.hadoop.hbase.master.region.MasterRegion {\n  public void update(org.apache.hadoop.hbase.master.region.UpdateMasterRegion) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result get(org.apache.hadoop.hbase.client.Get) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.ResultScanner getScanner(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.RegionScanner getRegionScanner(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HRegion$FlushResult flush(boolean) throws java.io.IOException;\n  public void requestRollAll();\n  public void waitUntilWalRollFinished() throws java.lang.InterruptedException;\n  public void close(boolean);\n  public static org.apache.hadoop.hbase.master.region.MasterRegion create(org.apache.hadoop.hbase.master.region.MasterRegionParams) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/region/MasterRegionFactory.class;;;public final class org.apache.hadoop.hbase.master.region.MasterRegionFactory {\n  public static final java.lang.String ARCHIVED_WAL_SUFFIX;\n  public static final java.lang.String ARCHIVED_HFILE_SUFFIX;\n  public static final java.lang.String USE_HSYNC_KEY;\n  public static final java.lang.String MASTER_STORE_DIR;\n  public static final java.lang.String TRACKER_IMPL;\n  public static final org.apache.hadoop.hbase.TableName TABLE_NAME;\n  public static final byte[] PROC_FAMILY;\n  public static final byte[] REGION_SERVER_FAMILY;\n  public org.apache.hadoop.hbase.master.region.MasterRegionFactory();\n  public static org.apache.hadoop.hbase.master.region.MasterRegion create(org.apache.hadoop.hbase.Server) throws java.io.IOException;\n}\n;;;No. This class provides static fields and methods to create instances of a MasterRegion, but it does not define any specific message format.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/region/MasterRegionFlusherAndCompactor.class;;;class org.apache.hadoop.hbase.master.region.MasterRegionFlusherAndCompactor implements java.io.Closeable {\n  public void close();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/region/MasterRegionParams.class;;;public class org.apache.hadoop.hbase.master.region.MasterRegionParams {\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams();\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams server(org.apache.hadoop.hbase.Server);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams regionDirName(java.lang.String);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams tableDescriptor(org.apache.hadoop.hbase.client.TableDescriptor);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams flushSize(long);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams flushPerChanges(long);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams flushIntervalMs(long);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams compactMin(int);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams maxWals(int);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams useHsync(boolean);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams ringBufferSlotCount(int);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams rollPeriodMs(long);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams archivedWalSuffix(java.lang.String);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams archivedHFileSuffix(java.lang.String);\n  public org.apache.hadoop.hbase.master.region.MasterRegionParams useMetaCellComparator(boolean);\n  public org.apache.hadoop.hbase.Server server();\n  public java.lang.String regionDirName();\n  public org.apache.hadoop.hbase.client.TableDescriptor tableDescriptor();\n  public long flushSize();\n  public long flushPerChanges();\n  public long flushIntervalMs();\n  public int compactMin();\n  public int maxWals();\n  public java.lang.Boolean useHsync();\n  public int ringBufferSlotCount();\n  public long rollPeriodMs();\n  public java.lang.String archivedWalSuffix();\n  public java.lang.String archivedHFileSuffix();\n  public java.lang.Boolean useMetaCellComparator();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/region/MasterRegionUtils.class;;;final class org.apache.hadoop.hbase.master.region.MasterRegionUtils {\n}\n;;;No.;;;N;;;No, it is not a task or a message definition. It is a utility class and does not represent a task or a message that can be put on a queue.;;;N
org/apache/hadoop/hbase/master/region/MasterRegionWALRoller.class;;;public final class org.apache.hadoop.hbase.master.region.MasterRegionWALRoller extends org.apache.hadoop.hbase.wal.AbstractWALRoller<org.apache.hadoop.hbase.Abortable> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/region/RegionScannerAsResultScanner.class;;;class org.apache.hadoop.hbase.master.region.RegionScannerAsResultScanner implements org.apache.hadoop.hbase.client.ResultScanner {\n  public boolean renewLease();\n  public org.apache.hadoop.hbase.client.Result next() throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.metrics.ScanMetrics getScanMetrics();\n  public void close();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/region/UpdateMasterRegion.class;;;public interface org.apache.hadoop.hbase.master.region.UpdateMasterRegion {\n  public abstract void update(org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException;\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/replication/AbstractPeerNoLockProcedure.class;;;public abstract class org.apache.hadoop.hbase.master.replication.AbstractPeerNoLockProcedure<TState> extends org.apache.hadoop.hbase.procedure2.StateMachineProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, TState> implements org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface {\n  public java.lang.String getPeerId();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/AbstractPeerProcedure.class;;;public abstract class org.apache.hadoop.hbase.master.replication.AbstractPeerProcedure<TState> extends org.apache.hadoop.hbase.master.replication.AbstractPeerNoLockProcedure<TState> implements org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface {\n  public org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch getLatch();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/AddPeerProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.AddPeerProcedure extends org.apache.hadoop.hbase.master.replication.ModifyPeerProcedure {\n  public org.apache.hadoop.hbase.master.replication.AddPeerProcedure();\n  public org.apache.hadoop.hbase.master.replication.AddPeerProcedure(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig, boolean);\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/ClaimReplicationQueueRemoteProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.ClaimReplicationQueueRemoteProcedure extends org.apache.hadoop.hbase.master.procedure.ServerRemoteProcedure implements org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface, org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName> {\n  public org.apache.hadoop.hbase.master.replication.ClaimReplicationQueueRemoteProcedure();\n  public org.apache.hadoop.hbase.master.replication.ClaimReplicationQueueRemoteProcedure(org.apache.hadoop.hbase.ServerName, java.lang.String, org.apache.hadoop.hbase.ServerName);\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public boolean hasMetaTableRegion();\n  public org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType getServerOperationType();\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/replication/ClaimReplicationQueuesProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.ClaimReplicationQueuesProcedure extends org.apache.hadoop.hbase.procedure2.Procedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv> implements org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface {\n  public org.apache.hadoop.hbase.master.replication.ClaimReplicationQueuesProcedure();\n  public org.apache.hadoop.hbase.master.replication.ClaimReplicationQueuesProcedure(org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public boolean hasMetaTableRegion();\n  public org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface$ServerOperationType getServerOperationType();\n}\n;;;No. It is a class definition for a procedure that may be executed by HBase, but it does not define any message or data structure that would be put on a message queue.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/master/replication/DisablePeerProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.DisablePeerProcedure extends org.apache.hadoop.hbase.master.replication.ModifyPeerProcedure {\n  public org.apache.hadoop.hbase.master.replication.DisablePeerProcedure();\n  public org.apache.hadoop.hbase.master.replication.DisablePeerProcedure(java.lang.String);\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n}\n;;;No;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/replication/EnablePeerProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.EnablePeerProcedure extends org.apache.hadoop.hbase.master.replication.ModifyPeerProcedure {\n  public org.apache.hadoop.hbase.master.replication.EnablePeerProcedure();\n  public org.apache.hadoop.hbase.master.replication.EnablePeerProcedure(java.lang.String);\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/ModifyPeerProcedure$1.class;;;class org.apache.hadoop.hbase.master.replication.ModifyPeerProcedure$1 {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/ModifyPeerProcedure.class;;;public abstract class org.apache.hadoop.hbase.master.replication.ModifyPeerProcedure extends org.apache.hadoop.hbase.master.replication.AbstractPeerProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$PeerModificationState> {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure$1.class;;;class org.apache.hadoop.hbase.master.replication.RecoverStandbyProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/replication/RecoverStandbyProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.RecoverStandbyProcedure extends org.apache.hadoop.hbase.master.replication.AbstractPeerNoLockProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$RecoverStandbyState> {\n  public org.apache.hadoop.hbase.master.replication.RecoverStandbyProcedure();\n  public org.apache.hadoop.hbase.master.replication.RecoverStandbyProcedure(java.lang.String, boolean);\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/RefreshPeerProcedure$1.class;;;class org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure$1 {\n}\n;;;No.;;;N;;;No. It is a nested class within the org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure class and is not a message definition nor a task definition.;;;N
org/apache/hadoop/hbase/master/replication/RefreshPeerProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure extends org.apache.hadoop.hbase.master.procedure.ServerRemoteProcedure implements org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface, org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteProcedure<org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName> {\n  public org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure();\n  public org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure(java.lang.String, org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType, org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.master.replication.RefreshPeerProcedure(java.lang.String, org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType, org.apache.hadoop.hbase.ServerName, int);\n  public java.lang.String getPeerId();\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/replication/RemovePeerProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.RemovePeerProcedure extends org.apache.hadoop.hbase.master.replication.ModifyPeerProcedure {\n  public org.apache.hadoop.hbase.master.replication.RemovePeerProcedure();\n  public org.apache.hadoop.hbase.master.replication.RemovePeerProcedure(java.lang.String);\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/ReplicationPeerManager.class;;;public class org.apache.hadoop.hbase.master.replication.ReplicationPeerManager {\n  public void addPeer(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig, boolean) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public void removePeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public void enablePeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public void disablePeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public void updatePeerConfig(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public java.util.List<org.apache.hadoop.hbase.replication.ReplicationPeerDescription> listPeers(java.util.regex.Pattern);\n  public java.util.Optional<org.apache.hadoop.hbase.replication.ReplicationPeerConfig> getPeerConfig(java.lang.String);\n  public void setPeerNewSyncReplicationState(java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public void transitPeerSyncReplicationState(java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public void removeAllQueues(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public void removeAllQueuesAndHFileRefs(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public java.util.List<java.lang.String> getSerialPeerIdsBelongsTo(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.replication.ReplicationQueueStorage getQueueStorage();\n  public static org.apache.hadoop.hbase.master.replication.ReplicationPeerManager create(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.conf.Configuration, java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public void acquireSyncReplicationPeerLock() throws java.lang.InterruptedException;\n  public void releaseSyncReplicationPeerLock();\n}\n;;;No. This is a class for managing replication peers in Apache HBase, but it does not define a specific message that could be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager$1.class;;;class org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager$1 implements org.apache.hadoop.hbase.master.ServerListener {\n  public void serverAdded(org.apache.hadoop.hbase.ServerName);\n}\n;;;No. It is an implementation of the ServerListener interface and not a message definition.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager$UsedReplayWorkersForPeer.class;;;final class org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager$UsedReplayWorkersForPeer {\n  public org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager$UsedReplayWorkersForPeer(java.lang.String);\n  public void used(org.apache.hadoop.hbase.ServerName);\n  public java.util.Optional<org.apache.hadoop.hbase.ServerName> acquire(org.apache.hadoop.hbase.master.ServerManager);\n  public void release(org.apache.hadoop.hbase.ServerName);\n  public void suspend(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public void wake(org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALManager.class;;;public class org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager {\n  public org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALManager(org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException, org.apache.hadoop.hbase.replication.ReplicationException;\n  public void registerPeer(java.lang.String);\n  public void unregisterPeer(java.lang.String);\n  public org.apache.hadoop.hbase.ServerName acquirePeerWorker(java.lang.String, org.apache.hadoop.hbase.procedure2.Procedure<?>) throws org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;\n  public void releasePeerWorker(java.lang.String, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler);\n  public void addUsedPeerWorker(java.lang.String, org.apache.hadoop.hbase.ServerName);\n  public void createPeerRemoteWALDir(java.lang.String) throws java.io.IOException;\n  public void renameToPeerReplayWALDir(java.lang.String) throws java.io.IOException;\n  public void renameToPeerSnapshotWALDir(java.lang.String) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.Path> getReplayWALsAndCleanUpUnusedFiles(java.lang.String) throws java.io.IOException;\n  public void removePeerRemoteWALs(java.lang.String) throws java.io.IOException;\n  public java.lang.String removeWALRootPath(org.apache.hadoop.fs.Path);\n  public void finishReplayWAL(java.lang.String) throws java.io.IOException;\n  public boolean isReplayWALFinished(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getRemoteWALDir();\n}\n;;;No. This class appears to provide functionality for replaying and managing Write Ahead Logs (WALs) in HBase, but it is not a message definition itself that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure$1.class;;;class org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALProcedure$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be an inner class within the SyncReplicationReplayWALProcedure class in the HBase Master replication module, but it does not define a specific message or task.;;;N
org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALProcedure extends org.apache.hadoop.hbase.master.replication.AbstractPeerNoLockProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$SyncReplicationReplayWALState> {\n  public org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALProcedure();\n  public org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALProcedure(java.lang.String, java.util.List<java.lang.String>);\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n}\n;;;No. This is a class definition in Java for a procedure related to the HBase master replication feature. It is not a message definition that can be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/replication/SyncReplicationReplayWALRemoteProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALRemoteProcedure extends org.apache.hadoop.hbase.master.procedure.ServerRemoteProcedure implements org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface {\n  public org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALRemoteProcedure();\n  public org.apache.hadoop.hbase.master.replication.SyncReplicationReplayWALRemoteProcedure(java.lang.String, java.util.List<java.lang.String>, org.apache.hadoop.hbase.ServerName);\n  public java.util.Optional<org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher$RemoteOperation> remoteCallBuild(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.ServerName);\n  public java.lang.String getPeerId();\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n  public java.util.Optional remoteCallBuild(java.lang.Object, java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure$1.class;;;class org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/replication/TransitPeerSyncReplicationStateProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure extends org.apache.hadoop.hbase.master.replication.AbstractPeerProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$PeerSyncReplicationStateTransitionState> {\n  public org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure();\n  public org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure(java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState);\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/replication/UpdatePeerConfigProcedure.class;;;public class org.apache.hadoop.hbase.master.replication.UpdatePeerConfigProcedure extends org.apache.hadoop.hbase.master.replication.ModifyPeerProcedure {\n  public org.apache.hadoop.hbase.master.replication.UpdatePeerConfigProcedure();\n  public org.apache.hadoop.hbase.master.replication.UpdatePeerConfigProcedure(java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig);\n  public org.apache.hadoop.hbase.master.procedure.PeerProcedureInterface$PeerOperationType getPeerOperationType();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/master/slowlog/SlowLogMasterService.class;;;public class org.apache.hadoop.hbase.master.slowlog.SlowLogMasterService {\n  public org.apache.hadoop.hbase.master.slowlog.SlowLogMasterService(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.master.MasterServices);\n  public void init() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler$1.class;;;class org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler$1 implements org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask {\n  public void editRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/snapshot/DisabledTableSnapshotHandler.class;;;public class org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler extends org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler {\n  public org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.snapshot.SnapshotManager) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler prepare() throws java.lang.Exception;\n  public void snapshotRegions(java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName>>) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler prepare() throws java.lang.Exception;\n  public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/snapshot/EnabledTableSnapshotHandler.class;;;public class org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler extends org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler {\n  public org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.snapshot.SnapshotManager) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler prepare() throws java.lang.Exception;\n  public org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler prepare() throws java.lang.Exception;\n  public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception;\n}\n;;;No. This is a class definition and not a message definition. It cannot be put directly on a message queue. However, objects of this class could potentially be wrapped in a message and put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/snapshot/MasterSnapshotVerifier.class;;;public final class org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier {\n  public org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.fs.FileSystem);\n  public void verifySnapshot(org.apache.hadoop.fs.Path, boolean) throws org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException, java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache$RefreshCacheTask.class;;;public class org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$RefreshCacheTask extends java.util.TimerTask {\n  public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$RefreshCacheTask(org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache);\n  public void run();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache$SnapshotDirectoryInfo.class;;;class org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotDirectoryInfo {\n  public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotDirectoryInfo(long, java.util.Collection<java.lang.String>);\n  public java.util.Collection<java.lang.String> getFiles();\n  public boolean hasBeenModified(long);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache$SnapshotFileInspector.class;;;interface org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotFileInspector {\n  public abstract java.util.Collection<java.lang.String> filesUnderSnapshot(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/snapshot/SnapshotFileCache.class;;;public class org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache implements org.apache.hadoop.hbase.Stoppable {\n  public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache(org.apache.hadoop.conf.Configuration, long, long, java.lang.String, org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotFileInspector) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, long, long, java.lang.String, org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotFileInspector);\n  public synchronized void triggerCacheRefreshForTesting();\n  public java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getUnreferencedFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>, org.apache.hadoop.hbase.master.snapshot.SnapshotManager) throws java.io.IOException;\n  public void stop(java.lang.String);\n  public boolean isStopped();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/snapshot/SnapshotHFileCleaner$1.class;;;class org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner$1 implements org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache$SnapshotFileInspector {\n  public java.util.Collection<java.lang.String> filesUnderSnapshot(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n;;;No. This is a class definition, but it does not define a message that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/snapshot/SnapshotHFileCleaner.class;;;public class org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate {\n  public static final java.lang.String HFILE_CACHE_REFRESH_PERIOD_CONF_KEY;\n  public org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner();\n  public java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>);\n  public void init(java.util.Map<java.lang.String, java.lang.Object>);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache getFileCacheForTesting();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/snapshot/SnapshotManager$1.class;;;class org.apache.hadoop.hbase.master.snapshot.SnapshotManager$1 extends org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil$NonceProcedureRunnable {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/snapshot/SnapshotManager.class;;;public class org.apache.hadoop.hbase.master.snapshot.SnapshotManager extends org.apache.hadoop.hbase.procedure.MasterProcedureManager implements org.apache.hadoop.hbase.Stoppable {\n  public static final java.lang.String HBASE_SNAPSHOT_SENTINELS_CLEANUP_TIMEOUT_MILLIS;\n  public static final long SNAPSHOT_SENTINELS_CLEANUP_TIMEOUT_MILLS_DEFAULT;\n  public static final java.lang.String HBASE_SNAPSHOT_ENABLED;\n  public static final java.lang.String ONLINE_SNAPSHOT_CONTROLLER_DESCRIPTION;\n  public static final java.lang.String SNAPSHOT_POOL_THREADS_KEY;\n  public static final int SNAPSHOT_POOL_THREADS_DEFAULT;\n  public static final java.lang.String SNAPSHOT_MAX_FILE_SIZE_PRESERVE;\n  public static final java.lang.String SNAPSHOT_PROCEDURE_ENABLED;\n  public static final boolean SNAPSHOT_PROCEDURE_ENABLED_DEFAULT;\n  public org.apache.hadoop.hbase.master.snapshot.SnapshotManager();\n  public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription> getCompletedSnapshots() throws java.io.IOException;\n  public void deleteSnapshot(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription) throws java.io.IOException;\n  public boolean isSnapshotDone(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription) throws java.io.IOException;\n  public boolean isTakingSnapshot(org.apache.hadoop.hbase.TableName);\n  public boolean isTableTakingAnySnapshot(org.apache.hadoop.hbase.TableName);\n  public synchronized void prepareWorkingDirectory(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription) throws org.apache.hadoop.hbase.snapshot.HBaseSnapshotException;\n  public java.util.concurrent.locks.ReadWriteLock getTakingSnapshotLock();\n  public synchronized boolean isTakingAnySnapshot();\n  public void takeSnapshot(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription) throws java.io.IOException;\n  public synchronized long takeSnapshot(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, long, long) throws java.io.IOException;\n  public synchronized void setSnapshotHandlerForTesting(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.master.SnapshotSentinel);\n  public long restoreOrCloneSnapshot(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.util.NonceKey, boolean, java.lang.String) throws java.io.IOException;\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public void checkSnapshotSupport() throws java.lang.UnsupportedOperationException;\n  public void initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster) throws org.apache.zookeeper.KeeperException, java.io.IOException, java.lang.UnsupportedOperationException;\n  public java.lang.String getProcedureSignature();\n  public void execProcedure(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException;\n  public void checkPermissions(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription, org.apache.hadoop.hbase.security.access.AccessChecker, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public boolean isProcedureDone(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException;\n  public void registerSnapshotProcedure(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, long);\n  public void unregisterSnapshotProcedure(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, long);\n  public boolean snapshotProcedureEnabled();\n  public org.apache.hadoop.hbase.ServerName acquireSnapshotVerifyWorker(org.apache.hadoop.hbase.master.procedure.SnapshotVerifyProcedure) throws org.apache.hadoop.hbase.procedure2.ProcedureSuspendedException;\n  public void releaseSnapshotVerifyWorker(org.apache.hadoop.hbase.master.procedure.SnapshotVerifyProcedure, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler);\n  public java.lang.Integer getAvailableWorker(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes, the class org.apache.hadoop.hbase.master.snapshot.SnapshotManager contains methods that define actions related to managing snapshots, and as such could be used as a message definition on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/snapshot/TakeSnapshotHandler.class;;;public abstract class org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler extends org.apache.hadoop.hbase.executor.EventHandler implements org.apache.hadoop.hbase.master.SnapshotSentinel,org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare {\n  public org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.snapshot.SnapshotManager) throws java.io.IOException;\n  public org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler prepare() throws java.lang.Exception;\n  public void process();\n  public void cancel(java.lang.String);\n  public boolean isFinished();\n  public long getCompletionTimestamp();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription getSnapshot();\n  public org.apache.hadoop.hbase.errorhandling.ForeignException getExceptionIfFailed();\n  public void rethrowExceptionIfFailed() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public void rethrowException() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public boolean hasException();\n  public org.apache.hadoop.hbase.errorhandling.ForeignException getException();\n  public org.apache.hadoop.hbase.executor.EventHandler prepare() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer$1.class;;;class org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer$ClientZkUpdater.class;;;final class org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer$ClientZkUpdater extends java.lang.Thread {\n  public org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer$ClientZkUpdater(org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer, java.lang.String, org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer$ZKData);\n  public void run();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer$ZKData.class;;;final class org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer$ZKData {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.class;;;public abstract class org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.Server);\n  public void start() throws org.apache.zookeeper.KeeperException;\n  public void nodeCreated(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public synchronized void nodeDeleted(java.lang.String);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/zksyncer/MasterAddressSyncer.class;;;public class org.apache.hadoop.hbase.master.zksyncer.MasterAddressSyncer extends org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer {\n  public org.apache.hadoop.hbase.master.zksyncer.MasterAddressSyncer(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.Server);\n}\n;;;No. It is a class definition for a Java class that extends another class. It does not define any specific message format or structure.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/master/zksyncer/MetaLocationSyncer.class;;;public class org.apache.hadoop.hbase.master.zksyncer.MetaLocationSyncer extends org.apache.hadoop.hbase.master.zksyncer.ClientZKSyncer {\n  public org.apache.hadoop.hbase.master.zksyncer.MetaLocationSyncer(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.Server);\n  public void setMetaReplicaCount(int);\n}\n;;;No, it is a class definition for a Java class that extends another class and has a constructor and a method. It is not a message definition that can be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a Java class.;;;N
org/apache/hadoop/hbase/mob/CachedMobFile.class;;;public class org.apache.hadoop.hbase.mob.CachedMobFile extends org.apache.hadoop.hbase.mob.MobFile implements java.lang.Comparable<org.apache.hadoop.hbase.mob.CachedMobFile> {\n  public org.apache.hadoop.hbase.mob.CachedMobFile(org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public static org.apache.hadoop.hbase.mob.CachedMobFile create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig) throws java.io.IOException;\n  public void access(long);\n  public int compareTo(org.apache.hadoop.hbase.mob.CachedMobFile);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public void open() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getReferenceCount();\n  public int compareTo(java.lang.Object);\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a Java class that extends another class and implements the Comparable interface. It contains methods for creating and accessing objects, as well as methods for comparing objects and managing references.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor$1.class;;;final class org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor$1 extends java.lang.ThreadLocal<java.lang.Boolean> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor$2.class;;;final class org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor$2 extends java.lang.ThreadLocal<java.lang.Boolean> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor$3.class;;;final class org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor$3 extends java.lang.ThreadLocal<java.util.HashMap<java.lang.String, java.lang.Long>> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor$4.class;;;class org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor$4 implements org.apache.hadoop.hbase.regionserver.compactions.Compactor$InternalScannerFactory {\n  public org.apache.hadoop.hbase.regionserver.ScanType getScanType(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n  public org.apache.hadoop.hbase.regionserver.InternalScanner createScanner(org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner>, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, long) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor$5.class;;;class org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor$5 implements org.apache.hadoop.hbase.regionserver.compactions.Compactor$CellSinkFactory<org.apache.hadoop.hbase.regionserver.StoreFileWriter> {\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n  public java.lang.Object createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer) throws java.io.IOException;\n}\n;;;No, it is a Java class definition and not a message definition.;;;N;;;No, it is not a task definition. It is a class implementation for a specific function.;;;N
org/apache/hadoop/hbase/mob/DefaultMobStoreCompactor.class;;;public class org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor extends org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor {\n  public org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HStore);\n  public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher$1.class;;;final class org.apache.hadoop.hbase.mob.DefaultMobStoreFlusher$1 extends java.lang.ThreadLocal<java.util.Set<java.lang.String>> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/DefaultMobStoreFlusher.class;;;public class org.apache.hadoop.hbase.mob.DefaultMobStoreFlusher extends org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher {\n  public org.apache.hadoop.hbase.mob.DefaultMobStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HStore) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.Path> flushSnapshot(org.apache.hadoop.hbase.regionserver.MemStoreSnapshot, long, org.apache.hadoop.hbase.monitoring.MonitoredTask, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/mob/ExpiredMobFileCleaner.class;;;public class org.apache.hadoop.hbase.mob.ExpiredMobFileCleaner extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public org.apache.hadoop.hbase.mob.ExpiredMobFileCleaner();\n  public void cleanExpiredMobFiles(java.lang.String, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor) throws java.io.IOException;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public int run(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/mob/ManualMobMaintHFileCleaner.class;;;public class org.apache.hadoop.hbase.mob.ManualMobMaintHFileCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate {\n  public org.apache.hadoop.hbase.mob.ManualMobMaintHFileCleaner();\n  public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/mob/MobCell.class;;;public class org.apache.hadoop.hbase.mob.MobCell implements java.io.Closeable {\n  public org.apache.hadoop.hbase.mob.MobCell(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.mob.MobCell(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.regionserver.StoreFileScanner);\n  public org.apache.hadoop.hbase.Cell getCell();\n  public void close() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/MobConstants.class;;;public final class org.apache.hadoop.hbase.mob.MobConstants {\n  public static final java.lang.String MOB_SCAN_RAW;\n  public static final java.lang.String MOB_CACHE_BLOCKS;\n  public static final java.lang.String MOB_SCAN_REF_ONLY;\n  public static final java.lang.String EMPTY_VALUE_ON_MOBCELL_MISS;\n  public static final java.lang.String MOB_FILE_CACHE_SIZE_KEY;\n  public static final int DEFAULT_MOB_FILE_CACHE_SIZE;\n  public static final java.lang.String MOB_DIR_NAME;\n  public static final java.lang.String MOB_REGION_NAME;\n  public static final byte[] MOB_REGION_NAME_BYTES;\n  public static final java.lang.String MOB_CLEANER_PERIOD;\n  public static final java.lang.String DEPRECATED_MOB_CLEANER_PERIOD;\n  public static final int DEFAULT_MOB_CLEANER_PERIOD;\n  public static final java.lang.String MOB_CACHE_EVICT_PERIOD;\n  public static final java.lang.String MOB_CACHE_EVICT_REMAIN_RATIO;\n  public static final org.apache.hadoop.hbase.Tag MOB_REF_TAG;\n  public static final float DEFAULT_EVICT_REMAIN_RATIO;\n  public static final long DEFAULT_MOB_CACHE_EVICT_PERIOD;\n  public static final java.lang.String TEMP_DIR_NAME;\n  public static final java.lang.String MOB_MAJOR_COMPACTION_REGION_BATCH_SIZE;\n  public static final int DEFAULT_MOB_MAJOR_COMPACTION_REGION_BATCH_SIZE;\n  public static final java.lang.String MOB_COMPACTION_CHORE_PERIOD;\n  public static final int DEFAULT_MOB_COMPACTION_CHORE_PERIOD;\n  public static final java.lang.String MOB_COMPACTOR_CLASS_KEY;\n  public static final java.lang.String OPTIMIZED_MOB_COMPACTION_TYPE;\n  public static final java.lang.String FULL_MOB_COMPACTION_TYPE;\n  public static final java.lang.String MOB_COMPACTION_TYPE_KEY;\n  public static final java.lang.String DEFAULT_MOB_COMPACTION_TYPE;\n  public static final java.lang.String MOB_COMPACTION_MAX_FILE_SIZE_KEY;\n  public static final long DEFAULT_MOB_COMPACTION_MAX_FILE_SIZE;\n  public static final java.lang.String MOB_UNSAFE_DISCARD_MISS_KEY;\n  public static final boolean DEFAULT_MOB_DISCARD_MISS;\n  public static final java.lang.String MIN_AGE_TO_ARCHIVE_KEY;\n  public static final long DEFAULT_MIN_AGE_TO_ARCHIVE;\n  public static final java.lang.String BULKLOAD_DIR_NAME;\n  public static final byte[] MOB_TABLE_LOCK_SUFFIX;\n  public static final java.lang.String EMPTY_STRING;\n  public static final java.lang.String MOB_COMPACTION_MERGEABLE_THRESHOLD;\n  public static final long DEFAULT_MOB_COMPACTION_MERGEABLE_THRESHOLD;\n  public static final java.lang.String MOB_DELFILE_MAX_COUNT;\n  public static final int DEFAULT_MOB_DELFILE_MAX_COUNT;\n  public static final java.lang.String MOB_COMPACTION_BATCH_SIZE;\n  public static final int DEFAULT_MOB_COMPACTION_BATCH_SIZE;\n  public static final java.lang.String MOB_COMPACTION_THREADS_MAX;\n  public static final int DEFAULT_MOB_COMPACTION_THREADS_MAX;\n}\n;;;No. This is a class containing constants related to the Apache HBase Mob feature, but it is not a message definition that would typically be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/MobFile.class;;;public class org.apache.hadoop.hbase.mob.MobFile {\n  public org.apache.hadoop.hbase.regionserver.StoreFileScanner getScanner() throws java.io.IOException;\n  public org.apache.hadoop.hbase.mob.MobCell readCell(org.apache.hadoop.hbase.Cell, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.mob.MobCell readCell(org.apache.hadoop.hbase.Cell, boolean, long) throws java.io.IOException;\n  public java.lang.String getFileName();\n  public void open() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public static org.apache.hadoop.hbase.mob.MobFile create(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/MobFileCache$EvictionThread.class;;;class org.apache.hadoop.hbase.mob.MobFileCache$EvictionThread extends java.lang.Thread {\n  public org.apache.hadoop.hbase.mob.MobFileCache$EvictionThread(org.apache.hadoop.hbase.mob.MobFileCache);\n  public void run();\n}\n;;;No.;;;N;;;Yes, it is a task definition that can be put on a task queue.;;;Y
org/apache/hadoop/hbase/mob/MobFileCache.class;;;public class org.apache.hadoop.hbase.mob.MobFileCache {\n  public org.apache.hadoop.hbase.mob.MobFileCache(org.apache.hadoop.conf.Configuration);\n  public void evict();\n  public void evictFile(java.lang.String);\n  public org.apache.hadoop.hbase.mob.MobFile openFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.hfile.CacheConfig) throws java.io.IOException;\n  public void closeFile(org.apache.hadoop.hbase.mob.MobFile);\n  public void shutdown();\n  public int getCacheSize();\n  public long getAccessCount();\n  public long getMissCount();\n  public long getEvictedFileCount();\n  public double getHitRatio();\n  public void printStatistics();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It appears to be a class representing a cache for storing and managing files in the Hadoop distributed file system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/MobFileCleanerChore.class;;;public class org.apache.hadoop.hbase.mob.MobFileCleanerChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.mob.MobFileCleanerChore(org.apache.hadoop.hbase.master.HMaster);\n  public org.apache.hadoop.hbase.mob.MobFileCleanerChore();\n  public void cleanupObsoleteMobFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void archiveMobFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, byte[], java.util.List<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n}\n;;;No, this class is not a message definition. It is a class definition for a scheduled task that performs cleaning and archiving of Mob files in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/MobFileCompactionChore.class;;;public class org.apache.hadoop.hbase.mob.MobFileCompactionChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.mob.MobFileCompactionChore(org.apache.hadoop.hbase.master.HMaster);\n  public org.apache.hadoop.hbase.mob.MobFileCompactionChore(org.apache.hadoop.conf.Configuration, int);\n  public void performMajorCompactionInBatches(org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor) throws java.io.IOException, java.lang.InterruptedException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/MobFileName.class;;;public final class org.apache.hadoop.hbase.mob.MobFileName {\n  public static final java.lang.String REGION_SEP;\n  public static org.apache.hadoop.hbase.mob.MobFileName create(byte[], java.lang.String, java.lang.String, java.lang.String);\n  public static org.apache.hadoop.hbase.mob.MobFileName create(java.lang.String, java.lang.String, java.lang.String, java.lang.String);\n  public static org.apache.hadoop.hbase.mob.MobFileName create(java.lang.String);\n  public static boolean isOldMobFileName(java.lang.String);\n  public static java.lang.String getStartKeyFromName(java.lang.String);\n  public static java.lang.String getDateFromName(java.lang.String);\n  public java.lang.String getStartKey();\n  public java.lang.String getRegionName();\n  public java.lang.String getDate();\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String getFileName();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/MobStoreEngine.class;;;public class org.apache.hadoop.hbase.mob.MobStoreEngine extends org.apache.hadoop.hbase.regionserver.DefaultStoreEngine {\n  public static final java.lang.String MOB_COMPACTOR_CLASS_KEY;\n  public org.apache.hadoop.hbase.mob.MobStoreEngine();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/MobUtils$1.class;;;final class org.apache.hadoop.hbase.mob.MobUtils$1 extends java.lang.ThreadLocal<java.text.SimpleDateFormat> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/mob/MobUtils.class;;;public final class org.apache.hadoop.hbase.mob.MobUtils {\n  public static final java.lang.String SEP;\n  public static java.lang.String formatDate(java.util.Date);\n  public static java.util.Date parseDate(java.lang.String) throws java.text.ParseException;\n  public static boolean isMobReferenceCell(org.apache.hadoop.hbase.Cell);\n  public static java.util.Optional<java.lang.String> getTableNameString(org.apache.hadoop.hbase.Cell);\n  public static java.util.Optional<org.apache.hadoop.hbase.TableName> getTableName(org.apache.hadoop.hbase.Cell);\n  public static boolean hasMobReferenceTag(java.util.List<org.apache.hadoop.hbase.Tag>);\n  public static boolean isRawMobScan(org.apache.hadoop.hbase.client.Scan);\n  public static boolean isRefOnlyScan(org.apache.hadoop.hbase.client.Scan);\n  public static boolean isCacheMobBlocks(org.apache.hadoop.hbase.client.Scan);\n  public static void setCacheMobBlocks(org.apache.hadoop.hbase.client.Scan, boolean);\n  public static void cleanExpiredMobFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, org.apache.hadoop.hbase.io.hfile.CacheConfig, long) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getMobHome(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.fs.Path getMobHome(org.apache.hadoop.fs.Path);\n  public static org.apache.hadoop.fs.Path getQualifiedMobRootDir(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getMobTableDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName);\n  public static org.apache.hadoop.fs.Path getMobRegionPath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName);\n  public static org.apache.hadoop.fs.Path getMobRegionPath(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName);\n  public static org.apache.hadoop.fs.Path getMobFamilyPath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, java.lang.String);\n  public static org.apache.hadoop.fs.Path getMobFamilyPath(org.apache.hadoop.fs.Path, java.lang.String);\n  public static org.apache.hadoop.hbase.client.RegionInfo getMobRegionInfo(org.apache.hadoop.hbase.TableName);\n  public static boolean isMobRegionInfo(org.apache.hadoop.hbase.client.RegionInfo);\n  public static boolean isMobRegionName(org.apache.hadoop.hbase.TableName, byte[]);\n  public static void removeMobFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.TableName, org.apache.hadoop.fs.Path, byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.Cell createMobRefCell(org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.Tag);\n  public static org.apache.hadoop.hbase.Cell createMobRefCell(org.apache.hadoop.hbase.Cell, byte[], byte[]);\n  public static org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, java.lang.String, org.apache.hadoop.fs.Path, long, org.apache.hadoop.hbase.io.compress.Compression$Algorithm, java.lang.String, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.crypto.Encryption$Context, boolean, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, org.apache.hadoop.hbase.mob.MobFileName, org.apache.hadoop.fs.Path, long, org.apache.hadoop.hbase.io.compress.Compression$Algorithm, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.crypto.Encryption$Context, boolean) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, org.apache.hadoop.fs.Path, long, org.apache.hadoop.hbase.io.compress.Compression$Algorithm, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.crypto.Encryption$Context, org.apache.hadoop.hbase.util.ChecksumType, int, int, org.apache.hadoop.hbase.regionserver.BloomType, boolean) throws java.io.IOException;\n  public static boolean hasValidMobRefCellValue(org.apache.hadoop.hbase.Cell);\n  public static int getMobValueLength(org.apache.hadoop.hbase.Cell);\n  public static java.lang.String getMobFileName(org.apache.hadoop.hbase.Cell);\n  public static boolean hasMobColumns(org.apache.hadoop.hbase.client.TableDescriptor);\n  public static java.util.List<org.apache.hadoop.hbase.client.ColumnFamilyDescriptor> getMobColumnFamilies(org.apache.hadoop.hbase.client.TableDescriptor);\n  public static boolean isReadEmptyValueOnMobCellMiss(org.apache.hadoop.hbase.client.Scan);\n  public static boolean isMobFileExpired(org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, long, java.lang.String);\n  public static byte[] serializeMobFileRefs(org.apache.hbase.thirdparty.com.google.common.collect.SetMultimap<org.apache.hadoop.hbase.TableName, java.lang.String>);\n  public static org.apache.hbase.thirdparty.com.google.common.collect.ImmutableSetMultimap$Builder<org.apache.hadoop.hbase.TableName, java.lang.String> deserializeMobFileRefs(byte[]) throws java.lang.IllegalStateException;\n}\n;;;No, this class provides utility functions for working with HBase Mob files, but it does not define a message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/monitoring/MemoryBoundedLogMessageBuffer$LogMessage.class;;;class org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer$LogMessage {\n  public final byte[] message;\n  public final long timestamp;\n  public org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer$LogMessage(java.lang.String, long);\n  public long estimateHeapUsage();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/monitoring/MemoryBoundedLogMessageBuffer.class;;;public class org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer {\n  public org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer(long);\n  public synchronized void add(java.lang.String);\n  public synchronized void dumpTo(java.io.PrintWriter);\n}\n;;;No. This class is a memory buffer implementation for monitoring log messages in the Hadoop HBase system, but it is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/monitoring/MonitoredRPCHandler.class;;;public interface org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler extends org.apache.hadoop.hbase.monitoring.MonitoredTask {\n  public abstract java.lang.String getRPC();\n  public abstract java.lang.String getRPC(boolean);\n  public abstract long getRPCPacketLength();\n  public abstract java.lang.String getClient();\n  public abstract long getRPCStartTime();\n  public abstract long getRPCQueueTime();\n  public abstract boolean isRPCRunning();\n  public abstract boolean isOperationRunning();\n  public abstract void setRPC(java.lang.String, java.lang.Object[], long);\n  public abstract void setRPCPacket(org.apache.hbase.thirdparty.com.google.protobuf.Message);\n  public abstract void setConnection(java.lang.String, int);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/monitoring/MonitoredRPCHandlerImpl.class;;;public class org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl extends org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl implements org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler {\n  public org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl();\n  public synchronized org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl clone();\n  public java.lang.String getStatus();\n  public long getRPCQueueTime();\n  public long getRPCStartTime();\n  public synchronized java.lang.String getRPC();\n  public synchronized java.lang.String getRPC(boolean);\n  public long getRPCPacketLength();\n  public java.lang.String getClient();\n  public boolean isRPCRunning();\n  public synchronized boolean isOperationRunning();\n  public synchronized void setRPC(java.lang.String, java.lang.Object[], long);\n  public void setRPCPacket(org.apache.hbase.thirdparty.com.google.protobuf.Message);\n  public void setConnection(java.lang.String, int);\n  public synchronized void markComplete(java.lang.String);\n  public synchronized java.util.Map<java.lang.String, java.lang.Object> toMap();\n  public java.lang.String toString();\n  public java.lang.String prettyPrintJournal();\n  public void disableStatusJournal();\n  public void enableStatusJournal(boolean);\n  public java.util.List getStatusJournal();\n  public java.lang.String toJSON() throws java.io.IOException;\n  public void expireNow();\n  public void cleanup();\n  public void setWarnTime(long);\n  public void setDescription(java.lang.String);\n  public void setStatus(java.lang.String);\n  public void abort(java.lang.String);\n  public void resume(java.lang.String);\n  public void pause(java.lang.String);\n  public long getCompletionTimestamp();\n  public long getWarnTime();\n  public long getStateTime();\n  public org.apache.hadoop.hbase.monitoring.MonitoredTask$State getState();\n  public long getStatusTime();\n  public java.lang.String getDescription();\n  public long getStartTime();\n  public org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl clone();\n  public org.apache.hadoop.hbase.monitoring.MonitoredTask clone();\n  public java.lang.Object clone() throws java.lang.CloneNotSupportedException;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/monitoring/MonitoredTask$State.class;;;public final class org.apache.hadoop.hbase.monitoring.MonitoredTask$State extends java.lang.Enum<org.apache.hadoop.hbase.monitoring.MonitoredTask$State> {\n  public static final org.apache.hadoop.hbase.monitoring.MonitoredTask$State RUNNING;\n  public static final org.apache.hadoop.hbase.monitoring.MonitoredTask$State WAITING;\n  public static final org.apache.hadoop.hbase.monitoring.MonitoredTask$State COMPLETE;\n  public static final org.apache.hadoop.hbase.monitoring.MonitoredTask$State ABORTED;\n  public static org.apache.hadoop.hbase.monitoring.MonitoredTask$State[] values();\n  public static org.apache.hadoop.hbase.monitoring.MonitoredTask$State valueOf(java.lang.String);\n}\n;;;No, this is not a message definition. It is an enum class that defines the possible states of a monitored task in HBase. It does not define any specific messages to be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition that defines the possible states of a Monitored Task.;;;N
org/apache/hadoop/hbase/monitoring/MonitoredTask$StatusJournalEntry.class;;;public interface org.apache.hadoop.hbase.monitoring.MonitoredTask$StatusJournalEntry {\n  public abstract java.lang.String getStatus();\n  public abstract long getTimeStamp();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/monitoring/MonitoredTask.class;;;public interface org.apache.hadoop.hbase.monitoring.MonitoredTask extends java.lang.Cloneable {\n  public abstract long getStartTime();\n  public abstract java.lang.String getDescription();\n  public abstract java.lang.String getStatus();\n  public abstract long getStatusTime();\n  public abstract org.apache.hadoop.hbase.monitoring.MonitoredTask$State getState();\n  public abstract long getStateTime();\n  public abstract long getCompletionTimestamp();\n  public abstract long getWarnTime();\n  public abstract void markComplete(java.lang.String);\n  public abstract void pause(java.lang.String);\n  public abstract void resume(java.lang.String);\n  public abstract void abort(java.lang.String);\n  public abstract void expireNow();\n  public abstract void setStatus(java.lang.String);\n  public abstract void setDescription(java.lang.String);\n  public abstract void setWarnTime(long);\n  public abstract java.util.List<org.apache.hadoop.hbase.monitoring.MonitoredTask$StatusJournalEntry> getStatusJournal();\n  public abstract void enableStatusJournal(boolean);\n  public abstract void disableStatusJournal();\n  public abstract java.lang.String prettyPrintJournal();\n  public abstract void cleanup();\n  public abstract org.apache.hadoop.hbase.monitoring.MonitoredTask clone();\n  public abstract java.util.Map<java.lang.String, java.lang.Object> toMap() throws java.io.IOException;\n  public abstract java.lang.String toJSON() throws java.io.IOException;\n}\n;;;No. This is an interface in Java, but it does not define a full message with a payload and metadata that would be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/monitoring/MonitoredTaskImpl$StatusJournalEntryImpl.class;;;class org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl$StatusJournalEntryImpl implements org.apache.hadoop.hbase.monitoring.MonitoredTask$StatusJournalEntry {\n  public org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl$StatusJournalEntryImpl(java.lang.String, long);\n  public java.lang.String getStatus();\n  public long getTimeStamp();\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/monitoring/MonitoredTaskImpl.class;;;class org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl implements org.apache.hadoop.hbase.monitoring.MonitoredTask {\n  public org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl();\n  public synchronized org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl clone();\n  public long getStartTime();\n  public java.lang.String getDescription();\n  public java.lang.String getStatus();\n  public long getStatusTime();\n  public org.apache.hadoop.hbase.monitoring.MonitoredTask$State getState();\n  public long getStateTime();\n  public long getWarnTime();\n  public long getCompletionTimestamp();\n  public void markComplete(java.lang.String);\n  public void pause(java.lang.String);\n  public void resume(java.lang.String);\n  public void abort(java.lang.String);\n  public void setStatus(java.lang.String);\n  public void setDescription(java.lang.String);\n  public void setWarnTime(long);\n  public void cleanup();\n  public void expireNow();\n  public java.util.Map<java.lang.String, java.lang.Object> toMap();\n  public java.lang.String toJSON() throws java.io.IOException;\n  public java.lang.String toString();\n  public java.util.List<org.apache.hadoop.hbase.monitoring.MonitoredTask$StatusJournalEntry> getStatusJournal();\n  public void enableStatusJournal(boolean);\n  public void disableStatusJournal();\n  public java.lang.String prettyPrintJournal();\n  public java.lang.Object clone() throws java.lang.CloneNotSupportedException;\n  public org.apache.hadoop.hbase.monitoring.MonitoredTask clone();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/monitoring/StateDumpServlet.class;;;public abstract class org.apache.hadoop.hbase.monitoring.StateDumpServlet extends javax.servlet.http.HttpServlet {\n  public org.apache.hadoop.hbase.monitoring.StateDumpServlet();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/monitoring/TaskMonitor$1.class;;;class org.apache.hadoop.hbase.monitoring.TaskMonitor$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/monitoring/TaskMonitor$MonitorRunnable.class;;;class org.apache.hadoop.hbase.monitoring.TaskMonitor$MonitorRunnable implements java.lang.Runnable {\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/monitoring/TaskMonitor$PassthroughInvocationHandler.class;;;class org.apache.hadoop.hbase.monitoring.TaskMonitor$PassthroughInvocationHandler<T> implements java.lang.reflect.InvocationHandler {\n  public org.apache.hadoop.hbase.monitoring.TaskMonitor$PassthroughInvocationHandler(T);\n  public java.lang.Object invoke(java.lang.Object, java.lang.reflect.Method, java.lang.Object[]) throws java.lang.Throwable;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/monitoring/TaskMonitor$TaskAndWeakRefPair.class;;;class org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskAndWeakRefPair {\n  public org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskAndWeakRefPair(org.apache.hadoop.hbase.monitoring.MonitoredTask, org.apache.hadoop.hbase.monitoring.MonitoredTask);\n  public org.apache.hadoop.hbase.monitoring.MonitoredTask get();\n  public boolean isDead();\n}\n;;;No.;;;N;;;No. It is not a task definition that might be put on a task queue, but rather a class that represents a pair of MonitoredTask objects with additional methods to get and check their status.;;;N
org/apache/hadoop/hbase/monitoring/TaskMonitor$TaskFilter$TaskType.class;;;public final class org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter$TaskType extends java.lang.Enum<org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter$TaskType> {\n  public static final org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter$TaskType GENERAL;\n  public static final org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter$TaskType HANDLER;\n  public static final org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter$TaskType RPC;\n  public static final org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter$TaskType OPERATION;\n  public static final org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter$TaskType ALL;\n  public static org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter$TaskType[] values();\n  public static org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter$TaskType valueOf(java.lang.String);\n  public java.lang.String toString();\n}\n;;;No. It is an enum class used for filtering tasks in HBase monitoring. It is not a message definition that would be put on a message queue.;;;N;;;No, this is not a task definition. It is a class definition that defines an enumeration of TaskTypes.;;;N
org/apache/hadoop/hbase/monitoring/TaskMonitor$TaskFilter.class;;;interface org.apache.hadoop.hbase.monitoring.TaskMonitor$TaskFilter {\n  public abstract boolean filter(org.apache.hadoop.hbase.monitoring.MonitoredTask);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/monitoring/TaskMonitor.class;;;public class org.apache.hadoop.hbase.monitoring.TaskMonitor {\n  public static final java.lang.String MAX_TASKS_KEY;\n  public static final int DEFAULT_MAX_TASKS;\n  public static final java.lang.String RPC_WARN_TIME_KEY;\n  public static final long DEFAULT_RPC_WARN_TIME;\n  public static final java.lang.String EXPIRATION_TIME_KEY;\n  public static final long DEFAULT_EXPIRATION_TIME;\n  public static final java.lang.String MONITOR_INTERVAL_KEY;\n  public static final long DEFAULT_MONITOR_INTERVAL;\n  public static synchronized org.apache.hadoop.hbase.monitoring.TaskMonitor get();\n  public synchronized org.apache.hadoop.hbase.monitoring.MonitoredTask createStatus(java.lang.String);\n  public synchronized org.apache.hadoop.hbase.monitoring.MonitoredTask createStatus(java.lang.String, boolean);\n  public synchronized org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler createRPCStatus(java.lang.String);\n  public java.util.List<org.apache.hadoop.hbase.monitoring.MonitoredTask> getTasks();\n  public synchronized java.util.List<org.apache.hadoop.hbase.monitoring.MonitoredTask> getTasks(java.lang.String);\n  public void dumpAsText(java.io.PrintWriter);\n  public synchronized void shutdown();\n}\n;;;No. This class provides methods for monitoring and managing tasks, but it is not a message definition. It would not be put on a message queue.;;;N;;;No;;;N
org/apache/hadoop/hbase/monitoring/ThreadMonitoring.class;;;public abstract class org.apache.hadoop.hbase.monitoring.ThreadMonitoring {\n  public org.apache.hadoop.hbase.monitoring.ThreadMonitoring();\n  public static java.lang.management.ThreadInfo getThreadInfo(java.lang.Thread);\n  public static java.lang.String formatThreadInfo(java.lang.management.ThreadInfo, java.lang.String);\n  public static void appendThreadInfo(java.lang.StringBuilder, java.lang.management.ThreadInfo, java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/BalancerDecisionDetails.class;;;public class org.apache.hadoop.hbase.namequeues.BalancerDecisionDetails extends org.apache.hadoop.hbase.namequeues.NamedQueuePayload {\n  public static final int BALANCER_DECISION_EVENT;\n  public org.apache.hadoop.hbase.namequeues.BalancerDecisionDetails(org.apache.hadoop.hbase.client.BalancerDecision);\n  public org.apache.hadoop.hbase.client.BalancerDecision getBalancerDecision();\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/namequeues/BalancerRejectionDetails.class;;;public class org.apache.hadoop.hbase.namequeues.BalancerRejectionDetails extends org.apache.hadoop.hbase.namequeues.NamedQueuePayload {\n  public static final int BALANCER_REJECTION_EVENT;\n  public org.apache.hadoop.hbase.namequeues.BalancerRejectionDetails(org.apache.hadoop.hbase.client.BalancerRejection);\n  public org.apache.hadoop.hbase.client.BalancerRejection getBalancerRejection();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a message definition that might be put on a message queue.;;;N
org/apache/hadoop/hbase/namequeues/DisruptorExceptionHandler.class;;;class org.apache.hadoop.hbase.namequeues.DisruptorExceptionHandler implements com.lmax.disruptor.ExceptionHandler<org.apache.hadoop.hbase.namequeues.RingBufferEnvelope> {\n  public void handleEventException(java.lang.Throwable, long, org.apache.hadoop.hbase.namequeues.RingBufferEnvelope);\n  public void handleOnStartException(java.lang.Throwable);\n  public void handleOnShutdownException(java.lang.Throwable);\n  public void handleEventException(java.lang.Throwable, long, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/LogEventHandler.class;;;class org.apache.hadoop.hbase.namequeues.LogEventHandler implements com.lmax.disruptor.EventHandler<org.apache.hadoop.hbase.namequeues.RingBufferEnvelope> {\n  public void onEvent(org.apache.hadoop.hbase.namequeues.RingBufferEnvelope, long, boolean);\n  public void onEvent(java.lang.Object, long, boolean) throws java.lang.Exception;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/namequeues/LogHandlerUtils.class;;;public class org.apache.hadoop.hbase.namequeues.LogHandlerUtils {\n  public org.apache.hadoop.hbase.namequeues.LogHandlerUtils();\n  public static java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.TooSlowLog$SlowLogPayload> getFilteredLogs(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$SlowLogResponseRequest, java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.TooSlowLog$SlowLogPayload>);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/NamedQueuePayload$NamedQueueEvent.class;;;public final class org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent extends java.lang.Enum<org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent> {\n  public static final org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent SLOW_LOG;\n  public static final org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent BALANCE_DECISION;\n  public static final org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent BALANCE_REJECTION;\n  public static org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent[] values();\n  public static org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent valueOf(java.lang.String);\n  public static org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent getEventByOrdinal(int);\n  public int getValue();\n}\n;;;No, it is an enumeration class defining constants.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/namequeues/NamedQueuePayload.class;;;public class org.apache.hadoop.hbase.namequeues.NamedQueuePayload {\n  public org.apache.hadoop.hbase.namequeues.NamedQueuePayload(int);\n  public org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent getNamedQueueEvent();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/NamedQueueRecorder.class;;;public class org.apache.hadoop.hbase.namequeues.NamedQueueRecorder {\n  public static org.apache.hadoop.hbase.namequeues.NamedQueueRecorder getInstance(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.namequeues.response.NamedQueueGetResponse getNamedQueueRecords(org.apache.hadoop.hbase.namequeues.request.NamedQueueGetRequest);\n  public boolean clearNamedQueue(org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent);\n  public void addRecord(org.apache.hadoop.hbase.namequeues.NamedQueuePayload);\n  public void persistAll(org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class that provides methods for managing named queues in the Hadoop HBase distributed database system.;;;N
org/apache/hadoop/hbase/namequeues/NamedQueueService.class;;;public interface org.apache.hadoop.hbase.namequeues.NamedQueueService {\n  public abstract org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent getEvent();\n  public abstract void consumeEventFromDisruptor(org.apache.hadoop.hbase.namequeues.NamedQueuePayload);\n  public abstract boolean clearNamedQueue();\n  public abstract org.apache.hadoop.hbase.namequeues.response.NamedQueueGetResponse getNamedQueueRecords(org.apache.hadoop.hbase.namequeues.request.NamedQueueGetRequest);\n  public abstract void persistAll();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/RingBufferEnvelope.class;;;final class org.apache.hadoop.hbase.namequeues.RingBufferEnvelope {\n  public void load(org.apache.hadoop.hbase.namequeues.NamedQueuePayload);\n  public org.apache.hadoop.hbase.namequeues.NamedQueuePayload getPayload();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/namequeues/RpcLogDetails.class;;;public class org.apache.hadoop.hbase.namequeues.RpcLogDetails extends org.apache.hadoop.hbase.namequeues.NamedQueuePayload {\n  public static final int SLOW_LOG_EVENT;\n  public org.apache.hadoop.hbase.namequeues.RpcLogDetails(org.apache.hadoop.hbase.ipc.RpcCall, org.apache.hbase.thirdparty.com.google.protobuf.Message, java.lang.String, long, java.lang.String, boolean, boolean);\n  public org.apache.hadoop.hbase.ipc.RpcCall getRpcCall();\n  public java.lang.String getClientAddress();\n  public long getResponseSize();\n  public java.lang.String getClassName();\n  public boolean isSlowLog();\n  public boolean isLargeLog();\n  public org.apache.hbase.thirdparty.com.google.protobuf.Message getParam();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/SlowLogPersistentService.class;;;public class org.apache.hadoop.hbase.namequeues.SlowLogPersistentService {\n  public org.apache.hadoop.hbase.namequeues.SlowLogPersistentService(org.apache.hadoop.conf.Configuration);\n  public void addToQueueForSysTable(org.apache.hadoop.hbase.shaded.protobuf.generated.TooSlowLog$SlowLogPayload);\n  public void addAllLogsToSysTable();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/SlowLogTableOpsChore.class;;;public class org.apache.hadoop.hbase.namequeues.SlowLogTableOpsChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.namequeues.SlowLogTableOpsChore(org.apache.hadoop.hbase.Stoppable, int, org.apache.hadoop.hbase.namequeues.NamedQueueRecorder);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/namequeues/impl/BalancerDecisionQueueService.class;;;public class org.apache.hadoop.hbase.namequeues.impl.BalancerDecisionQueueService implements org.apache.hadoop.hbase.namequeues.NamedQueueService {\n  public org.apache.hadoop.hbase.namequeues.impl.BalancerDecisionQueueService(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent getEvent();\n  public void consumeEventFromDisruptor(org.apache.hadoop.hbase.namequeues.NamedQueuePayload);\n  public boolean clearNamedQueue();\n  public org.apache.hadoop.hbase.namequeues.response.NamedQueueGetResponse getNamedQueueRecords(org.apache.hadoop.hbase.namequeues.request.NamedQueueGetRequest);\n  public void persistAll();\n}\n;;;No, this class is not a message definition. It is a service implementation that provides methods to handle and process messages from a queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/impl/BalancerRejectionQueueService.class;;;public class org.apache.hadoop.hbase.namequeues.impl.BalancerRejectionQueueService implements org.apache.hadoop.hbase.namequeues.NamedQueueService {\n  public org.apache.hadoop.hbase.namequeues.impl.BalancerRejectionQueueService(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent getEvent();\n  public void consumeEventFromDisruptor(org.apache.hadoop.hbase.namequeues.NamedQueuePayload);\n  public boolean clearNamedQueue();\n  public org.apache.hadoop.hbase.namequeues.response.NamedQueueGetResponse getNamedQueueRecords(org.apache.hadoop.hbase.namequeues.request.NamedQueueGetRequest);\n  public void persistAll();\n}\n;;;No, it is not a message definition. It is a class that provides implementation for a named queue service.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/impl/SlowLogQueueService.class;;;public class org.apache.hadoop.hbase.namequeues.impl.SlowLogQueueService implements org.apache.hadoop.hbase.namequeues.NamedQueueService {\n  public org.apache.hadoop.hbase.namequeues.impl.SlowLogQueueService(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent getEvent();\n  public void consumeEventFromDisruptor(org.apache.hadoop.hbase.namequeues.NamedQueuePayload);\n  public boolean clearNamedQueue();\n  public org.apache.hadoop.hbase.namequeues.response.NamedQueueGetResponse getNamedQueueRecords(org.apache.hadoop.hbase.namequeues.request.NamedQueueGetRequest);\n  public void persistAll();\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a service implementation with methods that perform various actions related to a named queue.;;;N
org/apache/hadoop/hbase/namequeues/request/NamedQueueGetRequest.class;;;public class org.apache.hadoop.hbase.namequeues.request.NamedQueueGetRequest {\n  public org.apache.hadoop.hbase.namequeues.request.NamedQueueGetRequest();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$SlowLogResponseRequest getSlowLogResponseRequest();\n  public void setSlowLogResponseRequest(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$SlowLogResponseRequest);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$BalancerDecisionsRequest getBalancerDecisionsRequest();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$BalancerRejectionsRequest getBalancerRejectionsRequest();\n  public void setBalancerDecisionsRequest(org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$BalancerDecisionsRequest);\n  public void setBalancerRejectionsRequest(org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$BalancerRejectionsRequest);\n  public org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent getNamedQueueEvent();\n  public void setNamedQueueEvent(int);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namequeues/response/NamedQueueGetResponse.class;;;public class org.apache.hadoop.hbase.namequeues.response.NamedQueueGetResponse {\n  public org.apache.hadoop.hbase.namequeues.response.NamedQueueGetResponse();\n  public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.TooSlowLog$SlowLogPayload> getSlowLogPayloads();\n  public void setSlowLogPayloads(java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.TooSlowLog$SlowLogPayload>);\n  public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.RecentLogs$BalancerDecision> getBalancerDecisions();\n  public void setBalancerDecisions(java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.RecentLogs$BalancerDecision>);\n  public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.RecentLogs$BalancerRejection> getBalancerRejections();\n  public void setBalancerRejections(java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.RecentLogs$BalancerRejection>);\n  public org.apache.hadoop.hbase.namequeues.NamedQueuePayload$NamedQueueEvent getNamedQueueEvent();\n  public void setNamedQueueEvent(int);\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/namespace/NamespaceAuditor.class;;;public class org.apache.hadoop.hbase.namespace.NamespaceAuditor {\n  public org.apache.hadoop.hbase.namespace.NamespaceAuditor(org.apache.hadoop.hbase.master.MasterServices);\n  public void start() throws java.io.IOException;\n  public void checkQuotaToCreateTable(org.apache.hadoop.hbase.TableName, int) throws java.io.IOException;\n  public void checkQuotaToUpdateRegion(org.apache.hadoop.hbase.TableName, int) throws java.io.IOException;\n  public int getRegionCountOfTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void checkQuotaToSplitRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void updateQuotaForRegionMerge(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void addNamespace(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void deleteNamespace(java.lang.String) throws java.io.IOException;\n  public void removeFromNamespaceUsage(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void removeRegionFromNamespaceUsage(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo getState(java.lang.String);\n  public boolean isInitialized();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/namespace/NamespaceStateManager.class;;;class org.apache.hadoop.hbase.namespace.NamespaceStateManager {\n  public org.apache.hadoop.hbase.namespace.NamespaceStateManager(org.apache.hadoop.hbase.master.MasterServices);\n  public void start() throws java.io.IOException;\n  public org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo getState(java.lang.String);\n  public synchronized void removeRegionFromTable(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition, but rather a class definition for managing namespaces in Hadoop HBase.;;;N
org/apache/hadoop/hbase/namespace/NamespaceTableAndRegionInfo.class;;;class org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo {\n  public org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo(java.lang.String);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/MasterProcedureManager.class;;;public abstract class org.apache.hadoop.hbase.procedure.MasterProcedureManager extends org.apache.hadoop.hbase.procedure.ProcedureManager implements org.apache.hadoop.hbase.Stoppable {\n  public org.apache.hadoop.hbase.procedure.MasterProcedureManager();\n  public abstract void initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster) throws org.apache.zookeeper.KeeperException, java.io.IOException, java.lang.UnsupportedOperationException;\n  public void execProcedure(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException;\n  public byte[] execProcedureWithRet(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException;\n  public abstract void checkPermissions(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription, org.apache.hadoop.hbase.security.access.AccessChecker, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public abstract boolean isProcedureDone(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/MasterProcedureManagerHost.class;;;public class org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost extends org.apache.hadoop.hbase.procedure.ProcedureManagerHost<org.apache.hadoop.hbase.procedure.MasterProcedureManager> {\n  public org.apache.hadoop.hbase.procedure.MasterProcedureManagerHost();\n  public void loadProcedures(org.apache.hadoop.conf.Configuration);\n  public void initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster) throws org.apache.zookeeper.KeeperException, java.io.IOException, java.lang.UnsupportedOperationException;\n  public void stop(java.lang.String);\n  public org.apache.hadoop.hbase.procedure.MasterProcedureManager getProcedureManager(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/Procedure.class;;;public class org.apache.hadoop.hbase.procedure.Procedure implements java.util.concurrent.Callable<java.lang.Void>, org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener {\n  public org.apache.hadoop.hbase.procedure.Procedure(org.apache.hadoop.hbase.procedure.ProcedureCoordinator, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long, java.lang.String, byte[], java.util.List<java.lang.String>);\n  public org.apache.hadoop.hbase.procedure.Procedure(org.apache.hadoop.hbase.procedure.ProcedureCoordinator, long, long, java.lang.String, byte[], java.util.List<java.lang.String>);\n  public java.lang.String getName();\n  public java.lang.String getStatus();\n  public org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher getErrorMonitor();\n  public final java.lang.Void call();\n  public void sendGlobalBarrierStart() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public void sendGlobalBarrierReached() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public void sendGlobalBarrierComplete();\n  public void barrierAcquiredByMember(java.lang.String);\n  public void barrierReleasedByMember(java.lang.String, byte[]);\n  public void waitForCompleted() throws org.apache.hadoop.hbase.errorhandling.ForeignException, java.lang.InterruptedException;\n  public java.util.HashMap<java.lang.String, byte[]> waitForCompletedWithRet() throws org.apache.hadoop.hbase.errorhandling.ForeignException, java.lang.InterruptedException;\n  public boolean isCompleted() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public void receive(org.apache.hadoop.hbase.errorhandling.ForeignException);\n  public static void waitForLatch(java.util.concurrent.CountDownLatch, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare, long, java.lang.String) throws org.apache.hadoop.hbase.errorhandling.ForeignException, java.lang.InterruptedException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/ProcedureCoordinationManager.class;;;public interface org.apache.hadoop.hbase.procedure.ProcedureCoordinationManager {\n  public abstract org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs getProcedureCoordinatorRpcs(java.lang.String, java.lang.String);\n  public abstract org.apache.hadoop.hbase.procedure.ProcedureMemberRpcs getProcedureMemberRpcs(java.lang.String) throws org.apache.zookeeper.KeeperException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/procedure/ProcedureCoordinator.class;;;public class org.apache.hadoop.hbase.procedure.ProcedureCoordinator {\n  public org.apache.hadoop.hbase.procedure.ProcedureCoordinator(org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs, java.util.concurrent.ThreadPoolExecutor);\n  public org.apache.hadoop.hbase.procedure.ProcedureCoordinator(org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs, java.util.concurrent.ThreadPoolExecutor, long, long);\n  public static java.util.concurrent.ThreadPoolExecutor defaultPool(java.lang.String, int);\n  public static java.util.concurrent.ThreadPoolExecutor defaultPool(java.lang.String, int, long);\n  public void close() throws java.io.IOException;\n  public void abortProcedure(java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignException);\n  public org.apache.hadoop.hbase.procedure.Procedure startProcedure(org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, java.lang.String, byte[], java.util.List<java.lang.String>);\n  public org.apache.hadoop.hbase.procedure.Procedure getProcedure(java.lang.String);\n  public java.util.Set<java.lang.String> getProcedureNames();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/ProcedureCoordinatorRpcs.class;;;public interface org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs extends java.io.Closeable {\n  public abstract boolean start(org.apache.hadoop.hbase.procedure.ProcedureCoordinator);\n  public abstract void sendAbortToMembers(org.apache.hadoop.hbase.procedure.Procedure, org.apache.hadoop.hbase.errorhandling.ForeignException) throws java.io.IOException;\n  public abstract void sendGlobalBarrierAcquire(org.apache.hadoop.hbase.procedure.Procedure, byte[], java.util.List<java.lang.String>) throws java.io.IOException, java.lang.IllegalArgumentException;\n  public abstract void sendGlobalBarrierReached(org.apache.hadoop.hbase.procedure.Procedure, java.util.List<java.lang.String>) throws java.io.IOException;\n  public abstract void resetMembers(org.apache.hadoop.hbase.procedure.Procedure) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition, but rather a set of RPC methods defined by an interface.;;;N
org/apache/hadoop/hbase/procedure/ProcedureManager.class;;;public abstract class org.apache.hadoop.hbase.procedure.ProcedureManager {\n  public org.apache.hadoop.hbase.procedure.ProcedureManager();\n  public abstract java.lang.String getProcedureSignature();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;No. This class only contains constructor and methods, but it does not define any message or data to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/ProcedureManagerHost.class;;;public abstract class org.apache.hadoop.hbase.procedure.ProcedureManagerHost<E extends org.apache.hadoop.hbase.procedure.ProcedureManager> {\n  public static final java.lang.String REGIONSERVER_PROCEDURE_CONF_KEY;\n  public static final java.lang.String MASTER_PROCEDURE_CONF_KEY;\n  public org.apache.hadoop.hbase.procedure.ProcedureManagerHost();\n  public E loadInstance(java.lang.Class<?>) throws java.io.IOException;\n  public void register(E);\n  public java.util.Set<E> getProcedureManagers();\n  public abstract void loadProcedures(org.apache.hadoop.conf.Configuration);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class that provides a framework for managing procedures in Hadoop HBase.;;;N;;;No, it is not a task definition, but rather a class defining an object with certain properties and methods.;;;N
org/apache/hadoop/hbase/procedure/ProcedureMember.class;;;public class org.apache.hadoop.hbase.procedure.ProcedureMember implements java.io.Closeable {\n  public org.apache.hadoop.hbase.procedure.ProcedureMember(org.apache.hadoop.hbase.procedure.ProcedureMemberRpcs, java.util.concurrent.ThreadPoolExecutor, org.apache.hadoop.hbase.procedure.SubprocedureFactory);\n  public static java.util.concurrent.ThreadPoolExecutor defaultPool(java.lang.String, int);\n  public static java.util.concurrent.ThreadPoolExecutor defaultPool(java.lang.String, int, long);\n  public org.apache.hadoop.hbase.procedure.Subprocedure createSubprocedure(java.lang.String, byte[]);\n  public boolean submitSubprocedure(org.apache.hadoop.hbase.procedure.Subprocedure);\n  public void receivedReachedGlobalBarrier(java.lang.String);\n  public void close() throws java.io.IOException;\n  public void controllerConnectionFailure(java.lang.String, java.lang.Throwable, java.lang.String);\n  public void receiveAbortProcedure(java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignException);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/ProcedureMemberRpcs.class;;;public interface org.apache.hadoop.hbase.procedure.ProcedureMemberRpcs extends java.io.Closeable {\n  public abstract void start(java.lang.String, org.apache.hadoop.hbase.procedure.ProcedureMember);\n  public abstract java.lang.String getMemberName();\n  public abstract void sendMemberAborted(org.apache.hadoop.hbase.procedure.Subprocedure, org.apache.hadoop.hbase.errorhandling.ForeignException) throws java.io.IOException;\n  public abstract void sendMemberAcquired(org.apache.hadoop.hbase.procedure.Subprocedure) throws java.io.IOException;\n  public abstract void sendMemberCompleted(org.apache.hadoop.hbase.procedure.Subprocedure, byte[]) throws java.io.IOException;\n}\n;;;No, this class is not a message definition. It contains methods that might be used to send procedure-related information between members, but it does not define the format or structure of any messages that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/RegionServerProcedureManager.class;;;public abstract class org.apache.hadoop.hbase.procedure.RegionServerProcedureManager extends org.apache.hadoop.hbase.procedure.ProcedureManager {\n  public org.apache.hadoop.hbase.procedure.RegionServerProcedureManager();\n  public abstract void initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices) throws org.apache.zookeeper.KeeperException;\n  public abstract void start();\n  public abstract void stop(boolean) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/RegionServerProcedureManagerHost.class;;;public class org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost extends org.apache.hadoop.hbase.procedure.ProcedureManagerHost<org.apache.hadoop.hbase.procedure.RegionServerProcedureManager> {\n  public org.apache.hadoop.hbase.procedure.RegionServerProcedureManagerHost();\n  public void initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices) throws org.apache.zookeeper.KeeperException;\n  public void start();\n  public void stop(boolean);\n  public void loadProcedures(org.apache.hadoop.conf.Configuration);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/Subprocedure$1.class;;;class org.apache.hadoop.hbase.procedure.Subprocedure$1 implements org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener {\n  public void receive(org.apache.hadoop.hbase.errorhandling.ForeignException);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/Subprocedure$SubprocedureImpl.class;;;public class org.apache.hadoop.hbase.procedure.Subprocedure$SubprocedureImpl extends org.apache.hadoop.hbase.procedure.Subprocedure {\n  public org.apache.hadoop.hbase.procedure.Subprocedure$SubprocedureImpl(org.apache.hadoop.hbase.procedure.ProcedureMember, java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long);\n  public void acquireBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public byte[] insideBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public void cleanup(java.lang.Exception);\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/Subprocedure.class;;;public abstract class org.apache.hadoop.hbase.procedure.Subprocedure implements java.util.concurrent.Callable<java.lang.Void> {\n  public org.apache.hadoop.hbase.procedure.Subprocedure(org.apache.hadoop.hbase.procedure.ProcedureMember, java.lang.String, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long);\n  public java.lang.String getName();\n  public java.lang.String getMemberName();\n  public final java.lang.Void call();\n  public abstract void acquireBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public abstract byte[] insideBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public abstract void cleanup(java.lang.Exception);\n  public void cancel(java.lang.String, java.lang.Throwable);\n  public void receiveReachedGlobalBarrier();\n  public void waitForLocallyCompleted() throws org.apache.hadoop.hbase.errorhandling.ForeignException, java.lang.InterruptedException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No. This is a class definition for a subprocedure in Hadoop HBase, but it is not a complete message definition that could be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/procedure/SubprocedureFactory.class;;;public interface org.apache.hadoop.hbase.procedure.SubprocedureFactory {\n  public abstract org.apache.hadoop.hbase.procedure.Subprocedure buildSubprocedure(java.lang.String, byte[]);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/ZKProcedureCoordinationManager.class;;;public class org.apache.hadoop.hbase.procedure.ZKProcedureCoordinationManager implements org.apache.hadoop.hbase.procedure.ProcedureCoordinationManager {\n  public org.apache.hadoop.hbase.procedure.ZKProcedureCoordinationManager(org.apache.hadoop.hbase.Server);\n  public org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs getProcedureCoordinatorRpcs(java.lang.String, java.lang.String);\n  public org.apache.hadoop.hbase.procedure.ProcedureMemberRpcs getProcedureMemberRpcs(java.lang.String) throws org.apache.zookeeper.KeeperException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator$1.class;;;class org.apache.hadoop.hbase.procedure.ZKProcedureCoordinator$1 extends org.apache.hadoop.hbase.procedure.ZKProcedureUtil {\n  public void nodeCreated(java.lang.String);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/procedure/ZKProcedureCoordinator.class;;;public class org.apache.hadoop.hbase.procedure.ZKProcedureCoordinator implements org.apache.hadoop.hbase.procedure.ProcedureCoordinatorRpcs {\n  public org.apache.hadoop.hbase.procedure.ZKProcedureCoordinator(org.apache.hadoop.hbase.zookeeper.ZKWatcher, java.lang.String, java.lang.String);\n  public final void sendGlobalBarrierAcquire(org.apache.hadoop.hbase.procedure.Procedure, byte[], java.util.List<java.lang.String>) throws java.io.IOException, java.lang.IllegalArgumentException;\n  public void sendGlobalBarrierReached(org.apache.hadoop.hbase.procedure.Procedure, java.util.List<java.lang.String>) throws java.io.IOException;\n  public final void resetMembers(org.apache.hadoop.hbase.procedure.Procedure) throws java.io.IOException;\n  public final boolean start(org.apache.hadoop.hbase.procedure.ProcedureCoordinator);\n  public final void sendAbortToMembers(org.apache.hadoop.hbase.procedure.Procedure, org.apache.hadoop.hbase.errorhandling.ForeignException);\n  public final void close() throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class that implements an interface and contains methods that can be called by other parts of the code.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs$1.class;;;class org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs$1 extends org.apache.hadoop.hbase.procedure.ZKProcedureUtil {\n  public void nodeCreated(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/ZKProcedureMemberRpcs.class;;;public class org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs implements org.apache.hadoop.hbase.procedure.ProcedureMemberRpcs {\n  public org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs(org.apache.hadoop.hbase.zookeeper.ZKWatcher, java.lang.String) throws org.apache.zookeeper.KeeperException;\n  public org.apache.hadoop.hbase.procedure.ZKProcedureUtil getZkController();\n  public java.lang.String getMemberName();\n  public void sendMemberAcquired(org.apache.hadoop.hbase.procedure.Subprocedure) throws java.io.IOException;\n  public void sendMemberCompleted(org.apache.hadoop.hbase.procedure.Subprocedure, byte[]) throws java.io.IOException;\n  public void sendMemberAborted(org.apache.hadoop.hbase.procedure.Subprocedure, org.apache.hadoop.hbase.errorhandling.ForeignException);\n  public void start(java.lang.String, org.apache.hadoop.hbase.procedure.ProcedureMember);\n  public void close() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/ZKProcedureUtil.class;;;public abstract class org.apache.hadoop.hbase.procedure.ZKProcedureUtil extends org.apache.hadoop.hbase.zookeeper.ZKListener implements java.io.Closeable {\n  public static final java.lang.String ACQUIRED_BARRIER_ZNODE_DEFAULT;\n  public static final java.lang.String REACHED_BARRIER_ZNODE_DEFAULT;\n  public static final java.lang.String ABORT_ZNODE_DEFAULT;\n  public final java.lang.String baseZNode;\n  public org.apache.hadoop.hbase.procedure.ZKProcedureUtil(org.apache.hadoop.hbase.zookeeper.ZKWatcher, java.lang.String) throws org.apache.zookeeper.KeeperException;\n  public void close() throws java.io.IOException;\n  public java.lang.String getAcquiredBarrierNode(java.lang.String);\n  public java.lang.String getReachedBarrierNode(java.lang.String);\n  public java.lang.String getAbortZNode(java.lang.String);\n  public java.lang.String getAbortZnode();\n  public java.lang.String getBaseZnode();\n  public java.lang.String getAcquiredBarrier();\n  public static java.lang.String getAcquireBarrierNode(org.apache.hadoop.hbase.procedure.ZKProcedureUtil, java.lang.String);\n  public static java.lang.String getReachedBarrierNode(org.apache.hadoop.hbase.procedure.ZKProcedureUtil, java.lang.String);\n  public static java.lang.String getAbortNode(org.apache.hadoop.hbase.procedure.ZKProcedureUtil, java.lang.String);\n  public org.apache.hadoop.hbase.zookeeper.ZKWatcher getWatcher();\n  public boolean isAbortPathNode(java.lang.String);\n  public void clearChildZNodes() throws org.apache.zookeeper.KeeperException;\n  public void clearZNodes(java.lang.String) throws org.apache.zookeeper.KeeperException;\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a utility class for working with ZooKeeper in the context of HBase procedures.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure$RegionFlushTask.class;;;class org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure$RegionFlushTask implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call() throws java.lang.Exception;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No. This class is a callable task that can be executed by a thread or a thread pool, but it is not a message definition that can be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/procedure/flush/FlushTableSubprocedure.class;;;public class org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure extends org.apache.hadoop.hbase.procedure.Subprocedure {\n  public org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure(org.apache.hadoop.hbase.procedure.ProcedureMember, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long, java.util.List<org.apache.hadoop.hbase.regionserver.HRegion>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedurePool);\n  public void acquireBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public byte[] insideBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public void cleanup(java.lang.Exception);\n  public void releaseBarrier();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/flush/MasterFlushTableProcedureManager.class;;;public class org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager extends org.apache.hadoop.hbase.procedure.MasterProcedureManager {\n  public static final java.lang.String FLUSH_TABLE_PROCEDURE_SIGNATURE;\n  public org.apache.hadoop.hbase.procedure.flush.MasterFlushTableProcedureManager();\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public void initialize(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.master.MetricsMaster) throws org.apache.zookeeper.KeeperException, java.io.IOException, java.lang.UnsupportedOperationException;\n  public java.lang.String getProcedureSignature();\n  public void execProcedure(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException;\n  public void checkPermissions(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription, org.apache.hadoop.hbase.security.access.AccessChecker, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public synchronized boolean isProcedureDone(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ProcedureDescription) throws java.io.IOException;\n}\n;;;No. This is not a message definition, but rather a class definition for a procedure manager in Hbase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager$FlushTableSubprocedureBuilder.class;;;public class org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedureBuilder implements org.apache.hadoop.hbase.procedure.SubprocedureFactory {\n  public org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedureBuilder(org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager);\n  public org.apache.hadoop.hbase.procedure.Subprocedure buildSubprocedure(java.lang.String, byte[]);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager$FlushTableSubprocedurePool.class;;;class org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager$FlushTableSubprocedurePool {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure/flush/RegionServerFlushTableProcedureManager.class;;;public class org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager extends org.apache.hadoop.hbase.procedure.RegionServerProcedureManager {\n  public static final java.lang.String FLUSH_REQUEST_THREADS_KEY;\n  public static final int FLUSH_REQUEST_THREADS_DEFAULT;\n  public static final java.lang.String FLUSH_TIMEOUT_MILLIS_KEY;\n  public static final long FLUSH_TIMEOUT_MILLIS_DEFAULT;\n  public static final java.lang.String FLUSH_REQUEST_WAKE_MILLIS_KEY;\n  public org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager();\n  public void start();\n  public void stop(boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.procedure.Subprocedure buildSubprocedure(java.lang.String, java.lang.String);\n  public void initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices) throws org.apache.zookeeper.KeeperException;\n  public java.lang.String getProcedureSignature();\n}\n;;;No, this class is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure2/BaseRSProcedureCallable.class;;;public abstract class org.apache.hadoop.hbase.procedure2.BaseRSProcedureCallable implements org.apache.hadoop.hbase.procedure2.RSProcedureCallable {\n  public org.apache.hadoop.hbase.procedure2.BaseRSProcedureCallable();\n  public final java.lang.Void call() throws java.lang.Exception;\n  public final void init(byte[], org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/procedure2/RSProcedureCallable.class;;;public interface org.apache.hadoop.hbase.procedure2.RSProcedureCallable extends java.util.concurrent.Callable<java.lang.Void> {\n  public abstract void init(byte[], org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public abstract org.apache.hadoop.hbase.executor.EventType getEventType();\n}\n;;;No. It is an interface definition for a Callable class, but it is not a message definition that might be put on a message queue.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/procedure2/store/region/HFileProcedurePrettyPrinter.class;;;public class org.apache.hadoop.hbase.procedure2.store.region.HFileProcedurePrettyPrinter extends org.apache.hadoop.hbase.util.AbstractHBaseTool {\n  public org.apache.hadoop.hbase.procedure2.store.region.HFileProcedurePrettyPrinter();\n  public org.apache.hadoop.hbase.procedure2.store.region.HFileProcedurePrettyPrinter(java.io.PrintStream);\n  public static void main(java.lang.String[]);\n}\n;;;No. It is a tool class for printing information about HBase procedures stored in HFiles, but it is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure2/store/region/RegionProcedureStore$1.class;;;class org.apache.hadoop.hbase.procedure2.store.region.RegionProcedureStore$1 implements org.apache.hadoop.hbase.procedure2.store.ProcedureStore$ProcedureLoader {\n  public void setMaxProcId(long);\n  public void load(org.apache.hadoop.hbase.procedure2.store.ProcedureStore$ProcedureIterator) throws java.io.IOException;\n  public void handleCorrupted(org.apache.hadoop.hbase.procedure2.store.ProcedureStore$ProcedureIterator) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/procedure2/store/region/RegionProcedureStore.class;;;public class org.apache.hadoop.hbase.procedure2.store.region.RegionProcedureStore extends org.apache.hadoop.hbase.procedure2.store.ProcedureStoreBase {\n  public org.apache.hadoop.hbase.procedure2.store.region.RegionProcedureStore(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.master.region.MasterRegion, org.apache.hadoop.hbase.procedure2.store.LeaseRecovery);\n  public void start(int) throws java.io.IOException;\n  public void stop(boolean);\n  public int getNumThreads();\n  public int setRunningProcedureCount(int);\n  public void recoverLease() throws java.io.IOException;\n  public void load(org.apache.hadoop.hbase.procedure2.store.ProcedureStore$ProcedureLoader) throws java.io.IOException;\n  public void insert(org.apache.hadoop.hbase.procedure2.Procedure<?>, org.apache.hadoop.hbase.procedure2.Procedure<?>[]);\n  public void insert(org.apache.hadoop.hbase.procedure2.Procedure<?>[]);\n  public void update(org.apache.hadoop.hbase.procedure2.Procedure<?>);\n  public void delete(long);\n  public void delete(org.apache.hadoop.hbase.procedure2.Procedure<?>, long[]);\n  public void delete(long[], int, int);\n  public void cleanup();\n}\n;;;No, it is not a message definition. It is a class definition for a procedure store in HBase.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/procedure2/store/region/WALProcedurePrettyPrinter.class;;;public class org.apache.hadoop.hbase.procedure2.store.region.WALProcedurePrettyPrinter extends org.apache.hadoop.hbase.util.AbstractHBaseTool {\n  public org.apache.hadoop.hbase.procedure2.store.region.WALProcedurePrettyPrinter();\n  public org.apache.hadoop.hbase.procedure2.store.region.WALProcedurePrettyPrinter(java.io.PrintStream);\n  public static void main(java.lang.String[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/protobuf/ReplicationProtobufUtil$1.class;;;final class org.apache.hadoop.hbase.protobuf.ReplicationProtobufUtil$1 implements org.apache.hadoop.hbase.io.SizedCellScanner {\n  public org.apache.hadoop.hbase.Cell current();\n  public boolean advance();\n  public long heapSize();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/protobuf/ReplicationProtobufUtil.class;;;public class org.apache.hadoop.hbase.protobuf.ReplicationProtobufUtil {\n  public org.apache.hadoop.hbase.protobuf.ReplicationProtobufUtil();\n  public static void replicateWALEntry(org.apache.hadoop.hbase.client.AsyncRegionServerAdmin, org.apache.hadoop.hbase.wal.WAL$Entry[], java.lang.String, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, int) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner> buildReplicateWALEntryRequest(org.apache.hadoop.hbase.wal.WAL$Entry[]);\n  public static org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner> buildReplicateWALEntryRequest(org.apache.hadoop.hbase.wal.WAL$Entry[], byte[], java.lang.String, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/ActivePolicyEnforcement.class;;;public class org.apache.hadoop.hbase.quotas.ActivePolicyEnforcement {\n  public org.apache.hadoop.hbase.quotas.ActivePolicyEnforcement(java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement>, java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot>, org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public org.apache.hadoop.hbase.quotas.ActivePolicyEnforcement(java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement>, java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot>, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcementFactory);\n  public org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement getPolicyEnforcement(org.apache.hadoop.hbase.regionserver.Region);\n  public org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement getPolicyEnforcement(org.apache.hadoop.hbase.TableName);\n  public java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement> getPolicies();\n  public java.lang.String toString();\n}\n;;;No, it is a class definition that is used in HBase for enforcing space quotas. It is not a message definition to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/AverageIntervalRateLimiter.class;;;public class org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter extends org.apache.hadoop.hbase.quotas.RateLimiter {\n  public org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter();\n  public long refill(long);\n  public long getWaitInterval(long, long, long);\n  public void setNextRefillTime(long);\n  public long getNextRefillTime();\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class definition for a rate limiter in Hadoop/HBase.;;;N
org/apache/hadoop/hbase/quotas/DefaultOperationQuota.class;;;public class org.apache.hadoop.hbase.quotas.DefaultOperationQuota implements org.apache.hadoop.hbase.quotas.OperationQuota {\n  public org.apache.hadoop.hbase.quotas.DefaultOperationQuota(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.quotas.QuotaLimiter...);\n  public org.apache.hadoop.hbase.quotas.DefaultOperationQuota(org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.quotas.QuotaLimiter>);\n  public void checkQuota(int, int, int) throws org.apache.hadoop.hbase.quotas.RpcThrottlingException;\n  public void close();\n  public long getReadAvailable();\n  public void addGetResult(org.apache.hadoop.hbase.client.Result);\n  public void addScanResult(java.util.List<org.apache.hadoop.hbase.client.Result>);\n  public void addMutation(org.apache.hadoop.hbase.client.Mutation);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/ExceedOperationQuota.class;;;public class org.apache.hadoop.hbase.quotas.ExceedOperationQuota extends org.apache.hadoop.hbase.quotas.DefaultOperationQuota {\n  public org.apache.hadoop.hbase.quotas.ExceedOperationQuota(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.quotas.QuotaLimiter, org.apache.hadoop.hbase.quotas.QuotaLimiter...);\n  public void checkQuota(int, int, int) throws org.apache.hadoop.hbase.quotas.RpcThrottlingException;\n  public void close();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/FileArchiverNotifier.class;;;public interface org.apache.hadoop.hbase.quotas.FileArchiverNotifier {\n  public abstract void addArchivedFiles(java.util.Set<java.util.Map$Entry<java.lang.String, java.lang.Long>>) throws java.io.IOException;\n  public abstract long computeAndStoreSnapshotSizes(java.util.Collection<java.lang.String>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/FileArchiverNotifierFactory.class;;;public interface org.apache.hadoop.hbase.quotas.FileArchiverNotifierFactory {\n  public abstract org.apache.hadoop.hbase.quotas.FileArchiverNotifier get(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.TableName);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/FileArchiverNotifierFactoryImpl$CacheKey.class;;;class org.apache.hadoop.hbase.quotas.FileArchiverNotifierFactoryImpl$CacheKey {\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/FileArchiverNotifierFactoryImpl.class;;;public final class org.apache.hadoop.hbase.quotas.FileArchiverNotifierFactoryImpl implements org.apache.hadoop.hbase.quotas.FileArchiverNotifierFactory {\n  public static org.apache.hadoop.hbase.quotas.FileArchiverNotifierFactory getInstance();\n  public org.apache.hadoop.hbase.quotas.FileArchiverNotifier get(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.TableName);\n  public int getCacheSize();\n}\n;;;No. This class is not a message definition that might be put on a message queue. It provides implementation details for a file archiver notifier factory in Hadoop HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl$QuotaSnapshotSizeSerializationException.class;;;public class org.apache.hadoop.hbase.quotas.FileArchiverNotifierImpl$QuotaSnapshotSizeSerializationException extends java.io.IOException {\n  public org.apache.hadoop.hbase.quotas.FileArchiverNotifierImpl$QuotaSnapshotSizeSerializationException(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl$SnapshotWithSize.class;;;class org.apache.hadoop.hbase.quotas.FileArchiverNotifierImpl$SnapshotWithSize {\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl$StoreFileReference.class;;;class org.apache.hadoop.hbase.quotas.FileArchiverNotifierImpl$StoreFileReference {\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.class;;;public class org.apache.hadoop.hbase.quotas.FileArchiverNotifierImpl implements org.apache.hadoop.hbase.quotas.FileArchiverNotifier {\n  public org.apache.hadoop.hbase.quotas.FileArchiverNotifierImpl(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.TableName);\n  public void addArchivedFiles(java.util.Set<java.util.Map$Entry<java.lang.String, java.lang.Long>>) throws java.io.IOException;\n  public long computeAndStoreSnapshotSizes(java.util.Collection<java.lang.String>) throws java.io.IOException;\n  public java.lang.String toString();\n}\n;;;No. The class has methods and constructors, but it does not define any message format or message fields.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/FileSystemUtilizationChore.class;;;public class org.apache.hadoop.hbase.quotas.FileSystemUtilizationChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.quotas.FileSystemUtilizationChore(org.apache.hadoop.hbase.regionserver.HRegionServer);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/quotas/FixedIntervalRateLimiter.class;;;public class org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter extends org.apache.hadoop.hbase.quotas.RateLimiter {\n  public org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter();\n  public long refill(long);\n  public long getWaitInterval(long, long, long);\n  public void setNextRefillTime(long);\n  public long getNextRefillTime();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/GlobalQuotaSettings.class;;;public abstract class org.apache.hadoop.hbase.quotas.GlobalQuotaSettings extends org.apache.hadoop.hbase.quotas.QuotaSettings {\n  public abstract java.util.List<org.apache.hadoop.hbase.quotas.QuotaSettings> getQuotaSettings();\n  public org.apache.hadoop.hbase.quotas.QuotaType getQuotaType();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/GlobalQuotaSettingsImpl$1.class;;;class org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/GlobalQuotaSettingsImpl.class;;;public class org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl extends org.apache.hadoop.hbase.quotas.GlobalQuotaSettings {\n  public java.util.List<org.apache.hadoop.hbase.quotas.QuotaSettings> getQuotaSettings();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager$1.class;;;class org.apache.hadoop.hbase.quotas.MasterQuotaManager$1 implements org.apache.hadoop.hbase.quotas.MasterQuotaManager$SetQuotaOperations {\n  public org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl fetch() throws java.io.IOException;\n  public void update(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void delete() throws java.io.IOException;\n  public void preApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void postApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager$2.class;;;class org.apache.hadoop.hbase.quotas.MasterQuotaManager$2 implements org.apache.hadoop.hbase.quotas.MasterQuotaManager$SetQuotaOperations {\n  public org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl fetch() throws java.io.IOException;\n  public void update(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void delete() throws java.io.IOException;\n  public void preApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void postApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n}\n;;;No. This is a class definition for a specific implementation of the MasterQuotaManager interface in the Hadoop HBase library. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager$3.class;;;class org.apache.hadoop.hbase.quotas.MasterQuotaManager$3 implements org.apache.hadoop.hbase.quotas.MasterQuotaManager$SetQuotaOperations {\n  public org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl fetch() throws java.io.IOException;\n  public void update(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void delete() throws java.io.IOException;\n  public void preApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void postApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n}\n;;;No. This is a Java class implementation that may be used by a message definition, but it is not a message definition itself.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager$4.class;;;class org.apache.hadoop.hbase.quotas.MasterQuotaManager$4 implements org.apache.hadoop.hbase.quotas.MasterQuotaManager$SetQuotaOperations {\n  public org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl fetch() throws java.io.IOException;\n  public void update(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void delete() throws java.io.IOException;\n  public void preApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void postApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n}\n;;;No, this class is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager$5.class;;;class org.apache.hadoop.hbase.quotas.MasterQuotaManager$5 implements org.apache.hadoop.hbase.quotas.MasterQuotaManager$SetQuotaOperations {\n  public org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl fetch() throws java.io.IOException;\n  public void update(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void delete() throws java.io.IOException;\n  public void preApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void postApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager$6.class;;;class org.apache.hadoop.hbase.quotas.MasterQuotaManager$6 implements org.apache.hadoop.hbase.quotas.MasterQuotaManager$SetQuotaOperations {\n  public org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl fetch() throws java.io.IOException;\n  public void update(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void delete() throws java.io.IOException;\n  public void preApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public void postApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n}\n;;;No, this is not a message definition. It is a class definition that implements a set of methods for managing quota settings. It could potentially be used in a message passing system, but it is not a message definition itself.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager$NamedLock.class;;;class org.apache.hadoop.hbase.quotas.MasterQuotaManager$NamedLock<T> {\n  public void lock(T) throws java.lang.InterruptedException;\n  public void unlock(T);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager$SetQuotaOperations.class;;;interface org.apache.hadoop.hbase.quotas.MasterQuotaManager$SetQuotaOperations {\n  public abstract org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl fetch() throws java.io.IOException;\n  public abstract void delete() throws java.io.IOException;\n  public abstract void update(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public abstract void preApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n  public abstract void postApply(org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl) throws java.io.IOException;\n}\n;;;No, the class is not a message definition. It is an interface for a set of operations that can be performed on a MasterQuotaManager's set of quota operations.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager$SizeSnapshotWithTimestamp.class;;;class org.apache.hadoop.hbase.quotas.MasterQuotaManager$SizeSnapshotWithTimestamp {\n  public org.apache.hadoop.hbase.quotas.MasterQuotaManager$SizeSnapshotWithTimestamp(long, long);\n  public long getSize();\n  public long getTime();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/MasterQuotaManager.class;;;public class org.apache.hadoop.hbase.quotas.MasterQuotaManager implements org.apache.hadoop.hbase.RegionStateListener {\n  public org.apache.hadoop.hbase.quotas.MasterQuotaManager(org.apache.hadoop.hbase.master.MasterServices);\n  public void start() throws java.io.IOException;\n  public void stop();\n  public boolean isQuotaInitialized();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaResponse setQuota(org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException;\n  public void setUserQuota(java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException;\n  public void setUserQuota(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException;\n  public void setUserQuota(java.lang.String, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException;\n  public void setTableQuota(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException;\n  public void setNamespaceQuota(java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException;\n  public void setRegionServerQuota(java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SetQuotaRequest) throws java.io.IOException, java.lang.InterruptedException;\n  public void setNamespaceQuota(org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void removeNamespaceQuota(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SwitchRpcThrottleResponse switchRpcThrottle(org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SwitchRpcThrottleRequest) throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsRpcThrottleEnabledResponse isRpcThrottleEnabled(org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$IsRpcThrottleEnabledRequest) throws java.io.IOException;\n  public boolean isRpcThrottleEnabled() throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SwitchExceedThrottleQuotaResponse switchExceedThrottleQuota(org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos$SwitchExceedThrottleQuotaRequest) throws java.io.IOException;\n  public boolean isExceedThrottleQuotaEnabled() throws java.io.IOException;\n  public void checkNamespaceTableAndRegionQuota(org.apache.hadoop.hbase.TableName, int) throws java.io.IOException;\n  public void checkAndUpdateNamespaceRegionQuota(org.apache.hadoop.hbase.TableName, int) throws java.io.IOException;\n  public int getRegionCountOfTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void onRegionMerged(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void onRegionSplit(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void removeTableFromNamespaceQuota(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.namespace.NamespaceAuditor getNamespaceQuotaManager();\n  public void onRegionSplitReverted(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void addRegionSize(org.apache.hadoop.hbase.client.RegionInfo, long, long);\n  public java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, java.lang.Long> snapshotRegionSizes();\n  public void processFileArchivals(org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$FileArchiveNotificationRequest, org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public void removeRegionSizesForTable(org.apache.hadoop.hbase.TableName);\n}\n;;;No. This is a class definition for the MasterQuotaManager in the Apache Hadoop HBase library, but it does not define a message format that can be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/quotas/MasterQuotasObserver.class;;;public class org.apache.hadoop.hbase.quotas.MasterQuotasObserver implements org.apache.hadoop.hbase.coprocessor.MasterCoprocessor,org.apache.hadoop.hbase.coprocessor.MasterObserver {\n  public static final java.lang.String REMOVE_QUOTA_ON_TABLE_DELETE;\n  public static final boolean REMOVE_QUOTA_ON_TABLE_DELETE_DEFAULT;\n  public org.apache.hadoop.hbase.quotas.MasterQuotasObserver();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.MasterObserver> getMasterObserver();\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public void postDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/NamespaceQuotaSnapshotStore.class;;;public class org.apache.hadoop.hbase.quotas.NamespaceQuotaSnapshotStore implements org.apache.hadoop.hbase.quotas.QuotaSnapshotStore<java.lang.String> {\n  public org.apache.hadoop.hbase.quotas.NamespaceQuotaSnapshotStore(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.quotas.QuotaObserverChore, java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, java.lang.Long>);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota getSpaceQuota(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getCurrentState(java.lang.String);\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getTargetState(java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota) throws java.io.IOException;\n  public java.lang.Iterable<java.util.Map$Entry<org.apache.hadoop.hbase.client.RegionInfo, java.lang.Long>> filterBySubject(java.lang.String);\n  public void setCurrentState(java.lang.String, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public void setRegionUsage(java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, java.lang.Long>);\n  public void setCurrentState(java.lang.Object, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public java.lang.Iterable filterBySubject(java.lang.Object);\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getTargetState(java.lang.Object, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota) throws java.io.IOException;\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getCurrentState(java.lang.Object);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota getSpaceQuota(java.lang.Object) throws java.io.IOException;\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/NoOpRegionSizeStore.class;;;public final class org.apache.hadoop.hbase.quotas.NoOpRegionSizeStore implements org.apache.hadoop.hbase.quotas.RegionSizeStore {\n  public static org.apache.hadoop.hbase.quotas.NoOpRegionSizeStore getInstance();\n  public java.util.Iterator<java.util.Map$Entry<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.quotas.RegionSize>> iterator();\n  public long heapSize();\n  public org.apache.hadoop.hbase.quotas.RegionSize getRegionSize(org.apache.hadoop.hbase.client.RegionInfo);\n  public void put(org.apache.hadoop.hbase.client.RegionInfo, long);\n  public void incrementRegionSize(org.apache.hadoop.hbase.client.RegionInfo, long);\n  public org.apache.hadoop.hbase.quotas.RegionSize remove(org.apache.hadoop.hbase.client.RegionInfo);\n  public int size();\n  public boolean isEmpty();\n  public void clear();\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/NoopOperationQuota.class;;;class org.apache.hadoop.hbase.quotas.NoopOperationQuota implements org.apache.hadoop.hbase.quotas.OperationQuota {\n  public static org.apache.hadoop.hbase.quotas.OperationQuota get();\n  public void checkQuota(int, int, int) throws org.apache.hadoop.hbase.quotas.RpcThrottlingException;\n  public void close();\n  public void addGetResult(org.apache.hadoop.hbase.client.Result);\n  public void addScanResult(java.util.List<org.apache.hadoop.hbase.client.Result>);\n  public void addMutation(org.apache.hadoop.hbase.client.Mutation);\n  public long getReadAvailable();\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class definition for a Java class.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/NoopQuotaLimiter.class;;;class org.apache.hadoop.hbase.quotas.NoopQuotaLimiter implements org.apache.hadoop.hbase.quotas.QuotaLimiter {\n  public void checkQuota(long, long, long, long, long, long) throws org.apache.hadoop.hbase.quotas.RpcThrottlingException;\n  public void grabQuota(long, long, long, long, long, long);\n  public void consumeWrite(long, long);\n  public void consumeRead(long, long);\n  public boolean isBypass();\n  public long getWriteAvailable();\n  public long getReadAvailable();\n  public java.lang.String toString();\n  public static org.apache.hadoop.hbase.quotas.QuotaLimiter get();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/OperationQuota$OperationType.class;;;public final class org.apache.hadoop.hbase.quotas.OperationQuota$OperationType extends java.lang.Enum<org.apache.hadoop.hbase.quotas.OperationQuota$OperationType> {\n  public static final org.apache.hadoop.hbase.quotas.OperationQuota$OperationType MUTATE;\n  public static final org.apache.hadoop.hbase.quotas.OperationQuota$OperationType GET;\n  public static final org.apache.hadoop.hbase.quotas.OperationQuota$OperationType SCAN;\n  public static org.apache.hadoop.hbase.quotas.OperationQuota$OperationType[] values();\n  public static org.apache.hadoop.hbase.quotas.OperationQuota$OperationType valueOf(java.lang.String);\n}\n;;;No. This is just a Java class definition and does not include any message or queue implementation details.;;;N;;;No, it is not a task definition. It defines an enumeration for operation types in HBase quotas.;;;N
org/apache/hadoop/hbase/quotas/OperationQuota.class;;;public interface org.apache.hadoop.hbase.quotas.OperationQuota {\n  public abstract void checkQuota(int, int, int) throws org.apache.hadoop.hbase.quotas.RpcThrottlingException;\n  public abstract void close();\n  public abstract void addGetResult(org.apache.hadoop.hbase.client.Result);\n  public abstract void addScanResult(java.util.List<org.apache.hadoop.hbase.client.Result>);\n  public abstract void addMutation(org.apache.hadoop.hbase.client.Mutation);\n  public abstract long getReadAvailable();\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an interface for implementing quotas for HBase operations.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/QuotaCache$Fetcher.class;;;interface org.apache.hadoop.hbase.quotas.QuotaCache$Fetcher<Key, Value> {\n  public abstract org.apache.hadoop.hbase.client.Get makeGet(java.util.Map$Entry<Key, Value>);\n  public abstract java.util.Map<Key, Value> fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException;\n}\n;;;Yes, it is a message definition.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/QuotaCache$QuotaRefresherChore$1.class;;;class org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$1 implements org.apache.hadoop.hbase.quotas.QuotaCache$Fetcher<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState> {\n  public org.apache.hadoop.hbase.client.Get makeGet(java.util.Map$Entry<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState>);\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState> fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/QuotaCache$QuotaRefresherChore$2.class;;;class org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$2 implements org.apache.hadoop.hbase.quotas.QuotaCache$Fetcher<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState> {\n  public org.apache.hadoop.hbase.client.Get makeGet(java.util.Map$Entry<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState>);\n  public java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState> fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/QuotaCache$QuotaRefresherChore$3.class;;;class org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$3 implements org.apache.hadoop.hbase.quotas.QuotaCache$Fetcher<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState> {\n  public org.apache.hadoop.hbase.client.Get makeGet(java.util.Map$Entry<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState>);\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState> fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException;\n}\n;;;Yes, it appears to be a message definition as it implements a message interface (org.apache.hadoop.hbase.quotas.QuotaCache$Fetcher) and has methods for constructing a message (makeGet) and processing a message (fetchEntries).;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/QuotaCache$QuotaRefresherChore$4.class;;;class org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore$4 implements org.apache.hadoop.hbase.quotas.QuotaCache$Fetcher<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState> {\n  public org.apache.hadoop.hbase.client.Get makeGet(java.util.Map$Entry<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState>);\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState> fetchEntries(java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/QuotaCache$QuotaRefresherChore.class;;;class org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.quotas.QuotaCache$QuotaRefresherChore(org.apache.hadoop.hbase.quotas.QuotaCache, int, org.apache.hadoop.hbase.Stoppable);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/quotas/QuotaCache.class;;;public class org.apache.hadoop.hbase.quotas.QuotaCache implements org.apache.hadoop.hbase.Stoppable {\n  public static final java.lang.String REFRESH_CONF_KEY;\n  public org.apache.hadoop.hbase.quotas.QuotaCache(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public void start() throws java.io.IOException;\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public org.apache.hadoop.hbase.quotas.QuotaLimiter getUserLimiter(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.quotas.UserQuotaState getUserQuotaState(org.apache.hadoop.security.UserGroupInformation);\n  public org.apache.hadoop.hbase.quotas.QuotaLimiter getTableLimiter(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.quotas.QuotaLimiter getNamespaceLimiter(java.lang.String);\n  public org.apache.hadoop.hbase.quotas.QuotaLimiter getRegionServerQuotaLimiter(java.lang.String);\n}\n;;;No.;;;N;;;No, it is not a task definition, but a class definition.;;;N
org/apache/hadoop/hbase/quotas/QuotaLimiter.class;;;public interface org.apache.hadoop.hbase.quotas.QuotaLimiter {\n  public abstract void checkQuota(long, long, long, long, long, long) throws org.apache.hadoop.hbase.quotas.RpcThrottlingException;\n  public abstract void grabQuota(long, long, long, long, long, long);\n  public abstract void consumeWrite(long, long);\n  public abstract void consumeRead(long, long);\n  public abstract boolean isBypass();\n  public abstract long getReadAvailable();\n  public abstract long getWriteAvailable();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/QuotaLimiterFactory.class;;;public class org.apache.hadoop.hbase.quotas.QuotaLimiterFactory {\n  public org.apache.hadoop.hbase.quotas.QuotaLimiterFactory();\n  public static org.apache.hadoop.hbase.quotas.QuotaLimiter fromThrottle(org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Throttle);\n  public static org.apache.hadoop.hbase.quotas.QuotaLimiter update(org.apache.hadoop.hbase.quotas.QuotaLimiter, org.apache.hadoop.hbase.quotas.QuotaLimiter);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/QuotaObserverChore$TablesWithQuotas.class;;;class org.apache.hadoop.hbase.quotas.QuotaObserverChore$TablesWithQuotas {\n  public org.apache.hadoop.hbase.quotas.QuotaObserverChore$TablesWithQuotas(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.conf.Configuration);\n  public void addTableQuotaTable(org.apache.hadoop.hbase.TableName);\n  public void addNamespaceQuotaTable(org.apache.hadoop.hbase.TableName);\n  public boolean hasTableQuota(org.apache.hadoop.hbase.TableName);\n  public boolean hasNamespaceQuota(org.apache.hadoop.hbase.TableName);\n  public java.util.Set<org.apache.hadoop.hbase.TableName> getTableQuotaTables();\n  public java.util.Set<org.apache.hadoop.hbase.TableName> getNamespaceQuotaTables();\n  public java.util.Set<java.lang.String> getNamespacesWithQuotas();\n  public org.apache.hbase.thirdparty.com.google.common.collect.Multimap<java.lang.String, org.apache.hadoop.hbase.TableName> getTablesByNamespace();\n  public java.util.Set<org.apache.hadoop.hbase.TableName> filterInsufficientlyReportedTables(org.apache.hadoop.hbase.quotas.QuotaSnapshotStore<org.apache.hadoop.hbase.TableName>) throws java.io.IOException;\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/QuotaObserverChore.class;;;public class org.apache.hadoop.hbase.quotas.QuotaObserverChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.quotas.QuotaObserverChore(org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.hbase.master.MetricsMaster);\n  public java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot> getTableQuotaSnapshots();\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot> getNamespaceQuotaSnapshots();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/QuotaSnapshotStore.class;;;public interface org.apache.hadoop.hbase.quotas.QuotaSnapshotStore<T> {\n  public static final org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot NO_QUOTA;\n  public abstract org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota getSpaceQuota(T) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getCurrentState(T);\n  public abstract org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getTargetState(T, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota) throws java.io.IOException;\n  public abstract java.lang.Iterable<java.util.Map$Entry<org.apache.hadoop.hbase.client.RegionInfo, java.lang.Long>> filterBySubject(T);\n  public abstract void setCurrentState(T, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public abstract void setRegionUsage(java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, java.lang.Long>);\n}\n;;;No. The class is an interface that defines methods for interacting with a quota snapshot store, but it does not define a message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/QuotaState.class;;;public class org.apache.hadoop.hbase.quotas.QuotaState {\n  public org.apache.hadoop.hbase.quotas.QuotaState();\n  public org.apache.hadoop.hbase.quotas.QuotaState(long);\n  public synchronized long getLastUpdate();\n  public synchronized long getLastQuery();\n  public synchronized java.lang.String toString();\n  public synchronized boolean isBypass();\n  public synchronized void setQuotas(org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas);\n  public synchronized void update(org.apache.hadoop.hbase.quotas.QuotaState);\n  public synchronized org.apache.hadoop.hbase.quotas.QuotaLimiter getGlobalLimiter();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/QuotaUtil$1.class;;;final class org.apache.hadoop.hbase.quotas.QuotaUtil$1 implements org.apache.hadoop.hbase.quotas.QuotaTableUtil$UserQuotasVisitor {\n  public void visitUserQuotas(java.lang.String, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas);\n  public void visitUserQuotas(java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas);\n  public void visitUserQuotas(java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/QuotaUtil$2.class;;;final class org.apache.hadoop.hbase.quotas.QuotaUtil$2 implements org.apache.hadoop.hbase.quotas.QuotaUtil$KeyFromRow<org.apache.hadoop.hbase.TableName> {\n  public org.apache.hadoop.hbase.TableName getKeyFromRow(byte[]);\n  public double getFactor(org.apache.hadoop.hbase.TableName);\n  public double getFactor(java.lang.Object);\n  public java.lang.Object getKeyFromRow(byte[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/QuotaUtil$3.class;;;final class org.apache.hadoop.hbase.quotas.QuotaUtil$3 implements org.apache.hadoop.hbase.quotas.QuotaUtil$KeyFromRow<java.lang.String> {\n  public java.lang.String getKeyFromRow(byte[]);\n  public double getFactor(java.lang.String);\n  public double getFactor(java.lang.Object);\n  public java.lang.Object getKeyFromRow(byte[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/QuotaUtil$4.class;;;final class org.apache.hadoop.hbase.quotas.QuotaUtil$4 implements org.apache.hadoop.hbase.quotas.QuotaUtil$KeyFromRow<java.lang.String> {\n  public java.lang.String getKeyFromRow(byte[]);\n  public double getFactor(java.lang.String);\n  public double getFactor(java.lang.Object);\n  public java.lang.Object getKeyFromRow(byte[]);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/quotas/QuotaUtil$KeyFromRow.class;;;interface org.apache.hadoop.hbase.quotas.QuotaUtil$KeyFromRow<T> {\n  public abstract T getKeyFromRow(byte[]);\n  public abstract double getFactor(T);\n}\n;;;No. This is an interface definition for a utility class in the Hadoop ecosystem. It is not a specific message definition to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/QuotaUtil.class;;;public class org.apache.hadoop.hbase.quotas.QuotaUtil extends org.apache.hadoop.hbase.quotas.QuotaTableUtil {\n  public static final java.lang.String QUOTA_CONF_KEY;\n  public static final java.lang.String READ_CAPACITY_UNIT_CONF_KEY;\n  public static final long DEFAULT_READ_CAPACITY_UNIT;\n  public static final java.lang.String WRITE_CAPACITY_UNIT_CONF_KEY;\n  public static final long DEFAULT_WRITE_CAPACITY_UNIT;\n  public static final org.apache.hadoop.hbase.client.TableDescriptor QUOTA_TABLE_DESC;\n  public org.apache.hadoop.hbase.quotas.QuotaUtil();\n  public static boolean isQuotaEnabled(org.apache.hadoop.conf.Configuration);\n  public static void addTableQuota(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException;\n  public static void deleteTableQuota(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static void addNamespaceQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException;\n  public static void deleteNamespaceQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String) throws java.io.IOException;\n  public static void addUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException;\n  public static void addUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException;\n  public static void addUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException;\n  public static void deleteUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String) throws java.io.IOException;\n  public static void deleteUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static void deleteUserQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static void addRegionServerQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas) throws java.io.IOException;\n  public static void deleteRegionServerQuota(org.apache.hadoop.hbase.client.Connection, java.lang.String) throws java.io.IOException;\n  public static java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.UserQuotaState> fetchUserQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>, java.util.Map<org.apache.hadoop.hbase.TableName, java.lang.Double>, double) throws java.io.IOException;\n  public static java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.QuotaState> fetchTableQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>, java.util.Map<org.apache.hadoop.hbase.TableName, java.lang.Double>) throws java.io.IOException;\n  public static java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState> fetchNamespaceQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>, double) throws java.io.IOException;\n  public static java.util.Map<java.lang.String, org.apache.hadoop.hbase.quotas.QuotaState> fetchRegionServerQuotas(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>) throws java.io.IOException;\n  public static <K> java.util.Map<K, org.apache.hadoop.hbase.quotas.QuotaState> fetchGlobalQuotas(java.lang.String, org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Get>, org.apache.hadoop.hbase.quotas.QuotaUtil$KeyFromRow<K>) throws java.io.IOException;\n  public static long calculateMutationSize(org.apache.hadoop.hbase.client.Mutation);\n  public static long calculateResultSize(org.apache.hadoop.hbase.client.Result);\n  public static long calculateResultSize(java.util.List<org.apache.hadoop.hbase.client.Result>);\n  public static void enableTableIfNotEnabled(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static void disableTableIfNotDisabled(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n}\n;;;No. This is a utility class with methods for manipulating quotas in HBase. It is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/RateLimiter$1.class;;;class org.apache.hadoop.hbase.quotas.RateLimiter$1 {\n}\n;;;No. This is an inner class definition for the class org.apache.hadoop.hbase.quotas.RateLimiter. It cannot be directly put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/RateLimiter.class;;;public abstract class org.apache.hadoop.hbase.quotas.RateLimiter {\n  public static final java.lang.String QUOTA_RATE_LIMITER_CONF_KEY;\n  public org.apache.hadoop.hbase.quotas.RateLimiter();\n  public synchronized void set(long, java.util.concurrent.TimeUnit);\n  public java.lang.String toString();\n  public synchronized void update(org.apache.hadoop.hbase.quotas.RateLimiter);\n  public synchronized boolean isBypass();\n  public synchronized long getLimit();\n  public synchronized long getAvailable();\n  public boolean canExecute();\n  public synchronized boolean canExecute(long);\n  public void consume();\n  public synchronized void consume(long);\n  public long waitInterval();\n  public synchronized long waitInterval(long);\n  public abstract void setNextRefillTime(long);\n  public abstract long getNextRefillTime();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/RegionServerRpcQuotaManager$1.class;;;class org.apache.hadoop.hbase.quotas.RegionServerRpcQuotaManager$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/RegionServerRpcQuotaManager.class;;;public class org.apache.hadoop.hbase.quotas.RegionServerRpcQuotaManager {\n  public org.apache.hadoop.hbase.quotas.RegionServerRpcQuotaManager(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public void start(org.apache.hadoop.hbase.ipc.RpcScheduler) throws java.io.IOException;\n  public void stop();\n  public void switchRpcThrottle(boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.quotas.OperationQuota getQuota(org.apache.hadoop.security.UserGroupInformation, org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.quotas.OperationQuota checkQuota(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.quotas.OperationQuota$OperationType) throws java.io.IOException, org.apache.hadoop.hbase.quotas.RpcThrottlingException;\n  public org.apache.hadoop.hbase.quotas.OperationQuota checkQuota(org.apache.hadoop.hbase.regionserver.Region, java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$Action>) throws java.io.IOException, org.apache.hadoop.hbase.quotas.RpcThrottlingException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class with several methods that can be called to manage Region Server RPC quotas in Hadoop HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/RegionServerSpaceQuotaManager.class;;;public class org.apache.hadoop.hbase.quotas.RegionServerSpaceQuotaManager {\n  public org.apache.hadoop.hbase.quotas.RegionServerSpaceQuotaManager(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public synchronized void start() throws java.io.IOException;\n  public synchronized void stop();\n  public boolean isStarted();\n  public java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot> copyQuotaSnapshots();\n  public void updateQuotaSnapshot(java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot>);\n  public org.apache.hadoop.hbase.quotas.ActivePolicyEnforcement getActiveEnforcements();\n  public java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot> getActivePoliciesAsMap();\n  public void enforceViolationPolicy(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public void disableViolationPolicyEnforcement(org.apache.hadoop.hbase.TableName);\n  public boolean areCompactionsDisabled(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.quotas.RegionSizeStore getRegionSizeStore();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$FileArchiveNotificationRequest buildFileArchiveRequest(org.apache.hadoop.hbase.TableName, java.util.Collection<java.util.Map$Entry<java.lang.String, java.lang.Long>>);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/RegionSize.class;;;public interface org.apache.hadoop.hbase.quotas.RegionSize extends org.apache.hadoop.hbase.io.HeapSize {\n  public abstract org.apache.hadoop.hbase.quotas.RegionSize setSize(long);\n  public abstract org.apache.hadoop.hbase.quotas.RegionSize incrementSize(long);\n  public abstract long getSize();\n}\n;;;No. This is an interface definition for a class related to managing HBase region size quotas. It is not a specific message that would be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/quotas/RegionSizeImpl.class;;;public class org.apache.hadoop.hbase.quotas.RegionSizeImpl implements org.apache.hadoop.hbase.quotas.RegionSize {\n  public org.apache.hadoop.hbase.quotas.RegionSizeImpl(long);\n  public long heapSize();\n  public org.apache.hadoop.hbase.quotas.RegionSizeImpl setSize(long);\n  public org.apache.hadoop.hbase.quotas.RegionSizeImpl incrementSize(long);\n  public long getSize();\n  public org.apache.hadoop.hbase.quotas.RegionSize incrementSize(long);\n  public org.apache.hadoop.hbase.quotas.RegionSize setSize(long);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/RegionSizeReportingChore.class;;;public class org.apache.hadoop.hbase.quotas.RegionSizeReportingChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.quotas.RegionSizeReportingChore(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/quotas/RegionSizeStore.class;;;public interface org.apache.hadoop.hbase.quotas.RegionSizeStore extends java.lang.Iterable<java.util.Map$Entry<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.quotas.RegionSize>>, org.apache.hadoop.hbase.io.HeapSize {\n  public abstract org.apache.hadoop.hbase.quotas.RegionSize getRegionSize(org.apache.hadoop.hbase.client.RegionInfo);\n  public abstract void put(org.apache.hadoop.hbase.client.RegionInfo, long);\n  public abstract void incrementRegionSize(org.apache.hadoop.hbase.client.RegionInfo, long);\n  public abstract org.apache.hadoop.hbase.quotas.RegionSize remove(org.apache.hadoop.hbase.client.RegionInfo);\n  public abstract int size();\n  public abstract boolean isEmpty();\n  public abstract void clear();\n}\n;;;No. This is an interface for a region size store in HBase, but it is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/RegionSizeStoreFactory.class;;;public final class org.apache.hadoop.hbase.quotas.RegionSizeStoreFactory {\n  public static org.apache.hadoop.hbase.quotas.RegionSizeStoreFactory getInstance();\n  public org.apache.hadoop.hbase.quotas.RegionSizeStore createStore();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/RegionSizeStoreImpl.class;;;public class org.apache.hadoop.hbase.quotas.RegionSizeStoreImpl implements org.apache.hadoop.hbase.quotas.RegionSizeStore {\n  public org.apache.hadoop.hbase.quotas.RegionSizeStoreImpl();\n  public java.util.Iterator<java.util.Map$Entry<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.quotas.RegionSize>> iterator();\n  public org.apache.hadoop.hbase.quotas.RegionSize getRegionSize(org.apache.hadoop.hbase.client.RegionInfo);\n  public void put(org.apache.hadoop.hbase.client.RegionInfo, long);\n  public void incrementRegionSize(org.apache.hadoop.hbase.client.RegionInfo, long);\n  public org.apache.hadoop.hbase.quotas.RegionSize remove(org.apache.hadoop.hbase.client.RegionInfo);\n  public long heapSize();\n  public int size();\n  public boolean isEmpty();\n  public void clear();\n}\n;;;No. This class is an implementation of an interface and provides methods to manipulate and store region size data in HBase. It is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/quotas/RpcThrottleStorage.class;;;public class org.apache.hadoop.hbase.quotas.RpcThrottleStorage {\n  public static final java.lang.String RPC_THROTTLE_ZNODE;\n  public static final java.lang.String RPC_THROTTLE_ZNODE_DEFAULT;\n  public org.apache.hadoop.hbase.quotas.RpcThrottleStorage(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.conf.Configuration);\n  public boolean isRpcThrottleEnabled() throws java.io.IOException;\n  public void switchRpcThrottle(boolean) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/SnapshotQuotaObserverChore.class;;;public class org.apache.hadoop.hbase.quotas.SnapshotQuotaObserverChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.quotas.SnapshotQuotaObserverChore(org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.hbase.master.MetricsMaster);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/quotas/SpaceLimitingException.class;;;public class org.apache.hadoop.hbase.quotas.SpaceLimitingException extends org.apache.hadoop.hbase.quotas.QuotaExceededException {\n  public org.apache.hadoop.hbase.quotas.SpaceLimitingException(java.lang.String);\n  public org.apache.hadoop.hbase.quotas.SpaceLimitingException(java.lang.String, java.lang.String);\n  public org.apache.hadoop.hbase.quotas.SpaceLimitingException(java.lang.String, java.lang.String, java.lang.Throwable);\n  public java.lang.String getViolationPolicy();\n  public java.lang.String getMessage();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/SpaceQuotaRefresherChore.class;;;public class org.apache.hadoop.hbase.quotas.SpaceQuotaRefresherChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaRefresherChore(org.apache.hadoop.hbase.quotas.RegionServerSpaceQuotaManager, org.apache.hadoop.hbase.client.Connection);\n  public java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot> fetchSnapshotsFromQuotaTable() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/SpaceQuotaSnapshotNotifier.class;;;public interface org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshotNotifier {\n  public abstract void initialize(org.apache.hadoop.hbase.client.Connection);\n  public abstract void transitionTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/SpaceQuotaSnapshotNotifierFactory.class;;;public class org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshotNotifierFactory {\n  public static final java.lang.String SNAPSHOT_NOTIFIER_KEY;\n  public static final java.lang.Class<? extends org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshotNotifier> SNAPSHOT_NOTIFIER_DEFAULT;\n  public static org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshotNotifierFactory getInstance();\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshotNotifier create(org.apache.hadoop.conf.Configuration);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/quotas/SpaceViolationPolicyEnforcement.class;;;public interface org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement {\n  public abstract void initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public abstract void enable() throws java.io.IOException;\n  public abstract void disable() throws java.io.IOException;\n  public abstract void check(org.apache.hadoop.hbase.client.Mutation) throws org.apache.hadoop.hbase.quotas.SpaceLimitingException;\n  public abstract java.lang.String getPolicyName();\n  public abstract boolean areCompactionsDisabled();\n  public abstract org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getQuotaSnapshot();\n  public abstract boolean shouldCheckBulkLoads();\n  public abstract long computeBulkLoadSize(org.apache.hadoop.fs.FileSystem, java.util.List<java.lang.String>) throws org.apache.hadoop.hbase.quotas.SpaceLimitingException;\n}\n;;;Yes, this class might be put on a message queue as it defines methods that can be called and executed in a distributed system.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/SpaceViolationPolicyEnforcementFactory$1.class;;;class org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcementFactory$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/SpaceViolationPolicyEnforcementFactory.class;;;public class org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcementFactory {\n  public static org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcementFactory getInstance();\n  public org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement create(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement createWithoutViolation(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n}\n;;;No, this class is not a message definition. It contains methods for creating instances of a SpaceViolationPolicyEnforcement object, but it is not designed to be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class defining methods for creating instances of a SpaceViolationPolicyEnforcement object.;;;N
org/apache/hadoop/hbase/quotas/TableQuotaSnapshotStore.class;;;public class org.apache.hadoop.hbase.quotas.TableQuotaSnapshotStore implements org.apache.hadoop.hbase.quotas.QuotaSnapshotStore<org.apache.hadoop.hbase.TableName> {\n  public org.apache.hadoop.hbase.quotas.TableQuotaSnapshotStore(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.quotas.QuotaObserverChore, java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, java.lang.Long>);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota getSpaceQuota(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getCurrentState(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getTargetState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota) throws java.io.IOException;\n  public java.lang.Iterable<java.util.Map$Entry<org.apache.hadoop.hbase.client.RegionInfo, java.lang.Long>> filterBySubject(org.apache.hadoop.hbase.TableName);\n  public void setCurrentState(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public void setRegionUsage(java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, java.lang.Long>);\n  public void setCurrentState(java.lang.Object, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public java.lang.Iterable filterBySubject(java.lang.Object);\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getTargetState(java.lang.Object, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota) throws java.io.IOException;\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getCurrentState(java.lang.Object);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$SpaceQuota getSpaceQuota(java.lang.Object) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/TableSpaceQuotaSnapshotNotifier.class;;;public class org.apache.hadoop.hbase.quotas.TableSpaceQuotaSnapshotNotifier implements org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshotNotifier {\n  public org.apache.hadoop.hbase.quotas.TableSpaceQuotaSnapshotNotifier();\n  public void transitionTable(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot) throws java.io.IOException;\n  public void initialize(org.apache.hadoop.hbase.client.Connection);\n}\n;;;No, the class is not a message definition that might be put on a message queue. It is a Java class that implements an interface for supporting space quota management in Apache HBase.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/quotas/TimeBasedLimiter.class;;;public class org.apache.hadoop.hbase.quotas.TimeBasedLimiter implements org.apache.hadoop.hbase.quotas.QuotaLimiter {\n  public void update(org.apache.hadoop.hbase.quotas.TimeBasedLimiter);\n  public void checkQuota(long, long, long, long, long, long) throws org.apache.hadoop.hbase.quotas.RpcThrottlingException;\n  public void grabQuota(long, long, long, long, long, long);\n  public void consumeWrite(long, long);\n  public void consumeRead(long, long);\n  public boolean isBypass();\n  public long getWriteAvailable();\n  public long getReadAvailable();\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/UserQuotaState.class;;;public class org.apache.hadoop.hbase.quotas.UserQuotaState extends org.apache.hadoop.hbase.quotas.QuotaState {\n  public org.apache.hadoop.hbase.quotas.UserQuotaState();\n  public org.apache.hadoop.hbase.quotas.UserQuotaState(long);\n  public synchronized java.lang.String toString();\n  public synchronized boolean isBypass();\n  public synchronized boolean hasBypassGlobals();\n  public synchronized void setQuotas(org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas);\n  public synchronized void setQuotas(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas);\n  public void setQuotas(java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$Quotas);\n  public synchronized void update(org.apache.hadoop.hbase.quotas.QuotaState);\n  public synchronized org.apache.hadoop.hbase.quotas.QuotaLimiter getTableLimiter(org.apache.hadoop.hbase.TableName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/policies/AbstractViolationPolicyEnforcement.class;;;public abstract class org.apache.hadoop.hbase.quotas.policies.AbstractViolationPolicyEnforcement implements org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement {\n  public org.apache.hadoop.hbase.quotas.policies.AbstractViolationPolicyEnforcement();\n  public void setRegionServerServices(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public void setTableName(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.regionserver.RegionServerServices getRegionServerServices();\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public void setQuotaSnapshot(org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot getQuotaSnapshot();\n  public void initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot);\n  public boolean areCompactionsDisabled();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/quotas/policies/DefaultViolationPolicyEnforcement.class;;;public class org.apache.hadoop.hbase.quotas.policies.DefaultViolationPolicyEnforcement extends org.apache.hadoop.hbase.quotas.policies.AbstractViolationPolicyEnforcement {\n  public org.apache.hadoop.hbase.quotas.policies.DefaultViolationPolicyEnforcement();\n  public void enable() throws java.io.IOException;\n  public void disable() throws java.io.IOException;\n  public java.lang.String getPolicyName();\n  public void check(org.apache.hadoop.hbase.client.Mutation) throws org.apache.hadoop.hbase.quotas.SpaceLimitingException;\n  public boolean shouldCheckBulkLoads();\n  public long computeBulkLoadSize(org.apache.hadoop.fs.FileSystem, java.util.List<java.lang.String>) throws org.apache.hadoop.hbase.quotas.SpaceLimitingException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/policies/DisableTableViolationPolicyEnforcement.class;;;public class org.apache.hadoop.hbase.quotas.policies.DisableTableViolationPolicyEnforcement extends org.apache.hadoop.hbase.quotas.policies.DefaultViolationPolicyEnforcement {\n  public org.apache.hadoop.hbase.quotas.policies.DisableTableViolationPolicyEnforcement();\n  public void enable() throws java.io.IOException;\n  public void disable() throws java.io.IOException;\n  public void check(org.apache.hadoop.hbase.client.Mutation) throws org.apache.hadoop.hbase.quotas.SpaceLimitingException;\n  public java.lang.String getPolicyName();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/policies/MissingSnapshotViolationPolicyEnforcement.class;;;public final class org.apache.hadoop.hbase.quotas.policies.MissingSnapshotViolationPolicyEnforcement extends org.apache.hadoop.hbase.quotas.policies.AbstractViolationPolicyEnforcement {\n  public static org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcement getInstance();\n  public boolean shouldCheckBulkLoads();\n  public long computeBulkLoadSize(org.apache.hadoop.fs.FileSystem, java.util.List<java.lang.String>) throws org.apache.hadoop.hbase.quotas.SpaceLimitingException;\n  public void enable() throws java.io.IOException;\n  public void disable() throws java.io.IOException;\n  public void check(org.apache.hadoop.hbase.client.Mutation) throws org.apache.hadoop.hbase.quotas.SpaceLimitingException;\n  public java.lang.String getPolicyName();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/policies/NoInsertsViolationPolicyEnforcement.class;;;public class org.apache.hadoop.hbase.quotas.policies.NoInsertsViolationPolicyEnforcement extends org.apache.hadoop.hbase.quotas.policies.DefaultViolationPolicyEnforcement {\n  public org.apache.hadoop.hbase.quotas.policies.NoInsertsViolationPolicyEnforcement();\n  public void enable();\n  public void disable();\n  public void check(org.apache.hadoop.hbase.client.Mutation) throws org.apache.hadoop.hbase.quotas.SpaceLimitingException;\n  public java.lang.String getPolicyName();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/policies/NoWritesCompactionsViolationPolicyEnforcement.class;;;public class org.apache.hadoop.hbase.quotas.policies.NoWritesCompactionsViolationPolicyEnforcement extends org.apache.hadoop.hbase.quotas.policies.NoWritesViolationPolicyEnforcement {\n  public org.apache.hadoop.hbase.quotas.policies.NoWritesCompactionsViolationPolicyEnforcement();\n  public synchronized void enable();\n  public synchronized void disable();\n  public java.lang.String getPolicyName();\n  public boolean areCompactionsDisabled();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/quotas/policies/NoWritesViolationPolicyEnforcement.class;;;public class org.apache.hadoop.hbase.quotas.policies.NoWritesViolationPolicyEnforcement extends org.apache.hadoop.hbase.quotas.policies.DefaultViolationPolicyEnforcement {\n  public org.apache.hadoop.hbase.quotas.policies.NoWritesViolationPolicyEnforcement();\n  public void enable();\n  public void disable();\n  public void check(org.apache.hadoop.hbase.client.Mutation) throws org.apache.hadoop.hbase.quotas.SpaceLimitingException;\n  public java.lang.String getPolicyName();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/AbstractMemStore.class;;;public abstract class org.apache.hadoop.hbase.regionserver.AbstractMemStore implements org.apache.hadoop.hbase.regionserver.MemStore {\n  public static final long FIXED_OVERHEAD;\n  public static final long DEEP_OVERHEAD;\n  public static void addToScanners(java.util.List<? extends org.apache.hadoop.hbase.regionserver.Segment>, long, java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner>);\n  public abstract void updateLowestUnflushedSequenceIdInWAL(boolean);\n  public void add(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public void add(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public void upsert(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, long, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public long timeOfOldestEdit();\n  public void clearSnapshot(long) throws org.apache.hadoop.hbase.exceptions.UnexpectedStateException;\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getSnapshotSize();\n  public java.lang.String toString();\n}\n;;;No, this is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/AbstractMultiFileWriter$WriterFactory.class;;;public interface org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter$WriterFactory {\n  public abstract org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter() throws java.io.IOException;\n  public default org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriterWithStoragePolicy(java.lang.String) throws java.io.IOException;\n}\n;;;No. This is an interface definition for a writer factory in the HBase region server. It is not a message definition that could be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface defining a method that creates a store file writer in HBase.;;;N
org/apache/hadoop/hbase/regionserver/AbstractMultiFileWriter.class;;;public abstract class org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter implements org.apache.hadoop.hbase.regionserver.CellSink,org.apache.hadoop.hbase.regionserver.ShipperListener {\n  public org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter();\n  public void init(org.apache.hadoop.hbase.regionserver.StoreScanner, org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter$WriterFactory);\n  public java.util.List<org.apache.hadoop.fs.Path> commitWriters(long, boolean) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.Path> commitWriters(long, boolean, java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.Path> abortWriters();\n  public void beforeShipped() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/AdaptiveMemStoreCompactionStrategy.class;;;public class org.apache.hadoop.hbase.regionserver.AdaptiveMemStoreCompactionStrategy extends org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy {\n  public static final java.lang.String ADAPTIVE_COMPACTION_THRESHOLD_KEY;\n  public static final java.lang.String ADAPTIVE_INITIAL_COMPACTION_PROBABILITY_KEY;\n  public org.apache.hadoop.hbase.regionserver.AdaptiveMemStoreCompactionStrategy(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action getAction(org.apache.hadoop.hbase.regionserver.VersionedSegmentsList);\n  public void updateStats(org.apache.hadoop.hbase.regionserver.Segment);\n  public void resetStats();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/BasicMemStoreCompactionStrategy.class;;;public class org.apache.hadoop.hbase.regionserver.BasicMemStoreCompactionStrategy extends org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy {\n  public org.apache.hadoop.hbase.regionserver.BasicMemStoreCompactionStrategy(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action getAction(org.apache.hadoop.hbase.regionserver.VersionedSegmentsList);\n}\n;;;No. This is a class definition for a class that extends another class and contains constructors and methods. It is not a message definition that can be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/BootstrapNodeManager.class;;;public class org.apache.hadoop.hbase.regionserver.BootstrapNodeManager {\n  public static final java.lang.String REQUEST_MASTER_INTERVAL_SECS;\n  public static final long DEFAULT_REQUEST_MASTER_INTERVAL_SECS;\n  public static final java.lang.String REQUEST_MASTER_MIN_INTERVAL_SECS;\n  public static final long DEFAULT_REQUEST_MASTER_MIN_INTERVAL_SECS;\n  public static final java.lang.String REQUEST_REGIONSERVER_INTERVAL_SECS;\n  public static final long DEFAULT_REQUEST_REGIONSERVER_INTERVAL_SECS;\n  public org.apache.hadoop.hbase.regionserver.BootstrapNodeManager(org.apache.hadoop.hbase.client.AsyncClusterConnection, org.apache.hadoop.hbase.zookeeper.MasterAddressTracker);\n  public void stop();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getBootstrapNodes();\n}\n;;;No.;;;N;;;No, it is not a task definition, but rather a class that manages bootstrap nodes in HBase.;;;N
org/apache/hadoop/hbase/regionserver/BrokenStoreFileCleaner.class;;;public class org.apache.hadoop.hbase.regionserver.BrokenStoreFileCleaner extends org.apache.hadoop.hbase.ScheduledChore {\n  public static final java.lang.String BROKEN_STOREFILE_CLEANER_ENABLED;\n  public static final boolean DEFAULT_BROKEN_STOREFILE_CLEANER_ENABLED;\n  public static final java.lang.String BROKEN_STOREFILE_CLEANER_TTL;\n  public static final long DEFAULT_BROKEN_STOREFILE_CLEANER_TTL;\n  public static final java.lang.String BROKEN_STOREFILE_CLEANER_DELAY;\n  public static final int DEFAULT_BROKEN_STOREFILE_CLEANER_DELAY;\n  public static final java.lang.String BROKEN_STOREFILE_CLEANER_DELAY_JITTER;\n  public static final double DEFAULT_BROKEN_STOREFILE_CLEANER_DELAY_JITTER;\n  public static final java.lang.String BROKEN_STOREFILE_CLEANER_PERIOD;\n  public static final int DEFAULT_BROKEN_STOREFILE_CLEANER_PERIOD;\n  public org.apache.hadoop.hbase.regionserver.BrokenStoreFileCleaner(int, int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public boolean setEnabled(boolean);\n  public boolean getEnabled();\n  public void chore();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/BusyRegionSplitPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.BusyRegionSplitPolicy extends org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy {\n  public static final float DEFAULT_MAX_BLOCKED_REQUESTS;\n  public static final long DEFAULT_MIN_AGE_MS;\n  public static final long DEFAULT_AGGREGATION_WINDOW;\n  public org.apache.hadoop.hbase.regionserver.BusyRegionSplitPolicy();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ByteBufferChunkKeyValue.class;;;public class org.apache.hadoop.hbase.regionserver.ByteBufferChunkKeyValue extends org.apache.hadoop.hbase.ByteBufferKeyValue {\n  public org.apache.hadoop.hbase.regionserver.ByteBufferChunkKeyValue(java.nio.ByteBuffer, int, int);\n  public org.apache.hadoop.hbase.regionserver.ByteBufferChunkKeyValue(java.nio.ByteBuffer, int, int, long);\n  public int getChunkId();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/CSLMImmutableSegment.class;;;public class org.apache.hadoop.hbase.regionserver.CSLMImmutableSegment extends org.apache.hadoop.hbase.regionserver.ImmutableSegment {\n  public static final long DEEP_OVERHEAD_CSLM;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CellArrayImmutableSegment.class;;;public class org.apache.hadoop.hbase.regionserver.CellArrayImmutableSegment extends org.apache.hadoop.hbase.regionserver.ImmutableSegment {\n  public static final long DEEP_OVERHEAD_CAM;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CellArrayMap.class;;;public class org.apache.hadoop.hbase.regionserver.CellArrayMap extends org.apache.hadoop.hbase.regionserver.CellFlatMap {\n  public org.apache.hadoop.hbase.regionserver.CellArrayMap(java.util.Comparator<? super org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.Cell[], int, int, boolean);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/CellChunkImmutableSegment.class;;;public class org.apache.hadoop.hbase.regionserver.CellChunkImmutableSegment extends org.apache.hadoop.hbase.regionserver.ImmutableSegment {\n  public static final long DEEP_OVERHEAD_CCM;\n  public static final float INDEX_CHUNK_UNUSED_SPACE_PRECENTAGE;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CellChunkMap.class;;;public class org.apache.hadoop.hbase.regionserver.CellChunkMap extends org.apache.hadoop.hbase.regionserver.CellFlatMap {\n  public org.apache.hadoop.hbase.regionserver.CellChunkMap(java.util.Comparator<? super org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.Chunk[], int, int, boolean);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CellFlatMap$1.class;;;class org.apache.hadoop.hbase.regionserver.CellFlatMap$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be an Inner class defined within the org.apache.hadoop.hbase.regionserver.CellFlatMap class.;;;N
org/apache/hadoop/hbase/regionserver/CellFlatMap$CellFlatMapCollection.class;;;final class org.apache.hadoop.hbase.regionserver.CellFlatMap$CellFlatMapCollection implements java.util.Collection<org.apache.hadoop.hbase.Cell> {\n  public int size();\n  public boolean isEmpty();\n  public void clear();\n  public boolean contains(java.lang.Object);\n  public java.util.Iterator<org.apache.hadoop.hbase.Cell> iterator();\n  public java.lang.Object[] toArray();\n  public <T> T[] toArray(T[]);\n  public boolean add(org.apache.hadoop.hbase.Cell);\n  public boolean remove(java.lang.Object);\n  public boolean containsAll(java.util.Collection<?>);\n  public boolean addAll(java.util.Collection<? extends org.apache.hadoop.hbase.Cell>);\n  public boolean removeAll(java.util.Collection<?>);\n  public boolean retainAll(java.util.Collection<?>);\n  public boolean add(java.lang.Object);\n}\n;;;No. It is a collection implementation for HBase cells. It is not a message definition that could be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/CellFlatMap$CellFlatMapEntry.class;;;class org.apache.hadoop.hbase.regionserver.CellFlatMap$CellFlatMapEntry implements java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> {\n  public org.apache.hadoop.hbase.regionserver.CellFlatMap$CellFlatMapEntry(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.Cell getKey();\n  public org.apache.hadoop.hbase.Cell getValue();\n  public org.apache.hadoop.hbase.Cell setValue(org.apache.hadoop.hbase.Cell);\n  public java.lang.Object setValue(java.lang.Object);\n  public java.lang.Object getValue();\n  public java.lang.Object getKey();\n}\n;;;No. This is a Java class definition, not a message definition. It does not contain any data to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CellFlatMap$CellFlatMapIterator.class;;;final class org.apache.hadoop.hbase.regionserver.CellFlatMap$CellFlatMapIterator implements java.util.Iterator<org.apache.hadoop.hbase.Cell> {\n  public boolean hasNext();\n  public org.apache.hadoop.hbase.Cell next();\n  public void remove();\n  public java.lang.Object next();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CellFlatMap.class;;;public abstract class org.apache.hadoop.hbase.regionserver.CellFlatMap implements java.util.NavigableMap<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> {\n  public org.apache.hadoop.hbase.regionserver.CellFlatMap(java.util.Comparator<? super org.apache.hadoop.hbase.Cell>, int, int, boolean);\n  public java.util.Comparator<? super org.apache.hadoop.hbase.Cell> comparator();\n  public int size();\n  public boolean isEmpty();\n  public java.util.NavigableMap<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> subMap(org.apache.hadoop.hbase.Cell, boolean, org.apache.hadoop.hbase.Cell, boolean);\n  public java.util.NavigableMap<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> headMap(org.apache.hadoop.hbase.Cell, boolean);\n  public java.util.NavigableMap<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> tailMap(org.apache.hadoop.hbase.Cell, boolean);\n  public java.util.NavigableMap<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> descendingMap();\n  public java.util.NavigableMap<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> subMap(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public java.util.NavigableMap<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> headMap(org.apache.hadoop.hbase.Cell);\n  public java.util.NavigableMap<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> tailMap(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.Cell firstKey();\n  public org.apache.hadoop.hbase.Cell lastKey();\n  public org.apache.hadoop.hbase.Cell lowerKey(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.Cell floorKey(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.Cell ceilingKey(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.Cell higherKey(org.apache.hadoop.hbase.Cell);\n  public boolean containsKey(java.lang.Object);\n  public boolean containsValue(java.lang.Object);\n  public org.apache.hadoop.hbase.Cell get(java.lang.Object);\n  public java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> lowerEntry(org.apache.hadoop.hbase.Cell);\n  public java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> higherEntry(org.apache.hadoop.hbase.Cell);\n  public java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> ceilingEntry(org.apache.hadoop.hbase.Cell);\n  public java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> floorEntry(org.apache.hadoop.hbase.Cell);\n  public java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> firstEntry();\n  public java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> lastEntry();\n  public java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> pollFirstEntry();\n  public java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell> pollLastEntry();\n  public org.apache.hadoop.hbase.Cell put(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public void clear();\n  public org.apache.hadoop.hbase.Cell remove(java.lang.Object);\n  public void putAll(java.util.Map<? extends org.apache.hadoop.hbase.Cell, ? extends org.apache.hadoop.hbase.Cell>);\n  public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> navigableKeySet();\n  public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> descendingKeySet();\n  public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> keySet();\n  public java.util.Collection<org.apache.hadoop.hbase.Cell> values();\n  public java.util.Set<java.util.Map$Entry<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> entrySet();\n  public java.util.SortedMap tailMap(java.lang.Object);\n  public java.util.SortedMap headMap(java.lang.Object);\n  public java.util.SortedMap subMap(java.lang.Object, java.lang.Object);\n  public java.util.NavigableMap tailMap(java.lang.Object, boolean);\n  public java.util.NavigableMap headMap(java.lang.Object, boolean);\n  public java.util.NavigableMap subMap(java.lang.Object, boolean, java.lang.Object, boolean);\n  public java.lang.Object higherKey(java.lang.Object);\n  public java.util.Map$Entry higherEntry(java.lang.Object);\n  public java.lang.Object ceilingKey(java.lang.Object);\n  public java.util.Map$Entry ceilingEntry(java.lang.Object);\n  public java.lang.Object floorKey(java.lang.Object);\n  public java.util.Map$Entry floorEntry(java.lang.Object);\n  public java.lang.Object lowerKey(java.lang.Object);\n  public java.util.Map$Entry lowerEntry(java.lang.Object);\n  public java.util.Set keySet();\n  public java.lang.Object lastKey();\n  public java.lang.Object firstKey();\n  public java.lang.Object remove(java.lang.Object);\n  public java.lang.Object put(java.lang.Object, java.lang.Object);\n  public java.lang.Object get(java.lang.Object);\n}\n;;;No. This class is an implementation of the java.util.NavigableMap interface and is not specific to a particular message format or system, so it is not a message definition.;;;N;;;No, it is not a task definition. It is a class definition and does not describe a specific task that needs to be executed.;;;N
org/apache/hadoop/hbase/regionserver/CellSet.class;;;public class org.apache.hadoop.hbase.regionserver.CellSet implements java.util.NavigableSet<org.apache.hadoop.hbase.Cell> {\n  public static final int UNKNOWN_NUM_UNIQUES;\n  public org.apache.hadoop.hbase.regionserver.CellSet(org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.Cell ceiling(org.apache.hadoop.hbase.Cell);\n  public java.util.Iterator<org.apache.hadoop.hbase.Cell> descendingIterator();\n  public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> descendingSet();\n  public org.apache.hadoop.hbase.Cell floor(org.apache.hadoop.hbase.Cell);\n  public java.util.SortedSet<org.apache.hadoop.hbase.Cell> headSet(org.apache.hadoop.hbase.Cell);\n  public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> headSet(org.apache.hadoop.hbase.Cell, boolean);\n  public org.apache.hadoop.hbase.Cell higher(org.apache.hadoop.hbase.Cell);\n  public java.util.Iterator<org.apache.hadoop.hbase.Cell> iterator();\n  public org.apache.hadoop.hbase.Cell lower(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.Cell pollFirst();\n  public org.apache.hadoop.hbase.Cell pollLast();\n  public java.util.SortedSet<org.apache.hadoop.hbase.Cell> subSet(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> subSet(org.apache.hadoop.hbase.Cell, boolean, org.apache.hadoop.hbase.Cell, boolean);\n  public java.util.SortedSet<org.apache.hadoop.hbase.Cell> tailSet(org.apache.hadoop.hbase.Cell);\n  public java.util.NavigableSet<org.apache.hadoop.hbase.Cell> tailSet(org.apache.hadoop.hbase.Cell, boolean);\n  public java.util.Comparator<? super org.apache.hadoop.hbase.Cell> comparator();\n  public org.apache.hadoop.hbase.Cell first();\n  public org.apache.hadoop.hbase.Cell last();\n  public boolean add(org.apache.hadoop.hbase.Cell);\n  public boolean addAll(java.util.Collection<? extends org.apache.hadoop.hbase.Cell>);\n  public void clear();\n  public boolean contains(java.lang.Object);\n  public boolean containsAll(java.util.Collection<?>);\n  public boolean isEmpty();\n  public boolean remove(java.lang.Object);\n  public boolean removeAll(java.util.Collection<?>);\n  public boolean retainAll(java.util.Collection<?>);\n  public org.apache.hadoop.hbase.Cell get(org.apache.hadoop.hbase.Cell);\n  public int size();\n  public java.lang.Object[] toArray();\n  public <T> T[] toArray(T[]);\n  public int getNumUniqueKeys();\n  public java.util.SortedSet tailSet(java.lang.Object);\n  public java.util.SortedSet headSet(java.lang.Object);\n  public java.util.SortedSet subSet(java.lang.Object, java.lang.Object);\n  public java.util.NavigableSet tailSet(java.lang.Object, boolean);\n  public java.util.NavigableSet headSet(java.lang.Object, boolean);\n  public java.util.NavigableSet subSet(java.lang.Object, boolean, java.lang.Object, boolean);\n  public java.lang.Object pollLast();\n  public java.lang.Object pollFirst();\n  public java.lang.Object higher(java.lang.Object);\n  public java.lang.Object ceiling(java.lang.Object);\n  public java.lang.Object floor(java.lang.Object);\n  public java.lang.Object lower(java.lang.Object);\n  public java.lang.Object last();\n  public java.lang.Object first();\n  public boolean add(java.lang.Object);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/CellSink.class;;;public interface org.apache.hadoop.hbase.regionserver.CellSink {\n  public abstract void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/ChangedReadersObserver.class;;;public interface org.apache.hadoop.hbase.regionserver.ChangedReadersObserver {\n  public abstract long getReadPoint();\n  public abstract void updateReaders(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/Chunk.class;;;public abstract class org.apache.hadoop.hbase.regionserver.Chunk {\n  public org.apache.hadoop.hbase.regionserver.Chunk(int, int, org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType);\n  public org.apache.hadoop.hbase.regionserver.Chunk(int, int, org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType, boolean);\n  public void init();\n  public int alloc(int);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ChunkCreator$1.class;;;class org.apache.hadoop.hbase.regionserver.ChunkCreator$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ChunkCreator$ChunkType.class;;;public final class org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType> {\n  public static final org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType INDEX_CHUNK;\n  public static final org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType DATA_CHUNK;\n  public static final org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType JUMBO_CHUNK;\n  public static org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType[] values();\n  public static org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No, it is not a task definition but rather a class definition for enumerating different types of chunks in the HBase region server.;;;N
org/apache/hadoop/hbase/regionserver/ChunkCreator$MemStoreChunkPool$StatisticsThread.class;;;class org.apache.hadoop.hbase.regionserver.ChunkCreator$MemStoreChunkPool$StatisticsThread extends java.lang.Thread {\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ChunkCreator$MemStoreChunkPool.class;;;class org.apache.hadoop.hbase.regionserver.ChunkCreator$MemStoreChunkPool implements org.apache.hadoop.hbase.regionserver.HeapMemoryManager$HeapMemoryTuneObserver {\n  public void onHeapMemoryTune(long, long);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ChunkCreator.class;;;public class org.apache.hadoop.hbase.regionserver.ChunkCreator {\n  public static final int SIZEOF_CHUNK_HEADER;\n  public static org.apache.hadoop.hbase.regionserver.ChunkCreator initialize(int, boolean, long, float, float, org.apache.hadoop.hbase.regionserver.HeapMemoryManager, float);\n  public static org.apache.hadoop.hbase.regionserver.ChunkCreator getInstance();\n}\n;;;No. This class defines static methods and variables for creating and managing memory chunks in a HBase region server. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CompactSplit$1.class;;;final class org.apache.hadoop.hbase.regionserver.CompactSplit$1 implements org.apache.hadoop.hbase.regionserver.CompactSplit$CompactionCompleteTracker {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CompactSplit$2.class;;;final class org.apache.hadoop.hbase.regionserver.CompactSplit$2 implements java.util.Comparator<java.lang.Runnable> {\n  public int compare(java.lang.Runnable, java.lang.Runnable);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No. This is a comparator class definition, which is not inherently a task or message that can be put on a queue.;;;N
org/apache/hadoop/hbase/regionserver/CompactSplit$AggregatingCompleteTracker.class;;;final class org.apache.hadoop.hbase.regionserver.CompactSplit$AggregatingCompleteTracker implements org.apache.hadoop.hbase.regionserver.CompactSplit$CompactionCompleteTracker {\n  public org.apache.hadoop.hbase.regionserver.CompactSplit$AggregatingCompleteTracker(org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, int);\n  public void completed(org.apache.hadoop.hbase.regionserver.Store);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/CompactSplit$CompactionCompleteTracker.class;;;public interface org.apache.hadoop.hbase.regionserver.CompactSplit$CompactionCompleteTracker {\n  public default void completed(org.apache.hadoop.hbase.regionserver.Store);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/CompactSplit$CompactionRunner.class;;;final class org.apache.hadoop.hbase.regionserver.CompactSplit$CompactionRunner implements java.lang.Runnable {\n  public org.apache.hadoop.hbase.regionserver.CompactSplit$CompactionRunner(org.apache.hadoop.hbase.regionserver.CompactSplit, org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.CompactSplit$CompactionCompleteTracker, java.util.concurrent.ThreadPoolExecutor, org.apache.hadoop.hbase.security.User);\n  public java.lang.String toString();\n  public void run();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/CompactSplit$Rejection.class;;;class org.apache.hadoop.hbase.regionserver.CompactSplit$Rejection implements java.util.concurrent.RejectedExecutionHandler {\n  public void rejectedExecution(java.lang.Runnable, java.util.concurrent.ThreadPoolExecutor);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CompactSplit.class;;;public class org.apache.hadoop.hbase.regionserver.CompactSplit implements org.apache.hadoop.hbase.regionserver.compactions.CompactionRequester,org.apache.hadoop.hbase.conf.PropagatingConfigurationObserver {\n  public static final java.lang.String LARGE_COMPACTION_THREADS;\n  public static final int LARGE_COMPACTION_THREADS_DEFAULT;\n  public static final java.lang.String SMALL_COMPACTION_THREADS;\n  public static final int SMALL_COMPACTION_THREADS_DEFAULT;\n  public static final java.lang.String SPLIT_THREADS;\n  public static final int SPLIT_THREADS_DEFAULT;\n  public static final java.lang.String REGION_SERVER_REGION_SPLIT_LIMIT;\n  public static final int DEFAULT_REGION_SERVER_REGION_SPLIT_LIMIT;\n  public static final java.lang.String HBASE_REGION_SERVER_ENABLE_COMPACTION;\n  public org.apache.hadoop.hbase.regionserver.CompactSplit(org.apache.hadoop.conf.Configuration);\n  public java.lang.String toString();\n  public java.lang.String dumpQueue();\n  public synchronized boolean requestSplit(org.apache.hadoop.hbase.regionserver.Region);\n  public synchronized void requestCompaction(org.apache.hadoop.hbase.regionserver.HRegion, java.lang.String, int, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public synchronized void requestCompaction(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HStore, java.lang.String, int, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void switchCompaction(boolean);\n  public synchronized void requestSystemCompaction(org.apache.hadoop.hbase.regionserver.HRegion, java.lang.String) throws java.io.IOException;\n  public void requestSystemCompaction(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HStore, java.lang.String) throws java.io.IOException;\n  public synchronized void requestSystemCompaction(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HStore, java.lang.String, boolean) throws java.io.IOException;\n  public int getCompactionQueueSize();\n  public int getLargeCompactionQueueSize();\n  public int getSmallCompactionQueueSize();\n  public int getSplitQueueSize();\n  public int getRegionSplitLimit();\n  public boolean isUnderCompaction(org.apache.hadoop.hbase.regionserver.HStore);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public void registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public void deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public org.apache.hadoop.hbase.regionserver.throttle.ThroughputController getCompactionThroughputController();\n  public void clearLongCompactionsQueue();\n  public void clearShortCompactionsQueue();\n  public boolean isCompactionsEnabled();\n  public void setCompactionsEnabled(boolean);\n}\n;;;Yes, this class includes methods that could be used to request compaction or splitting of regions in a HBase database, which could be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/CompactedHFilesDischargeHandler.class;;;public class org.apache.hadoop.hbase.regionserver.CompactedHFilesDischargeHandler extends org.apache.hadoop.hbase.executor.EventHandler {\n  public org.apache.hadoop.hbase.regionserver.CompactedHFilesDischargeHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.executor.EventType, org.apache.hadoop.hbase.regionserver.HStore);\n  public void process() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/CompactedHFilesDischarger.class;;;public class org.apache.hadoop.hbase.regionserver.CompactedHFilesDischarger extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.regionserver.CompactedHFilesDischarger(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public org.apache.hadoop.hbase.regionserver.CompactedHFilesDischarger(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.hbase.regionserver.RegionServerServices, boolean);\n  public void chore();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/CompactingMemStore$1.class;;;class org.apache.hadoop.hbase.regionserver.CompactingMemStore$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/CompactingMemStore$InMemoryCompactionRunnable.class;;;class org.apache.hadoop.hbase.regionserver.CompactingMemStore$InMemoryCompactionRunnable implements java.lang.Runnable {\n  public void run();\n}\n;;;no;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/CompactingMemStore$IndexType.class;;;public final class org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType> {\n  public static final org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType CSLM_MAP;\n  public static final org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType ARRAY_MAP;\n  public static final org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType CHUNK_MAP;\n  public static org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType[] values();\n  public static org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType valueOf(java.lang.String);\n}\n;;;No. This is a Java class definition and not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CompactingMemStore.class;;;public class org.apache.hadoop.hbase.regionserver.CompactingMemStore extends org.apache.hadoop.hbase.regionserver.AbstractMemStore {\n  public static final java.lang.String COMPACTING_MEMSTORE_TYPE_KEY;\n  public static final java.lang.String COMPACTING_MEMSTORE_TYPE_DEFAULT;\n  public static final java.lang.String IN_MEMORY_FLUSH_THRESHOLD_FACTOR_KEY;\n  public static final java.lang.String IN_MEMORY_CONPACTION_POOL_SIZE_KEY;\n  public static final int IN_MEMORY_CONPACTION_POOL_SIZE_DEFAULT;\n  public static final long DEEP_OVERHEAD;\n  public org.apache.hadoop.hbase.regionserver.CompactingMemStore(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.RegionServicesForStores, org.apache.hadoop.hbase.MemoryCompactionPolicy) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize size();\n  public long preFlushSeqIDEstimation();\n  public boolean isSloppy();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSnapshot snapshot();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getFlushableSize();\n  public void setInMemoryCompactionCompleted();\n  public void updateLowestUnflushedSequenceIdInWAL(boolean);\n  public void startReplayingFromWAL();\n  public void stopReplayingFromWAL();\n  public void setCompositeSnapshot(boolean);\n  public boolean swapCompactedSegments(org.apache.hadoop.hbase.regionserver.VersionedSegmentsList, org.apache.hadoop.hbase.regionserver.ImmutableSegment, boolean);\n  public void flattenOneSegment(long, org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action);\n  public org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType getIndexType();\n  public boolean hasImmutableSegments();\n  public org.apache.hadoop.hbase.regionserver.VersionedSegmentsList getImmutableSegments();\n  public long getSmallestReadPoint();\n  public org.apache.hadoop.hbase.regionserver.HStore getStore();\n  public java.lang.String getFamilyName();\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(long) throws java.io.IOException;\n  public void debug();\n}\n;;;No, this class is not a message definition. It is a Java class that defines the behavior and properties of a specific component in the HBase system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CompactionPipeline.class;;;public class org.apache.hadoop.hbase.regionserver.CompactionPipeline {\n  public static final long FIXED_OVERHEAD;\n  public static final long DEEP_OVERHEAD;\n  public org.apache.hadoop.hbase.regionserver.CompactionPipeline(org.apache.hadoop.hbase.regionserver.RegionServicesForStores);\n  public boolean pushHead(org.apache.hadoop.hbase.regionserver.MutableSegment);\n  public org.apache.hadoop.hbase.regionserver.VersionedSegmentsList getVersionedList();\n  public org.apache.hadoop.hbase.regionserver.VersionedSegmentsList getVersionedTail();\n  public boolean swap(org.apache.hadoop.hbase.regionserver.VersionedSegmentsList, org.apache.hadoop.hbase.regionserver.ImmutableSegment, boolean, boolean);\n  public boolean flattenOneSegment(long, org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType, org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action);\n  public boolean isEmpty();\n  public java.util.List<? extends org.apache.hadoop.hbase.regionserver.Segment> getSegments();\n  public long size();\n  public long getMinSequenceId();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getTailSize();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getPipelineSize();\n  public org.apache.hadoop.hbase.regionserver.Segment getTail();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a class definition for a component in the HBase region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CompositeImmutableSegment.class;;;public class org.apache.hadoop.hbase.regionserver.CompositeImmutableSegment extends org.apache.hadoop.hbase.regionserver.ImmutableSegment {\n  public org.apache.hadoop.hbase.regionserver.CompositeImmutableSegment(org.apache.hadoop.hbase.CellComparator, java.util.List<org.apache.hadoop.hbase.regionserver.ImmutableSegment>);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.Segment> getAllSegments();\n  public int getNumOfSegments();\n  public boolean isEmpty();\n  public int getCellsCount();\n  public void close();\n  public org.apache.hadoop.hbase.Cell maybeCloneWithAllocator(org.apache.hadoop.hbase.Cell, boolean);\n  public boolean shouldSeek(org.apache.hadoop.hbase.io.TimeRange, long);\n  public org.apache.hadoop.hbase.regionserver.KeyValueScanner getScanner(long);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(long);\n  public boolean isTagsPresent();\n  public void incScannerCount();\n  public void decScannerCount();\n  public long getDataSize();\n  public long getHeapSize();\n  public long incMemStoreSize(long, long, long, int);\n  public long getMinSequenceId();\n  public org.apache.hadoop.hbase.regionserver.TimeRangeTracker getTimeRangeTracker();\n  public org.apache.hadoop.hbase.Cell last();\n  public java.util.Iterator<org.apache.hadoop.hbase.Cell> iterator();\n  public java.util.SortedSet<org.apache.hadoop.hbase.Cell> headSet(org.apache.hadoop.hbase.Cell);\n  public int compare(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public int compareRows(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public java.lang.String toString();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a Java class for the HBase region server implementation.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy extends org.apache.hadoop.hbase.regionserver.RegionSplitPolicy {\n  public org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy();\n  public java.lang.String toString();\n  public boolean positiveJitterRate();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/CreateStoreFileWriterParams.class;;;public final class org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams {\n  public long maxKeyCount();\n  public org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams maxKeyCount(long);\n  public org.apache.hadoop.hbase.io.compress.Compression$Algorithm compression();\n  public org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams compression(org.apache.hadoop.hbase.io.compress.Compression$Algorithm);\n  public boolean isCompaction();\n  public org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams isCompaction(boolean);\n  public boolean includeMVCCReadpoint();\n  public org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams includeMVCCReadpoint(boolean);\n  public boolean includesTag();\n  public org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams includesTag(boolean);\n  public boolean shouldDropBehind();\n  public org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams shouldDropBehind(boolean);\n  public long totalCompactedFilesSize();\n  public org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams totalCompactedFilesSize(long);\n  public java.lang.String fileStoragePolicy();\n  public org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams fileStoragePolicy(java.lang.String);\n  public java.util.function.Consumer<org.apache.hadoop.fs.Path> writerCreationTracker();\n  public org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams writerCreationTracker(java.util.function.Consumer<org.apache.hadoop.fs.Path>);\n  public static org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams create();\n}\n;;;No. This class is not a message definition that might be put on a message queue. It is a parameter object used in the HBase region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/CustomizedScanInfoBuilder.class;;;public class org.apache.hadoop.hbase.regionserver.CustomizedScanInfoBuilder implements org.apache.hadoop.hbase.regionserver.ScanOptions {\n  public org.apache.hadoop.hbase.regionserver.CustomizedScanInfoBuilder(org.apache.hadoop.hbase.regionserver.ScanInfo);\n  public org.apache.hadoop.hbase.regionserver.CustomizedScanInfoBuilder(org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan);\n  public int getMaxVersions();\n  public void setMaxVersions(int);\n  public long getTTL();\n  public void setTTL(long);\n  public org.apache.hadoop.hbase.regionserver.ScanInfo build();\n  public java.lang.String toString();\n  public void setKeepDeletedCells(org.apache.hadoop.hbase.KeepDeletedCells);\n  public org.apache.hadoop.hbase.KeepDeletedCells getKeepDeletedCells();\n  public int getMinVersions();\n  public void setMinVersions(int);\n  public long getTimeToPurgeDeletes();\n  public void setTimeToPurgeDeletes(long);\n  public org.apache.hadoop.hbase.client.Scan getScan();\n}\n;;;No. The class provides implementation for interface ScanOptions, which might be used in the process of building a message, but it is not a message definition itself.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/DateTieredMultiFileWriter.class;;;public class org.apache.hadoop.hbase.regionserver.DateTieredMultiFileWriter extends org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter {\n  public org.apache.hadoop.hbase.regionserver.DateTieredMultiFileWriter(java.util.List<java.lang.Long>, java.util.Map<java.lang.Long, java.lang.String>, boolean);\n  public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/DateTieredStoreEngine$1.class;;;class org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine$1 {\n}\n;;;no;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/DateTieredStoreEngine$DateTieredCompactionContext.class;;;final class org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine$DateTieredCompactionContext extends org.apache.hadoop.hbase.regionserver.compactions.CompactionContext {\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> preSelect(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public boolean select(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean) throws java.io.IOException;\n  public void forceSelect(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n  public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/DateTieredStoreEngine.class;;;public class org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine extends org.apache.hadoop.hbase.regionserver.StoreEngine<org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher, org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionPolicy, org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactor, org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager> {\n  public org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine();\n  public boolean needsCompaction(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionContext createCompaction() throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner$1.class;;;class org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner$1 {\n}\n;;;No.;;;N;;;No, it is not a message or task definition. It appears to be a nested class within the HBase region server's `DefaultHeapMemoryTuner` class.;;;N
org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner$StepDirection.class;;;final class org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner$StepDirection extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner$StepDirection> {\n  public static final org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner$StepDirection INCREASE_BLOCK_CACHE_SIZE;\n  public static final org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner$StepDirection INCREASE_MEMSTORE_SIZE;\n  public static final org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner$StepDirection NEUTRAL;\n  public static org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner$StepDirection[] values();\n  public static org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner$StepDirection valueOf(java.lang.String);\n}\n;;;No. This is an enum definition that would not be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/DefaultHeapMemoryTuner.class;;;class org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner implements org.apache.hadoop.hbase.regionserver.HeapMemoryTuner {\n  public static final java.lang.String MAX_STEP_KEY;\n  public static final java.lang.String MIN_STEP_KEY;\n  public static final java.lang.String SUFFICIENT_MEMORY_LEVEL_KEY;\n  public static final java.lang.String LOOKUP_PERIODS_KEY;\n  public static final java.lang.String NUM_PERIODS_TO_IGNORE;\n  public static final float DEFAULT_MAX_STEP_VALUE;\n  public static final float DEFAULT_MIN_STEP_VALUE;\n  public static final float DEFAULT_SUFFICIENT_MEMORY_LEVEL_VALUE;\n  public static final int DEFAULT_LOOKUP_PERIODS;\n  public static final int DEFAULT_NUM_PERIODS_IGNORED;\n  public org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult tune(org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/DefaultMemStore.class;;;public class org.apache.hadoop.hbase.regionserver.DefaultMemStore extends org.apache.hadoop.hbase.regionserver.AbstractMemStore {\n  public static final long DEEP_OVERHEAD;\n  public static final long FIXED_OVERHEAD;\n  public org.apache.hadoop.hbase.regionserver.DefaultMemStore();\n  public org.apache.hadoop.hbase.regionserver.DefaultMemStore(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.regionserver.DefaultMemStore(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.regionserver.RegionServicesForStores);\n  public org.apache.hadoop.hbase.regionserver.MemStoreSnapshot snapshot();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getFlushableSize();\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(long) throws java.io.IOException;\n  public void updateLowestUnflushedSequenceIdInWAL(boolean);\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize size();\n  public long preFlushSeqIDEstimation();\n  public boolean isSloppy();\n  public static void main(java.lang.String[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/DefaultStoreEngine$1.class;;;class org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/DefaultStoreEngine$DefaultCompactionContext.class;;;class org.apache.hadoop.hbase.regionserver.DefaultStoreEngine$DefaultCompactionContext extends org.apache.hadoop.hbase.regionserver.compactions.CompactionContext {\n  public boolean select(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> preSelect(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/DefaultStoreEngine.class;;;public class org.apache.hadoop.hbase.regionserver.DefaultStoreEngine extends org.apache.hadoop.hbase.regionserver.StoreEngine<org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher, org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy, org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor, org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager> {\n  public static final java.lang.String DEFAULT_STORE_FLUSHER_CLASS_KEY;\n  public static final java.lang.String DEFAULT_COMPACTOR_CLASS_KEY;\n  public static final java.lang.String DEFAULT_COMPACTION_POLICY_CLASS_KEY;\n  public org.apache.hadoop.hbase.regionserver.DefaultStoreEngine();\n  public boolean needsCompaction(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionContext createCompaction();\n}\n;;;No, it is a definition of a class in Java programming language, but it is not a message definition and would not be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/DefaultStoreFileManager.class;;;class org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager implements org.apache.hadoop.hbase.regionserver.StoreFileManager {\n  public org.apache.hadoop.hbase.regionserver.DefaultStoreFileManager(org.apache.hadoop.hbase.CellComparator, java.util.Comparator<org.apache.hadoop.hbase.regionserver.HStoreFile>, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration);\n  public void loadFiles(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public final java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getStorefiles();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getCompactedfiles();\n  public void insertNewFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public org.apache.hbase.thirdparty.com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.HStoreFile> clearFiles();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> clearCompactedFiles();\n  public final int getStorefileCount();\n  public final int getCompactedFilesCount();\n  public void addCompactionResults(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public void removeCompactedFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public final java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile> getCandidateFilesForRowKeyBefore(org.apache.hadoop.hbase.KeyValue);\n  public java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile> updateCandidateFilesForRowKeyBefore(java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile>, org.apache.hadoop.hbase.KeyValue, org.apache.hadoop.hbase.Cell);\n  public final java.util.Optional<byte[]> getSplitPoint() throws java.io.IOException;\n  public final java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getFilesForScan(byte[], boolean, byte[], boolean);\n  public int getStoreCompactionPriority();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getUnneededFiles(long, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public double getCompactionPressure();\n  public java.util.Comparator<org.apache.hadoop.hbase.regionserver.HStoreFile> getStoreFileComparator();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a class definition for the implementation of a file manager in the HBase region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/DefaultStoreFlusher.class;;;public class org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher extends org.apache.hadoop.hbase.regionserver.StoreFlusher {\n  public org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HStore);\n  public java.util.List<org.apache.hadoop.fs.Path> flushSnapshot(org.apache.hadoop.hbase.regionserver.MemStoreSnapshot, long, org.apache.hadoop.hbase.monitoring.MonitoredTask, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n}\n;;;No. This class is an implementation of a StoreFlusher in the HBase region server and is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/DelimitedKeyPrefixRegionSplitPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitPolicy extends org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy {\n  public static final java.lang.String DELIMITER_KEY;\n  public org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitPolicy();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/DelimitedKeyPrefixRegionSplitRestriction.class;;;public class org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitRestriction extends org.apache.hadoop.hbase.regionserver.RegionSplitRestriction {\n  public static final java.lang.String DELIMITER_KEY;\n  public org.apache.hadoop.hbase.regionserver.DelimitedKeyPrefixRegionSplitRestriction();\n  public void initialize(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public byte[] getRestrictedSplitPoint(byte[]);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/DisabledRegionSplitPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.DisabledRegionSplitPolicy extends org.apache.hadoop.hbase.regionserver.RegionSplitPolicy {\n  public org.apache.hadoop.hbase.regionserver.DisabledRegionSplitPolicy();\n}\n;;;No. This class is a definition of a policy and does not contain any message or data that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/DumpRegionServerMetrics.class;;;public final class org.apache.hadoop.hbase.regionserver.DumpRegionServerMetrics {\n  public static java.lang.String dumpMetrics() throws javax.management.MalformedObjectNameException, java.io.IOException;\n  public static void main(java.lang.String[]) throws java.io.IOException, javax.management.MalformedObjectNameException;\n}\n;;;No. This class is not a message definition that might be put on a message queue. It is a Java class with static methods for dumping HBase region server metrics.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/EagerMemStoreCompactionStrategy.class;;;public class org.apache.hadoop.hbase.regionserver.EagerMemStoreCompactionStrategy extends org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy {\n  public org.apache.hadoop.hbase.regionserver.EagerMemStoreCompactionStrategy(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action getAction(org.apache.hadoop.hbase.regionserver.VersionedSegmentsList);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/FavoredNodesForRegion.class;;;public interface org.apache.hadoop.hbase.regionserver.FavoredNodesForRegion {\n  public abstract void updateRegionFavoredNodesMapping(java.lang.String, java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ServerName>);\n  public abstract java.net.InetSocketAddress[] getFavoredNodesForRegion(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/FifoRpcSchedulerFactory.class;;;public class org.apache.hadoop.hbase.regionserver.FifoRpcSchedulerFactory implements org.apache.hadoop.hbase.regionserver.RpcSchedulerFactory {\n  public org.apache.hadoop.hbase.regionserver.FifoRpcSchedulerFactory();\n  public org.apache.hadoop.hbase.ipc.RpcScheduler create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.Abortable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/FlushAllLargeStoresPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.FlushAllLargeStoresPolicy extends org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy {\n  public org.apache.hadoop.hbase.regionserver.FlushAllLargeStoresPolicy();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStore> selectStoresToFlush();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/FlushAllStoresPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.FlushAllStoresPolicy extends org.apache.hadoop.hbase.regionserver.FlushPolicy {\n  public org.apache.hadoop.hbase.regionserver.FlushAllStoresPolicy();\n  public java.lang.String toString();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStore> selectStoresToFlush();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/FlushLargeStoresPolicy.class;;;public abstract class org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy extends org.apache.hadoop.hbase.regionserver.FlushPolicy {\n  public static final java.lang.String HREGION_COLUMNFAMILY_FLUSH_SIZE_LOWER_BOUND;\n  public static final java.lang.String HREGION_COLUMNFAMILY_FLUSH_SIZE_LOWER_BOUND_MIN;\n  public static final long DEFAULT_HREGION_COLUMNFAMILY_FLUSH_SIZE_LOWER_BOUND_MIN;\n  public org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy();\n  public java.lang.String toString();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/FlushLifeCycleTracker$1.class;;;final class org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker$1 implements org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/FlushLifeCycleTracker.class;;;public interface org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker {\n  public static final org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker DUMMY;\n  public default void notExecuted(java.lang.String);\n  public default void beforeExecution();\n  public default void afterExecution();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/FlushNonSloppyStoresFirstPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.FlushNonSloppyStoresFirstPolicy extends org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy {\n  public org.apache.hadoop.hbase.regionserver.FlushNonSloppyStoresFirstPolicy();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStore> selectStoresToFlush();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/FlushPolicy.class;;;public abstract class org.apache.hadoop.hbase.regionserver.FlushPolicy extends org.apache.hadoop.conf.Configured {\n  public org.apache.hadoop.hbase.regionserver.FlushPolicy();\n  public abstract java.util.Collection<org.apache.hadoop.hbase.regionserver.HStore> selectStoresToFlush();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/FlushPolicyFactory.class;;;public class org.apache.hadoop.hbase.regionserver.FlushPolicyFactory {\n  public static final java.lang.String HBASE_FLUSH_POLICY_KEY;\n  public org.apache.hadoop.hbase.regionserver.FlushPolicyFactory();\n  public static org.apache.hadoop.hbase.regionserver.FlushPolicy create(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.FlushPolicy> getFlushPolicyClass(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/FlushRequestListener.class;;;public interface org.apache.hadoop.hbase.regionserver.FlushRequestListener {\n  public abstract void flushRequested(org.apache.hadoop.hbase.regionserver.FlushType, org.apache.hadoop.hbase.regionserver.Region);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/FlushRequester.class;;;public interface org.apache.hadoop.hbase.regionserver.FlushRequester {\n  public abstract boolean requestFlush(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker);\n  public abstract boolean requestFlush(org.apache.hadoop.hbase.regionserver.HRegion, java.util.List<byte[]>, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker);\n  public abstract boolean requestDelayedFlush(org.apache.hadoop.hbase.regionserver.HRegion, long);\n  public abstract void registerFlushRequestListener(org.apache.hadoop.hbase.regionserver.FlushRequestListener);\n  public abstract boolean unregisterFlushRequestListener(org.apache.hadoop.hbase.regionserver.FlushRequestListener);\n  public abstract void setGlobalMemStoreLimit(long);\n}\n;;;No, this is an interface for a request handler. It does not define a message.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/FlushType.class;;;final class org.apache.hadoop.hbase.regionserver.FlushType extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.FlushType> {\n  public static final org.apache.hadoop.hbase.regionserver.FlushType NORMAL;\n  public static final org.apache.hadoop.hbase.regionserver.FlushType ABOVE_ONHEAP_LOWER_MARK;\n  public static final org.apache.hadoop.hbase.regionserver.FlushType ABOVE_ONHEAP_HIGHER_MARK;\n  public static final org.apache.hadoop.hbase.regionserver.FlushType ABOVE_OFFHEAP_LOWER_MARK;\n  public static final org.apache.hadoop.hbase.regionserver.FlushType ABOVE_OFFHEAP_HIGHER_MARK;\n  public static org.apache.hadoop.hbase.regionserver.FlushType[] values();\n  public static org.apache.hadoop.hbase.regionserver.FlushType valueOf(java.lang.String);\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is an enumerated type definition with various constants representing flush types.;;;N;;;No, this is not a task definition that might be put on a task queue. It is an enumeration class that defines constants representing the different types of flushes in the HBase region server.;;;N
org/apache/hadoop/hbase/regionserver/HMobStore.class;;;public class org.apache.hadoop.hbase.regionserver.HMobStore extends org.apache.hadoop.hbase.regionserver.HStore {\n  public org.apache.hadoop.hbase.regionserver.HMobStore(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, org.apache.hadoop.conf.Configuration, boolean) throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriterInTmp(java.util.Date, long, org.apache.hadoop.hbase.io.compress.Compression$Algorithm, byte[], boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriterInTmp(java.lang.String, org.apache.hadoop.fs.Path, long, org.apache.hadoop.hbase.io.compress.Compression$Algorithm, byte[], boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriterInTmp(org.apache.hadoop.hbase.mob.MobFileName, org.apache.hadoop.fs.Path, long, org.apache.hadoop.hbase.io.compress.Compression$Algorithm, boolean) throws java.io.IOException;\n  public void commitFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.mob.MobCell resolve(org.apache.hadoop.hbase.Cell, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.mob.MobCell resolve(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.mob.MobCell resolve(org.apache.hadoop.hbase.Cell, boolean, long, boolean) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.Path> getLocations(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getPath();\n  public void updateCellsCountCompactedToMob(long);\n  public long getCellsCountCompactedToMob();\n  public void updateCellsCountCompactedFromMob(long);\n  public long getCellsCountCompactedFromMob();\n  public void updateCellsSizeCompactedToMob(long);\n  public long getCellsSizeCompactedToMob();\n  public void updateCellsSizeCompactedFromMob(long);\n  public long getCellsSizeCompactedFromMob();\n  public void updateMobFlushCount();\n  public long getMobFlushCount();\n  public void updateMobFlushedCellsCount(long);\n  public long getMobFlushedCellsCount();\n  public void updateMobFlushedCellsSize(long);\n  public long getMobFlushedCellsSize();\n  public void updateMobScanCellsCount(long);\n  public long getMobScanCellsCount();\n  public void updateMobScanCellsSize(long);\n  public long getMobScanCellsSize();\n  public byte[] getRefCellTags();\n}\n;;;Yes, the class org.apache.hadoop.hbase.regionserver.HMobStore is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegion$1.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$1 implements java.util.concurrent.Callable<org.apache.hadoop.hbase.regionserver.HStore> {\n  public org.apache.hadoop.hbase.regionserver.HStore call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition that implements the java.util.concurrent.Callable interface.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/HRegion$2.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$2 implements java.util.concurrent.Callable<org.apache.hadoop.hbase.util.Pair<byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>>> {\n  public org.apache.hadoop.hbase.util.Pair<byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>> call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/HRegion$3.class;;;final class org.apache.hadoop.hbase.regionserver.HRegion$3 implements java.util.concurrent.ThreadFactory {\n  public java.lang.Thread newThread(java.lang.Runnable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$4.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$4 extends org.apache.hadoop.hbase.regionserver.HRegion$MutationBatchOperation {\n  public org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation> lockRowsAndBuildMiniBatch(java.util.List<org.apache.hadoop.hbase.regionserver.Region$RowLock>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegion$5.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$5 implements org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hbase.thirdparty.com.google.protobuf.Message> {\n  public void run(org.apache.hbase.thirdparty.com.google.protobuf.Message);\n  public void run(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$6.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$6 {\n}\n;;;No. It is a class definition and cannot be put on a message queue directly.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be a nested class within the regionserver package of the Hadoop HBase project.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$BatchOperation$1.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation$1 implements org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation$Visitor {\n  public boolean visit(int) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$BatchOperation$Visitor.class;;;interface org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation$Visitor {\n  public abstract boolean visit(int) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$BatchOperation.class;;;abstract class org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation<T> {\n  public org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation(org.apache.hadoop.hbase.regionserver.HRegion, T[]);\n  public void visitBatchOperations(boolean, int, org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation$Visitor) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.Mutation getMutation(int);\n  public abstract long getNonceGroup(int);\n  public abstract long getNonce(int);\n  public abstract org.apache.hadoop.hbase.client.Mutation[] getMutationsForCoprocs();\n  public abstract boolean isInReplay();\n  public abstract long getOrigLogSeqNum();\n  public abstract void startRegionOperation() throws java.io.IOException;\n  public abstract void closeRegionOperation() throws java.io.IOException;\n  public abstract void checkAndPrepare() throws java.io.IOException;\n  public abstract void prepareMiniBatchOperations(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, long, java.util.List<org.apache.hadoop.hbase.regionserver.Region$RowLock>) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry writeMiniBatchOperationsToMemStore(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry, long) throws java.io.IOException;\n  public boolean isDone();\n  public int size();\n  public boolean isOperationPending(int);\n  public java.util.List<java.util.UUID> getClusterIds();\n  public org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation> lockRowsAndBuildMiniBatch(java.util.List<org.apache.hadoop.hbase.regionserver.Region$RowLock>) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.util.NonceKey, org.apache.hadoop.hbase.wal.WALEdit>> buildWALEdits(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public void completeMiniBatchOperations(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry) throws java.io.IOException;\n  public void doPostOpCleanupForMiniBatch(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.wal.WALEdit, boolean) throws java.io.IOException;\n}\n;;;Yes, the class might be put on a message queue as it defines a set of operations that can be performed on a batch of operations.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegion$BulkLoadListener.class;;;public interface org.apache.hadoop.hbase.regionserver.HRegion$BulkLoadListener {\n  public abstract java.lang.String prepareBulkLoad(byte[], java.lang.String, boolean, java.lang.String) throws java.io.IOException;\n  public abstract void doneBulkLoad(byte[], java.lang.String) throws java.io.IOException;\n  public abstract void failedBulkLoad(byte[], java.lang.String) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$FlushResult$Result.class;;;public final class org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result> {\n  public static final org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result FLUSHED_NO_COMPACTION_NEEDED;\n  public static final org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result FLUSHED_COMPACTION_NEEDED;\n  public static final org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result CANNOT_FLUSH_MEMSTORE_EMPTY;\n  public static final org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result CANNOT_FLUSH;\n  public static org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result[] values();\n  public static org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$FlushResult.class;;;public interface org.apache.hadoop.hbase.regionserver.HRegion$FlushResult {\n  public abstract org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result getResult();\n  public abstract boolean isFlushSucceeded();\n  public abstract boolean isCompactionNeeded();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegion$FlushResultImpl.class;;;public class org.apache.hadoop.hbase.regionserver.HRegion$FlushResultImpl implements org.apache.hadoop.hbase.regionserver.HRegion$FlushResult {\n  public boolean isFlushSucceeded();\n  public boolean isCompactionNeeded();\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.regionserver.HRegion$FlushResult$Result getResult();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegion$MutationBatchOperation$1.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$MutationBatchOperation$1 implements org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation$Visitor {\n  public boolean visit(int) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$MutationBatchOperation.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$MutationBatchOperation extends org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation<org.apache.hadoop.hbase.client.Mutation> {\n  public org.apache.hadoop.hbase.regionserver.HRegion$MutationBatchOperation(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.client.Mutation[], boolean, long, long);\n  public org.apache.hadoop.hbase.client.Mutation getMutation(int);\n  public long getNonceGroup(int);\n  public long getNonce(int);\n  public org.apache.hadoop.hbase.client.Mutation[] getMutationsForCoprocs();\n  public boolean isInReplay();\n  public long getOrigLogSeqNum();\n  public void startRegionOperation() throws java.io.IOException;\n  public void closeRegionOperation() throws java.io.IOException;\n  public void checkAndPreparePut(org.apache.hadoop.hbase.client.Put) throws java.io.IOException;\n  public void checkAndPrepare() throws java.io.IOException;\n  public void prepareMiniBatchOperations(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, long, java.util.List<org.apache.hadoop.hbase.regionserver.Region$RowLock>) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.util.NonceKey, org.apache.hadoop.hbase.wal.WALEdit>> buildWALEdits(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry writeMiniBatchOperationsToMemStore(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry, long) throws java.io.IOException;\n  public void completeMiniBatchOperations(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry) throws java.io.IOException;\n  public void doPostOpCleanupForMiniBatch(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.wal.WALEdit, boolean) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegion$ObservedExceptionsInBatch.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$ObservedExceptionsInBatch {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$PrepareFlushResult.class;;;public class org.apache.hadoop.hbase.regionserver.HRegion$PrepareFlushResult {\n  public org.apache.hadoop.hbase.regionserver.HRegion$FlushResult getResult();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegion$ReplayBatchOperation.class;;;final class org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatchOperation extends org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation<org.apache.hadoop.hbase.wal.WALSplitUtil$MutationReplay> {\n  public org.apache.hadoop.hbase.regionserver.HRegion$ReplayBatchOperation(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.wal.WALSplitUtil$MutationReplay[], long);\n  public org.apache.hadoop.hbase.client.Mutation getMutation(int);\n  public long getNonceGroup(int);\n  public long getNonce(int);\n  public org.apache.hadoop.hbase.client.Mutation[] getMutationsForCoprocs();\n  public boolean isInReplay();\n  public long getOrigLogSeqNum();\n  public void startRegionOperation() throws java.io.IOException;\n  public void closeRegionOperation() throws java.io.IOException;\n  public void checkAndPrepare() throws java.io.IOException;\n  public void prepareMiniBatchOperations(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, long, java.util.List<org.apache.hadoop.hbase.regionserver.Region$RowLock>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry writeMiniBatchOperationsToMemStore(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry, long) throws java.io.IOException;\n  public void completeMiniBatchOperations(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry) throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegion$RowLockContext.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$RowLockContext {\n  public void setThreadName(java.lang.String);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$RowLockImpl.class;;;public class org.apache.hadoop.hbase.regionserver.HRegion$RowLockImpl implements org.apache.hadoop.hbase.regionserver.Region$RowLock {\n  public org.apache.hadoop.hbase.regionserver.HRegion$RowLockImpl(org.apache.hadoop.hbase.regionserver.HRegion$RowLockContext, java.util.concurrent.locks.Lock);\n  public java.util.concurrent.locks.Lock getLock();\n  public org.apache.hadoop.hbase.regionserver.HRegion$RowLockContext getContext();\n  public void release();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegion$WriteState.class;;;class org.apache.hadoop.hbase.regionserver.HRegion$WriteState {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegion.class;;;public class org.apache.hadoop.hbase.regionserver.HRegion implements org.apache.hadoop.hbase.io.HeapSize,org.apache.hadoop.hbase.conf.PropagatingConfigurationObserver,org.apache.hadoop.hbase.regionserver.Region {\n  public static final java.lang.String LOAD_CFS_ON_DEMAND_CONFIG_KEY;\n  public static final java.lang.String HBASE_MAX_CELL_SIZE_KEY;\n  public static final int DEFAULT_MAX_CELL_SIZE;\n  public static final java.lang.String HBASE_REGIONSERVER_MINIBATCH_SIZE;\n  public static final int DEFAULT_HBASE_REGIONSERVER_MINIBATCH_SIZE;\n  public static final java.lang.String WAL_HSYNC_CONF_KEY;\n  public static final boolean DEFAULT_WAL_HSYNC;\n  public static final java.lang.String COMPACTION_AFTER_BULKLOAD_ENABLE;\n  public static final java.lang.String SPLIT_IGNORE_BLOCKING_ENABLED_KEY;\n  public static final java.lang.String SPECIAL_RECOVERED_EDITS_DIR;\n  public static final java.lang.String USE_META_CELL_COMPARATOR;\n  public static final boolean DEFAULT_USE_META_CELL_COMPARATOR;\n  public static final java.lang.String FAIR_REENTRANT_CLOSE_LOCK;\n  public static final boolean DEFAULT_FAIR_REENTRANT_CLOSE_LOCK;\n  public static final java.lang.String MEMSTORE_PERIODIC_FLUSH_INTERVAL;\n  public static final int DEFAULT_CACHE_FLUSH_INTERVAL;\n  public static final int SYSTEM_CACHE_FLUSH_INTERVAL;\n  public static final java.lang.String MEMSTORE_FLUSH_PER_CHANGES;\n  public static final long DEFAULT_FLUSH_PER_CHANGES;\n  public static final long MAX_FLUSH_PER_CHANGES;\n  public static final java.lang.String CLOSE_WAIT_ABORT;\n  public static final boolean DEFAULT_CLOSE_WAIT_ABORT;\n  public static final java.lang.String CLOSE_WAIT_TIME;\n  public static final long DEFAULT_CLOSE_WAIT_TIME;\n  public static final java.lang.String CLOSE_WAIT_INTERVAL;\n  public static final long DEFAULT_CLOSE_WAIT_INTERVAL;\n  public static final long FIXED_OVERHEAD;\n  public static final long DEEP_OVERHEAD;\n  public void setRestoredRegion(boolean);\n  public long getSmallestReadPoint();\n  public org.apache.hadoop.hbase.regionserver.HRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public org.apache.hadoop.hbase.regionserver.HRegion(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public long initialize() throws java.io.IOException;\n  public boolean hasReferences();\n  public void blockUpdates();\n  public void unblockUpdates();\n  public org.apache.hadoop.hbase.HDFSBlocksDistribution getHDFSBlocksDistribution();\n  public static org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public long getReadRequestsCount();\n  public long getCpRequestsCount();\n  public long getFilteredReadRequestsCount();\n  public long getWriteRequestsCount();\n  public long getMemStoreDataSize();\n  public long getMemStoreHeapSize();\n  public long getMemStoreOffHeapSize();\n  public org.apache.hadoop.hbase.regionserver.RegionServicesForStores getRegionServicesForStores();\n  public long getNumMutationsWithoutWAL();\n  public long getDataInMemoryWithoutWAL();\n  public long getBlockedRequestsCount();\n  public long getCheckAndMutateChecksPassed();\n  public long getCheckAndMutateChecksFailed();\n  public org.apache.hadoop.hbase.regionserver.MetricsRegion getMetrics();\n  public boolean isClosed();\n  public boolean isClosing();\n  public boolean isReadOnly();\n  public boolean isAvailable();\n  public boolean isSplittable();\n  public boolean isMergeable();\n  public boolean areWritesEnabled();\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl getMVCC();\n  public long getMaxFlushedSeqId();\n  public long getReadPoint(org.apache.hadoop.hbase.client.IsolationLevel);\n  public boolean isLoadingCfsOnDemandDefault();\n  public java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>> close() throws java.io.IOException;\n  public java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>> close(boolean) throws java.io.IOException;\n  public java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>> close(boolean, boolean) throws java.io.IOException;\n  public void setClosing(boolean);\n  public void setTimeoutForWriteLock(long);\n  public void waitForFlushesAndCompactions();\n  public void waitForFlushes();\n  public boolean waitForFlushes(long);\n  public org.apache.hadoop.conf.Configuration getReadOnlyConfiguration();\n  public org.apache.hadoop.hbase.client.TableDescriptor getTableDescriptor();\n  public void setTableDescriptor(org.apache.hadoop.hbase.client.TableDescriptor);\n  public org.apache.hadoop.hbase.wal.WAL getWAL();\n  public org.apache.hadoop.hbase.io.hfile.BlockCache getBlockCache();\n  public void setBlockCache(org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public org.apache.hadoop.hbase.mob.MobFileCache getMobFileCache();\n  public void setMobFileCache(org.apache.hadoop.hbase.mob.MobFileCache);\n  public org.apache.hadoop.fs.FileSystem getFilesystem();\n  public org.apache.hadoop.hbase.regionserver.HRegionFileSystem getRegionFileSystem();\n  public org.apache.hadoop.fs.Path getWALRegionDir() throws java.io.IOException;\n  public long getEarliestFlushTimeForAllStores();\n  public long getOldestHfileTs(boolean) throws java.io.IOException;\n  public void compact(boolean) throws java.io.IOException;\n  public void compactStores() throws java.io.IOException;\n  public boolean compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController) throws java.io.IOException;\n  public boolean compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HRegion$FlushResult flush(boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HRegion$FlushResultImpl flushcache(boolean, boolean, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HRegion$FlushResultImpl flushcache(java.util.List<byte[]>, boolean, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.RegionScannerImpl getScanner(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.RegionScannerImpl getScanner(org.apache.hadoop.hbase.client.Scan, java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner>) throws java.io.IOException;\n  public void delete(org.apache.hadoop.hbase.client.Delete) throws java.io.IOException;\n  public void put(org.apache.hadoop.hbase.client.Put) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.OperationStatus[] batchMutate(org.apache.hadoop.hbase.client.Mutation[], boolean, long, long) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.OperationStatus[] batchMutate(org.apache.hadoop.hbase.client.Mutation[]) throws java.io.IOException;\n  public boolean checkAndMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.io.TimeRange, org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException;\n  public boolean checkAndMutate(byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.io.TimeRange, org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException;\n  public boolean checkAndRowMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.io.TimeRange, org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException;\n  public boolean checkAndRowMutate(byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.io.TimeRange, org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.CheckAndMutateResult checkAndMutate(org.apache.hadoop.hbase.client.CheckAndMutate) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.CheckAndMutateResult checkAndMutate(org.apache.hadoop.hbase.client.CheckAndMutate, long, long) throws java.io.IOException;\n  public void addRegionToSnapshot(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare) throws java.io.IOException;\n  public void setReadsEnabled(boolean);\n  public void checkTimestamps(java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.Cell>>, long) throws org.apache.hadoop.hbase.exceptions.FailedSanityCheckException;\n  public boolean refreshStoreFiles() throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HStore getStore(byte[]);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStore> getStores();\n  public java.util.List<java.lang.String> getStoreFileList(byte[][]) throws java.lang.IllegalArgumentException;\n  public org.apache.hadoop.hbase.regionserver.Region$RowLock getRowLock(byte[]) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.Region$RowLock getRowLock(byte[], boolean) throws java.io.IOException;\n  public int getReadLockCount();\n  public java.util.concurrent.ConcurrentHashMap<org.apache.hadoop.hbase.util.HashedBytes, org.apache.hadoop.hbase.regionserver.HRegion$RowLockContext> getLockedRows();\n  public java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>> bulkLoadHFiles(java.util.Collection<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, boolean, org.apache.hadoop.hbase.regionserver.HRegion$BulkLoadListener) throws java.io.IOException;\n  public java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>> bulkLoadHFiles(java.util.Collection<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, boolean, org.apache.hadoop.hbase.regionserver.HRegion$BulkLoadListener, boolean, java.util.List<java.lang.String>, boolean) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n  public static org.apache.hadoop.hbase.regionserver.HRegion newHRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public static org.apache.hadoop.hbase.regionserver.HRegion createHRegion(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL, boolean) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion createHRegion(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL, boolean, org.apache.hadoop.hbase.regionserver.RegionServerServices) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion createHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegionFileSystem createRegionDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion createHRegion(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion openHRegionFromTableDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException;\n  public java.util.NavigableMap<byte[], java.lang.Integer> getReplicationScope();\n  public static org.apache.hadoop.hbase.regionserver.HRegion openHRegion(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.Region openHRegion(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion openReadOnlyFileSystemHRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public static void warmupHRegion(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getRegionDir(org.apache.hadoop.fs.Path, java.lang.String);\n  public static boolean rowIsInRange(org.apache.hadoop.hbase.client.RegionInfo, byte[]);\n  public static boolean rowIsInRange(org.apache.hadoop.hbase.client.RegionInfo, byte[], int, short);\n  public org.apache.hadoop.hbase.client.Result get(org.apache.hadoop.hbase.client.Get) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.Cell> get(org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result mutateRow(org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result mutateRow(org.apache.hadoop.hbase.client.RowMutations, long, long) throws java.io.IOException;\n  public void mutateRowsWithLocks(java.util.Collection<org.apache.hadoop.hbase.client.Mutation>, java.util.Collection<byte[]>, long, long) throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$RegionLoadStats getLoadStatistics();\n  public org.apache.hadoop.hbase.client.Result append(org.apache.hadoop.hbase.client.Append) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result append(org.apache.hadoop.hbase.client.Append, long, long) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result increment(org.apache.hadoop.hbase.client.Increment) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result increment(org.apache.hadoop.hbase.client.Increment, long, long) throws java.io.IOException;\n  public long heapSize();\n  public boolean registerService(org.apache.hbase.thirdparty.com.google.protobuf.Service);\n  public org.apache.hbase.thirdparty.com.google.protobuf.Message execService(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CoprocessorServiceCall) throws java.io.IOException;\n  public java.util.Optional<byte[]> checkSplit();\n  public java.util.Optional<byte[]> checkSplit(boolean);\n  public int getCompactPriority();\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost getCoprocessorHost();\n  public void setCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost);\n  public void startRegionOperation() throws java.io.IOException;\n  public void startRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException;\n  public void closeRegionOperation() throws java.io.IOException;\n  public void closeRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException;\n  public long getOpenSeqNum();\n  public java.util.Map<byte[], java.lang.Long> getMaxStoreSeqId();\n  public long getOldestSeqIdOfStore(byte[]);\n  public org.apache.hadoop.hbase.client.CompactionState getCompactionState();\n  public void reportCompactionRequestStart(boolean);\n  public void reportCompactionRequestEnd(boolean, int, long);\n  public void reportCompactionRequestFailure();\n  public void incrementCompactionsQueuedCount();\n  public void decrementCompactionsQueuedCount();\n  public void incrementFlushesQueuedCount();\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public void registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public void deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public org.apache.hadoop.hbase.CellComparator getCellComparator();\n  public long getMemStoreFlushSize();\n  public void requestCompaction(java.lang.String, int, boolean, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker) throws java.io.IOException;\n  public void requestCompaction(byte[], java.lang.String, int, boolean, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker) throws java.io.IOException;\n  public void requestFlush(org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public java.util.Optional<org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationSink> getRegionReplicationSink();\n  public void addReadRequestsCount(long);\n  public void addWriteRequestsCount(long);\n  public org.apache.hadoop.hbase.regionserver.RegionScanner getScanner(org.apache.hadoop.hbase.client.Scan, java.util.List) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.RegionScanner getScanner(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.Store getStore(byte[]);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegionFileSystem.class;;;public class org.apache.hadoop.hbase.regionserver.HRegionFileSystem {\n  public static final java.lang.String REGION_INFO_FILE;\n  public static final java.lang.String REGION_MERGES_DIR;\n  public static final java.lang.String REGION_SPLITS_DIR;\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfoForFS();\n  public org.apache.hadoop.fs.Path getTableDir();\n  public org.apache.hadoop.fs.Path getRegionDir();\n  public org.apache.hadoop.fs.Path getTempDir();\n  public org.apache.hadoop.fs.Path getStoreDir(java.lang.String);\n  public static org.apache.hadoop.fs.Path getStoreHomedir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, byte[]);\n  public static org.apache.hadoop.fs.Path getStoreHomedir(org.apache.hadoop.fs.Path, java.lang.String, byte[]);\n  public void setStoragePolicy(java.lang.String, java.lang.String);\n  public java.lang.String getStoragePolicyName(java.lang.String);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileInfo> getStoreFiles(java.lang.String) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileInfo> getStoreFiles(java.lang.String, boolean) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.fs.LocatedFileStatus> getStoreFilesLocatedStatus(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, java.lang.String, boolean) throws java.io.IOException;\n  public boolean hasReferences(java.lang.String) throws java.io.IOException;\n  public boolean hasReferences(org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public java.util.Collection<java.lang.String> getFamilies() throws java.io.IOException;\n  public void deleteFamily(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path createTempName();\n  public org.apache.hadoop.fs.Path createTempName(java.lang.String);\n  public org.apache.hadoop.fs.Path commitStoreFile(java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeStoreFile(java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void removeStoreFiles(java.lang.String, java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getSplitsDir(org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.fs.Path commitDaughterRegion(org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.fs.Path>, org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv) throws java.io.IOException;\n  public void createSplitsDir(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path splitStoreFile(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.regionserver.HStoreFile, byte[], boolean, org.apache.hadoop.hbase.regionserver.RegionSplitPolicy) throws java.io.IOException;\n  public void cleanupMergedRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path mergeStoreFile(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.regionserver.HStoreFile) throws java.io.IOException;\n  public void commitMergedRegion(java.util.List<org.apache.hadoop.fs.Path>, org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.RegionInfo loadRegionInfoFileContent(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegionFileSystem createRegionOnFileSystem(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegionFileSystem openRegionFromFileSystem(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, boolean) throws java.io.IOException;\n  public static void deleteRegionFromFileSystem(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegionServer$CompactionChecker.class;;;class org.apache.hadoop.hbase.regionserver.HRegionServer$CompactionChecker extends org.apache.hadoop.hbase.ScheduledChore {\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/HRegionServer$MovedRegionInfo.class;;;class org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionInfo {\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public long getSeqNum();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegionServer$PeriodicMemStoreFlusher.class;;;class org.apache.hadoop.hbase.regionserver.HRegionServer$PeriodicMemStoreFlusher extends org.apache.hadoop.hbase.ScheduledChore {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HRegionServer$SystemExitWhenAbortTimeout.class;;;class org.apache.hadoop.hbase.regionserver.HRegionServer$SystemExitWhenAbortTimeout extends java.util.TimerTask {\n  public org.apache.hadoop.hbase.regionserver.HRegionServer$SystemExitWhenAbortTimeout();\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/HRegionServer.class;;;public class org.apache.hadoop.hbase.regionserver.HRegionServer extends org.apache.hadoop.hbase.HBaseServerBase<org.apache.hadoop.hbase.regionserver.RSRpcServices> implements org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.LastSequenceId {\n  public static boolean TEST_SKIP_REPORTING_TRANSITION;\n  public static final java.lang.String REGIONSERVER;\n  public org.apache.hadoop.hbase.regionserver.HRegionServer(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void dumpRowLocks(java.io.PrintWriter);\n  public boolean registerService(org.apache.hbase.thirdparty.com.google.protobuf.Service);\n  public java.lang.String getClusterId();\n  public boolean isClusterUp();\n  public void run();\n  public boolean reportRegionSizesForQuotas(org.apache.hadoop.hbase.quotas.RegionSizeStore);\n  public org.apache.hadoop.hbase.regionserver.RegionServerAccounting getRegionServerAccounting();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos$RegionLoad createRegionLoad(java.lang.String) throws java.io.IOException;\n  public boolean isOnline();\n  public org.apache.hadoop.hbase.zookeeper.MasterAddressTracker getMasterAddressTracker();\n  public java.util.List<org.apache.hadoop.hbase.wal.WAL> getWALs();\n  public org.apache.hadoop.hbase.wal.WAL getWAL(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.LogRoller getWalRoller();\n  public void stop(java.lang.String);\n  public void stop(java.lang.String, boolean, org.apache.hadoop.hbase.security.User);\n  public void waitForServerOnline();\n  public void postOpenDeployTasks(org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext) throws java.io.IOException;\n  public boolean reportRegionStateTransition(org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext);\n  public org.apache.hadoop.hbase.regionserver.RSRpcServices getRSRpcServices();\n  public void abort(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.hbase.regionserver.ReplicationSourceService getReplicationSourceService();\n  public org.apache.hadoop.hbase.regionserver.ReplicationSinkService getReplicationSinkService();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds getLastSequenceId(byte[]);\n  public int getNumberOfOnlineRegions();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HRegion> getOnlineRegionsLocalContext();\n  public void addRegion(org.apache.hadoop.hbase.regionserver.HRegion);\n  public org.apache.hadoop.hbase.regionserver.FlushRequester getFlushRequester();\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequester getCompactionRequestor();\n  public org.apache.hadoop.hbase.regionserver.LeaseManager getLeaseManager();\n  public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost getRegionServerCoprocessorHost();\n  public java.util.concurrent.ConcurrentMap<byte[], java.lang.Boolean> getRegionsInTransitionInRS();\n  public org.apache.hadoop.hbase.quotas.RegionServerRpcQuotaManager getRegionServerRpcQuotaManager();\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus> getWalGroupsReplicationStatus();\n  public static void main(java.lang.String[]);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HRegion> getRegions(org.apache.hadoop.hbase.TableName);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HRegion> getRegions();\n  public java.util.Set<org.apache.hadoop.hbase.TableName> getOnlineTables();\n  public java.lang.String[] getRegionServerCoprocessors();\n  public org.apache.hadoop.hbase.regionserver.HRegion getOnlineRegion(byte[]);\n  public org.apache.hadoop.hbase.regionserver.HRegion getRegion(java.lang.String);\n  public boolean removeRegion(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.regionserver.HRegion getRegionByEncodedName(java.lang.String) throws org.apache.hadoop.hbase.NotServingRegionException;\n  public void updateRegionFavoredNodesMapping(java.lang.String, java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$ServerName>);\n  public java.net.InetSocketAddress[] getFavoredNodesForRegion(java.lang.String);\n  public org.apache.hadoop.hbase.regionserver.ServerNonceManager getNonceManager();\n  public org.apache.hadoop.hbase.regionserver.HRegionServer$MovedRegionInfo getMovedRegion(java.lang.String);\n  public int movedRegionCacheExpiredTime();\n  public org.apache.hadoop.hbase.regionserver.CompactSplit getCompactSplitThread();\n  public java.util.Optional<org.apache.hadoop.hbase.io.hfile.BlockCache> getBlockCache();\n  public java.util.Optional<org.apache.hadoop.hbase.mob.MobFileCache> getMobFileCache();\n  public double getCompactionPressure();\n  public org.apache.hadoop.hbase.regionserver.HeapMemoryManager getHeapMemoryManager();\n  public org.apache.hadoop.hbase.regionserver.MemStoreFlusher getMemStoreFlusher();\n  public boolean walRollRequestFinished();\n  public org.apache.hadoop.hbase.regionserver.throttle.ThroughputController getFlushThroughputController();\n  public double getFlushPressure();\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionServer getMetrics();\n  public org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager getSecureBulkLoadManager();\n  public org.apache.hadoop.hbase.client.locking.EntityLock regionLock(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, java.lang.String, org.apache.hadoop.hbase.Abortable);\n  public void unassign(byte[]) throws java.io.IOException;\n  public org.apache.hadoop.hbase.quotas.RegionServerSpaceQuotaManager getRegionServerSpaceQuotaManager();\n  public boolean reportFileArchivalForQuotas(org.apache.hadoop.hbase.TableName, java.util.Collection<java.util.Map$Entry<java.lang.String, java.lang.Long>>);\n  public void remoteProcedureComplete(long, java.lang.Throwable);\n  public void finishRegionProcedure(long);\n  public org.apache.hadoop.hbase.regionserver.CompactedHFilesDischarger getCompactedHFilesDischarger();\n  public long getRetryPauseTime();\n  public java.util.Optional<org.apache.hadoop.hbase.ServerName> getActiveMaster();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getBackupMasters();\n  public java.util.Iterator<org.apache.hadoop.hbase.ServerName> getBootstrapNodes();\n  public java.util.List<org.apache.hadoop.hbase.HRegionLocation> getMetaLocations();\n  public org.apache.hadoop.hbase.regionserver.BrokenStoreFileCleaner getBrokenStoreFileCleaner();\n  public org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationBufferManager getRegionReplicationBufferManager();\n  public org.apache.hadoop.hbase.regionserver.Region getRegion(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HRegionServerCommandLine.class;;;public class org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine extends org.apache.hadoop.hbase.util.ServerCommandLine {\n  public org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine(java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>);\n  public int run(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/HRegionWALFileSystem.class;;;public class org.apache.hadoop.hbase.regionserver.HRegionWALFileSystem extends org.apache.hadoop.hbase.regionserver.HRegionFileSystem {\n  public void archiveRecoveredEdits(java.lang.String, java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n}\n;;;No. This class is not a message definition. It is a Java class that defines a method for archiving recovered edits.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HStore$1.class;;;class org.apache.hadoop.hbase.regionserver.HStore$1 implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No. This is not a message definition, but a class definition that implements the Callable interface.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/regionserver/HStore$2.class;;;class org.apache.hadoop.hbase.regionserver.HStore$2 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue because it is a class definition and not a task or function definition.;;;N
org/apache/hadoop/hbase/regionserver/HStore$StoreFileWriterCreationTracker.class;;;final class org.apache.hadoop.hbase.regionserver.HStore$StoreFileWriterCreationTracker implements java.util.function.Consumer<org.apache.hadoop.fs.Path> {\n  public void accept(org.apache.hadoop.fs.Path);\n  public java.util.Set<org.apache.hadoop.fs.Path> get();\n  public void accept(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HStore$StoreFlusherImpl.class;;;final class org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl implements org.apache.hadoop.hbase.regionserver.StoreFlushContext {\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize prepare();\n  public void flushCache(org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.io.IOException;\n  public boolean commit(org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.io.IOException;\n  public long getOutputFileSize();\n  public java.util.List<org.apache.hadoop.fs.Path> getCommittedFiles();\n  public void replayFlush(java.util.List<java.lang.String>, boolean) throws java.io.IOException;\n  public void abort() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HStore.class;;;public class org.apache.hadoop.hbase.regionserver.HStore implements org.apache.hadoop.hbase.regionserver.Store,org.apache.hadoop.hbase.io.HeapSize,org.apache.hadoop.hbase.regionserver.StoreConfigInformation,org.apache.hadoop.hbase.conf.PropagatingConfigurationObserver {\n  public static final java.lang.String MEMSTORE_CLASS_NAME;\n  public static final java.lang.String COMPACTCHECKER_INTERVAL_MULTIPLIER_KEY;\n  public static final java.lang.String BLOCKING_STOREFILES_KEY;\n  public static final java.lang.String BLOCK_STORAGE_POLICY_KEY;\n  public static final java.lang.String DEFAULT_BLOCK_STORAGE_POLICY;\n  public static final int DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER;\n  public static final int DEFAULT_BLOCKING_STOREFILE_COUNT;\n  public static final long FIXED_OVERHEAD;\n  public static final long DEEP_OVERHEAD;\n  public static long determineTTLFromFamily(org.apache.hadoop.hbase.client.ColumnFamilyDescriptor);\n  public java.lang.String getColumnFamilyName();\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n  public org.apache.hadoop.hbase.regionserver.HRegionFileSystem getRegionFileSystem();\n  public long getStoreFileTtl();\n  public long getMemStoreFlushSize();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getFlushableSize();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getSnapshotSize();\n  public long getCompactionCheckMultiplier();\n  public long getBlockingFileCount();\n  public org.apache.hadoop.hbase.client.ColumnFamilyDescriptor getColumnFamilyDescriptor();\n  public java.util.OptionalLong getMaxSequenceId();\n  public java.util.OptionalLong getMaxMemStoreTS();\n  public org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder getDataBlockEncoder();\n  public void refreshStoreFiles() throws java.io.IOException;\n  public void refreshStoreFiles(java.util.Collection<java.lang.String>) throws java.io.IOException;\n  public void startReplayingFromWAL();\n  public void stopReplayingFromWAL();\n  public void add(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public void add(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public long timeOfOldestEdit();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getStorefiles();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getCompactedFiles();\n  public void assertBulkLoadHFileOk(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path> preBulkLoadHFile(java.lang.String, long) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path bulkLoadHFile(byte[], java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void bulkLoadHFile(org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException;\n  public org.apache.hbase.thirdparty.com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.HStoreFile> close() throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HStoreFile tryCommitRecoveredHFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(boolean, boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher, byte[], byte[], long) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher, byte[], boolean, byte[], boolean, long) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher, byte[], byte[], long, boolean) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher, byte[], boolean, byte[], boolean, long, boolean) throws java.io.IOException;\n  public void addChangedReaderObserver(org.apache.hadoop.hbase.regionserver.ChangedReadersObserver);\n  public void deleteChangedReaderObserver(org.apache.hadoop.hbase.regionserver.ChangedReadersObserver);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void replayCompactionMarker(org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$CompactionDescriptor, boolean, boolean) throws java.io.IOException;\n  public boolean hasReferences();\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress getCompactionProgress();\n  public boolean shouldPerformMajorCompaction() throws java.io.IOException;\n  public java.util.Optional<org.apache.hadoop.hbase.regionserver.compactions.CompactionContext> requestCompaction() throws java.io.IOException;\n  public java.util.Optional<org.apache.hadoop.hbase.regionserver.compactions.CompactionContext> requestCompaction(int, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void cancelRequestedCompaction(org.apache.hadoop.hbase.regionserver.compactions.CompactionContext);\n  public boolean canSplit();\n  public java.util.Optional<byte[]> getSplitPoint();\n  public long getLastCompactSize();\n  public long getSize();\n  public void triggerMajorCompaction();\n  public org.apache.hadoop.hbase.regionserver.KeyValueScanner getScanner(org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, long) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> recreateScanners(java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner>, boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher, byte[], boolean, byte[], boolean, long, boolean) throws java.io.IOException;\n  public java.lang.String toString();\n  public int getStorefilesCount();\n  public int getCompactedFilesCount();\n  public java.util.OptionalLong getMaxStoreFileAge();\n  public java.util.OptionalLong getMinStoreFileAge();\n  public java.util.OptionalDouble getAvgStoreFileAge();\n  public long getNumReferenceFiles();\n  public long getNumHFiles();\n  public long getStoreSizeUncompressed();\n  public long getStorefilesSize();\n  public long getHFilesSize();\n  public long getStorefilesRootLevelIndexSize();\n  public long getTotalStaticIndexSize();\n  public long getTotalStaticBloomSize();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getMemStoreSize();\n  public int getCompactPriority();\n  public boolean throttleCompaction(long);\n  public org.apache.hadoop.hbase.regionserver.HRegion getHRegion();\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost getCoprocessorHost();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public boolean areWritesEnabled();\n  public long getSmallestReadPoint();\n  public void upsert(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, long, org.apache.hadoop.hbase.regionserver.MemStoreSizing) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFlushContext createFlushContext(long, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker);\n  public boolean needsCompaction();\n  public org.apache.hadoop.hbase.io.hfile.CacheConfig getCacheConfig();\n  public long heapSize();\n  public org.apache.hadoop.hbase.CellComparator getComparator();\n  public org.apache.hadoop.hbase.regionserver.ScanInfo getScanInfo();\n  public boolean hasTooManyStoreFiles();\n  public long getFlushedCellsCount();\n  public long getFlushedCellsSize();\n  public long getFlushedOutputFileSize();\n  public long getCompactedCellsCount();\n  public long getCompactedCellsSize();\n  public long getMajorCompactedCellsCount();\n  public long getMajorCompactedCellsSize();\n  public void updateCompactedMetrics(boolean, org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress);\n  public org.apache.hadoop.hbase.regionserver.StoreEngine<?, ?, ?, ?> getStoreEngine();\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public void registerChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public void deregisterChildren(org.apache.hadoop.hbase.conf.ConfigurationManager);\n  public double getCompactionPressure();\n  public boolean isPrimaryReplicaStore();\n  public void preSnapshotOperation();\n  public void postSnapshotOperation();\n  public synchronized void closeAndArchiveCompactedFiles() throws java.io.IOException;\n  public java.lang.Long preFlushSeqIDEstimation();\n  public boolean isSloppyMemStore();\n  public int getCurrentParallelPutCount();\n  public int getStoreRefCount();\n  public int getMaxCompactedStoreFileRefCount();\n  public long getMemstoreOnlyRowReadsCount();\n  public long getMixedRowReadsCount();\n  public org.apache.hadoop.conf.Configuration getReadOnlyConfiguration();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HStoreFile.class;;;public class org.apache.hadoop.hbase.regionserver.HStoreFile implements org.apache.hadoop.hbase.regionserver.StoreFile {\n  public static final byte[] MAX_SEQ_ID_KEY;\n  public static final byte[] MAJOR_COMPACTION_KEY;\n  public static final byte[] EXCLUDE_FROM_MINOR_COMPACTION_KEY;\n  public static final byte[] COMPACTION_EVENT_KEY;\n  public static final byte[] BLOOM_FILTER_TYPE_KEY;\n  public static final byte[] BLOOM_FILTER_PARAM_KEY;\n  public static final byte[] DELETE_FAMILY_COUNT;\n  public static final byte[] LAST_BLOOM_KEY;\n  public static final byte[] TIMERANGE_KEY;\n  public static final byte[] EARLIEST_PUT_TS;\n  public static final byte[] MOB_CELLS_COUNT;\n  public static final byte[] NULL_VALUE;\n  public static final byte[] MOB_FILE_REFS;\n  public static final byte[] BULKLOAD_TASK_KEY;\n  public static final byte[] BULKLOAD_TIME_KEY;\n  public static final byte[] SKIP_RESET_SEQ_ID;\n  public org.apache.hadoop.hbase.io.hfile.CacheConfig getCacheConf();\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> getFirstKey();\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> getLastKey();\n  public org.apache.hadoop.hbase.CellComparator getComparator();\n  public long getMaxMemStoreTS();\n  public org.apache.hadoop.hbase.regionserver.HStoreFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.regionserver.BloomType, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HStoreFile(org.apache.hadoop.hbase.regionserver.StoreFileInfo, org.apache.hadoop.hbase.regionserver.BloomType, org.apache.hadoop.hbase.io.hfile.CacheConfig);\n  public org.apache.hadoop.hbase.regionserver.StoreFileInfo getFileInfo();\n  public org.apache.hadoop.fs.Path getPath();\n  public org.apache.hadoop.fs.Path getEncodedPath();\n  public org.apache.hadoop.fs.Path getQualifiedPath();\n  public boolean isReference();\n  public boolean isHFile();\n  public boolean isMajorCompactionResult();\n  public boolean excludeFromMinorCompaction();\n  public long getMaxSequenceId();\n  public long getModificationTimestamp() throws java.io.IOException;\n  public byte[] getMetadataValue(byte[]);\n  public boolean isBulkLoadResult();\n  public boolean isCompactedAway();\n  public int getRefCount();\n  public boolean isReferencedInReads();\n  public java.util.OptionalLong getBulkLoadTimestamp();\n  public org.apache.hadoop.hbase.HDFSBlocksDistribution getHDFSBlockDistribution();\n  public void initReader() throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileScanner getPreadScanner(boolean, long, long, boolean);\n  public org.apache.hadoop.hbase.regionserver.StoreFileScanner getStreamScanner(boolean, boolean, boolean, long, long, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileReader getReader();\n  public synchronized void closeStoreFile(boolean) throws java.io.IOException;\n  public void deleteStoreFile() throws java.io.IOException;\n  public void markCompactedAway();\n  public java.lang.String toString();\n  public java.lang.String toStringDetailed();\n  public java.util.OptionalLong getMinimumTimestamp();\n  public java.util.OptionalLong getMaximumTimestamp();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HeapMemoryManager$1.class;;;class org.apache.hadoop.hbase.regionserver.HeapMemoryManager$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HeapMemoryManager$HeapMemoryTuneObserver.class;;;public interface org.apache.hadoop.hbase.regionserver.HeapMemoryManager$HeapMemoryTuneObserver {\n  public abstract void onHeapMemoryTune(long, long);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HeapMemoryManager$HeapMemoryTunerChore.class;;;class org.apache.hadoop.hbase.regionserver.HeapMemoryManager$HeapMemoryTunerChore extends org.apache.hadoop.hbase.ScheduledChore implements org.apache.hadoop.hbase.regionserver.FlushRequestListener {\n  public org.apache.hadoop.hbase.regionserver.HeapMemoryManager$HeapMemoryTunerChore(org.apache.hadoop.hbase.regionserver.HeapMemoryManager);\n  public void flushRequested(org.apache.hadoop.hbase.regionserver.FlushType, org.apache.hadoop.hbase.regionserver.Region);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/HeapMemoryManager$TunerContext.class;;;public final class org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext {\n  public org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext();\n  public long getBlockedFlushCount();\n  public void setBlockedFlushCount(long);\n  public long getUnblockedFlushCount();\n  public void setUnblockedFlushCount(long);\n  public long getEvictCount();\n  public void setEvictCount(long);\n  public float getCurMemStoreSize();\n  public void setCurMemStoreSize(float);\n  public float getCurBlockCacheSize();\n  public void setCurBlockCacheSize(float);\n  public long getCacheMissCount();\n  public void setCacheMissCount(long);\n  public float getCurBlockCacheUsed();\n  public void setCurBlockCacheUsed(float);\n  public float getCurMemStoreUsed();\n  public void setCurMemStoreUsed(float);\n  public void setOffheapMemStore(boolean);\n  public boolean isOffheapMemStore();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/HeapMemoryManager$TunerResult.class;;;public final class org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult {\n  public org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult(boolean);\n  public float getMemStoreSize();\n  public void setMemStoreSize(float);\n  public float getBlockCacheSize();\n  public void setBlockCacheSize(float);\n  public boolean needsTuning();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HeapMemoryManager.class;;;public class org.apache.hadoop.hbase.regionserver.HeapMemoryManager {\n  public static final java.lang.String BLOCK_CACHE_SIZE_MAX_RANGE_KEY;\n  public static final java.lang.String BLOCK_CACHE_SIZE_MIN_RANGE_KEY;\n  public static final java.lang.String MEMSTORE_SIZE_MAX_RANGE_KEY;\n  public static final java.lang.String MEMSTORE_SIZE_MIN_RANGE_KEY;\n  public static final java.lang.String HBASE_RS_HEAP_MEMORY_TUNER_PERIOD;\n  public static final int HBASE_RS_HEAP_MEMORY_TUNER_DEFAULT_PERIOD;\n  public static final java.lang.String HBASE_RS_HEAP_MEMORY_TUNER_CLASS;\n  public static final float HEAP_OCCUPANCY_ERROR_VALUE;\n  public void start(org.apache.hadoop.hbase.ChoreService);\n  public void stop();\n  public void registerTuneObserver(org.apache.hadoop.hbase.regionserver.HeapMemoryManager$HeapMemoryTuneObserver);\n  public float getHeapOccupancyPercent();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/HeapMemoryTuner.class;;;public interface org.apache.hadoop.hbase.regionserver.HeapMemoryTuner extends org.apache.hadoop.conf.Configurable {\n  public abstract org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerResult tune(org.apache.hadoop.hbase.regionserver.HeapMemoryManager$TunerContext);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/ImmutableMemStoreLAB.class;;;public class org.apache.hadoop.hbase.regionserver.ImmutableMemStoreLAB implements org.apache.hadoop.hbase.regionserver.MemStoreLAB {\n  public org.apache.hadoop.hbase.regionserver.ImmutableMemStoreLAB(java.util.List<org.apache.hadoop.hbase.regionserver.MemStoreLAB>);\n  public org.apache.hadoop.hbase.Cell copyCellInto(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.Cell forceCopyOfBigCellInto(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.regionserver.Chunk getNewExternalChunk(org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType);\n  public org.apache.hadoop.hbase.regionserver.Chunk getNewExternalChunk(int);\n  public void close();\n  public void incScannerCount();\n  public void decScannerCount();\n  public boolean isOnHeap();\n  public boolean isOffHeap();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/ImmutableSegment.class;;;public abstract class org.apache.hadoop.hbase.regionserver.ImmutableSegment extends org.apache.hadoop.hbase.regionserver.Segment {\n  public static final long DEEP_OVERHEAD;\n  public int getNumUniqueKeys();\n  public int getNumOfSegments();\n  public java.util.List<org.apache.hadoop.hbase.regionserver.Segment> getAllSegments();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy extends org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy {\n  public org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/InputStreamBlockDistribution.class;;;public class org.apache.hadoop.hbase.regionserver.InputStreamBlockDistribution {\n  public org.apache.hadoop.hbase.regionserver.InputStreamBlockDistribution(org.apache.hadoop.fs.FSDataInputStream, org.apache.hadoop.hbase.regionserver.StoreFileInfo);\n  public static boolean isEnabled(org.apache.hadoop.conf.Configuration);\n  public synchronized org.apache.hadoop.hbase.HDFSBlocksDistribution getHDFSBlockDistribution();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/InternalScan.class;;;public class org.apache.hadoop.hbase.regionserver.InternalScan extends org.apache.hadoop.hbase.client.Scan {\n  public org.apache.hadoop.hbase.regionserver.InternalScan(org.apache.hadoop.hbase.client.Get);\n  public org.apache.hadoop.hbase.regionserver.InternalScan(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public void checkOnlyMemStore();\n  public void checkOnlyStoreFiles();\n  public boolean isCheckOnlyMemStore();\n  public boolean isCheckOnlyStoreFiles();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/InternalScanner.class;;;public interface org.apache.hadoop.hbase.regionserver.InternalScanner extends java.io.Closeable {\n  public default boolean next(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public abstract boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException;\n  public abstract void close() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy extends org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy {\n  public static final java.lang.String PREFIX_LENGTH_KEY_DEPRECATED;\n  public static final java.lang.String PREFIX_LENGTH_KEY;\n  public org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy();\n  public java.lang.String toString();\n}\n;;;No. This class does not define any message or data structure. It is a policy class for splitting regions in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/KeyPrefixRegionSplitRestriction.class;;;public class org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitRestriction extends org.apache.hadoop.hbase.regionserver.RegionSplitRestriction {\n  public static final java.lang.String PREFIX_LENGTH_KEY;\n  public org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitRestriction();\n  public void initialize(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public byte[] getRestrictedSplitPoint(byte[]);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/KeyValueHeap$KVScannerComparator.class;;;public class org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator implements java.util.Comparator<org.apache.hadoop.hbase.regionserver.KeyValueScanner> {\n  public org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator(org.apache.hadoop.hbase.CellComparator);\n  public int compare(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.regionserver.KeyValueScanner);\n  public int compare(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.CellComparator getComparator();\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/KeyValueHeap.class;;;public class org.apache.hadoop.hbase.regionserver.KeyValueHeap extends org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner implements org.apache.hadoop.hbase.regionserver.KeyValueScanner,org.apache.hadoop.hbase.regionserver.InternalScanner {\n  public org.apache.hadoop.hbase.regionserver.KeyValueHeap(java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.CellComparator) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell peek();\n  public org.apache.hadoop.hbase.Cell next() throws java.io.IOException;\n  public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException;\n  public void close();\n  public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException;\n  public java.util.PriorityQueue<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getHeap();\n  public org.apache.hadoop.hbase.Cell getNextIndexedKey();\n  public void shipped() throws java.io.IOException;\n}\n;;;Yes, the class org.apache.hadoop.hbase.regionserver.KeyValueHeap is a message definition that could be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/KeyValueScanner.class;;;public interface org.apache.hadoop.hbase.regionserver.KeyValueScanner extends org.apache.hadoop.hbase.regionserver.Shipper,java.io.Closeable {\n  public static final org.apache.hadoop.hbase.Cell NO_NEXT_INDEXED_KEY;\n  public abstract org.apache.hadoop.hbase.Cell peek();\n  public abstract org.apache.hadoop.hbase.Cell next() throws java.io.IOException;\n  public abstract boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public abstract boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public default long getScannerOrder();\n  public abstract void close();\n  public abstract boolean shouldUseScanner(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.HStore, long);\n  public abstract boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException;\n  public abstract boolean realSeekDone();\n  public abstract void enforceSeek() throws java.io.IOException;\n  public abstract boolean isFileScanner();\n  public abstract org.apache.hadoop.fs.Path getFilePath();\n  public abstract boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public abstract boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public abstract boolean seekToLastRow() throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.Cell getNextIndexedKey();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/LastSequenceId.class;;;public interface org.apache.hadoop.hbase.regionserver.LastSequenceId {\n  public abstract org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos$RegionStoreSequenceIds getLastSequenceId(byte[]);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/LeaseListener.class;;;public interface org.apache.hadoop.hbase.regionserver.LeaseListener {\n  public abstract void leaseExpired();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/LeaseManager$Lease.class;;;class org.apache.hadoop.hbase.regionserver.LeaseManager$Lease implements java.util.concurrent.Delayed {\n  public java.lang.String getLeaseName();\n  public org.apache.hadoop.hbase.regionserver.LeaseListener getListener();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public long getDelay(java.util.concurrent.TimeUnit);\n  public int compareTo(java.util.concurrent.Delayed);\n  public void resetExpirationTime();\n  public int compareTo(java.lang.Object);\n}\n;;;No, this class is not a complete message definition as it only provides methods for managing leases in the HBase region server. It would need to be integrated into a larger message definition to be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/LeaseManager$LeaseStillHeldException.class;;;public class org.apache.hadoop.hbase.regionserver.LeaseManager$LeaseStillHeldException extends java.io.IOException {\n  public org.apache.hadoop.hbase.regionserver.LeaseManager$LeaseStillHeldException(java.lang.String);\n  public java.lang.String getName();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/LeaseManager.class;;;public class org.apache.hadoop.hbase.regionserver.LeaseManager extends java.lang.Thread {\n  public org.apache.hadoop.hbase.regionserver.LeaseManager(int);\n  public void run();\n  public void closeAfterLeasesExpire();\n  public void close();\n  public org.apache.hadoop.hbase.regionserver.LeaseManager$Lease createLease(java.lang.String, int, org.apache.hadoop.hbase.regionserver.LeaseListener) throws org.apache.hadoop.hbase.regionserver.LeaseManager$LeaseStillHeldException;\n  public void addLease(org.apache.hadoop.hbase.regionserver.LeaseManager$Lease) throws org.apache.hadoop.hbase.regionserver.LeaseManager$LeaseStillHeldException;\n  public void renewLease(java.lang.String) throws org.apache.hadoop.hbase.regionserver.LeaseException;\n  public void cancelLease(java.lang.String) throws org.apache.hadoop.hbase.regionserver.LeaseException;\n}\n;;;No. This is a class definition for a class that manages leases in the HBase region server. It is not a message definition that defines the content of a message that can be sent on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/LogRoller.class;;;public class org.apache.hadoop.hbase.regionserver.LogRoller extends org.apache.hadoop.hbase.wal.AbstractWALRoller<org.apache.hadoop.hbase.regionserver.RegionServerServices> {\n  public org.apache.hadoop.hbase.regionserver.LogRoller(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MasterFifoRpcSchedulerFactory.class;;;public class org.apache.hadoop.hbase.regionserver.MasterFifoRpcSchedulerFactory extends org.apache.hadoop.hbase.regionserver.FifoRpcSchedulerFactory {\n  public org.apache.hadoop.hbase.regionserver.MasterFifoRpcSchedulerFactory();\n  public org.apache.hadoop.hbase.ipc.RpcScheduler create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.Abortable);\n}\n;;;No. It is a factory class used to create instances of the MasterFifoRpcScheduler, which is a message scheduler rather than a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStore.class;;;public interface org.apache.hadoop.hbase.regionserver.MemStore {\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreSnapshot snapshot();\n  public abstract void clearSnapshot(long) throws org.apache.hadoop.hbase.exceptions.UnexpectedStateException;\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreSize getFlushableSize();\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreSize getSnapshotSize();\n  public abstract void add(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public abstract void add(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public abstract long timeOfOldestEdit();\n  public abstract void upsert(java.lang.Iterable<org.apache.hadoop.hbase.Cell>, long, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public abstract java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(long) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreSize size();\n  public abstract long preFlushSeqIDEstimation();\n  public abstract boolean isSloppy();\n  public default void startReplayingFromWAL();\n  public default void stopReplayingFromWAL();\n}\n;;;No. The class is an interface that defines a set of abstract methods for a MemStore object, but it does not define a specific message format that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreCompactionStrategy$Action.class;;;public final class org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action> {\n  public static final org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action NOOP;\n  public static final org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action FLATTEN;\n  public static final org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action FLATTEN_COUNT_UNIQUE_KEYS;\n  public static final org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action MERGE;\n  public static final org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action MERGE_COUNT_UNIQUE_KEYS;\n  public static final org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action COMPACT;\n  public static org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action[] values();\n  public static org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action valueOf(java.lang.String);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a Java class definition.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreCompactionStrategy.class;;;public abstract class org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy {\n  public static final java.lang.String COMPACTING_MEMSTORE_THRESHOLD_KEY;\n  public static final int COMPACTING_MEMSTORE_THRESHOLD_DEFAULT;\n  public org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public java.lang.String toString();\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action getAction(org.apache.hadoop.hbase.regionserver.VersionedSegmentsList);\n  public void updateStats(org.apache.hadoop.hbase.regionserver.Segment);\n  public void resetStats();\n}\n;;;Yes, it is a message definition as it defines methods that can be called and data that can be passed around as a message.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/MemStoreCompactor$1.class;;;class org.apache.hadoop.hbase.regionserver.MemStoreCompactor$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreCompactor.class;;;public class org.apache.hadoop.hbase.regionserver.MemStoreCompactor {\n  public static final long DEEP_OVERHEAD;\n  public org.apache.hadoop.hbase.regionserver.MemStoreCompactor(org.apache.hadoop.hbase.regionserver.CompactingMemStore, org.apache.hadoop.hbase.MemoryCompactionPolicy) throws org.apache.hadoop.hbase.exceptions.IllegalArgumentIOException;\n  public java.lang.String toString();\n  public boolean start() throws java.io.IOException;\n  public void stop();\n  public void resetStats();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreCompactorSegmentsIterator.class;;;public class org.apache.hadoop.hbase.regionserver.MemStoreCompactorSegmentsIterator extends org.apache.hadoop.hbase.regionserver.MemStoreSegmentsIterator {\n  public org.apache.hadoop.hbase.regionserver.MemStoreCompactorSegmentsIterator(java.util.List<org.apache.hadoop.hbase.regionserver.ImmutableSegment>, org.apache.hadoop.hbase.CellComparator, int, org.apache.hadoop.hbase.regionserver.HStore) throws java.io.IOException;\n  public boolean hasNext();\n  public org.apache.hadoop.hbase.Cell next();\n  public void close();\n  public void remove();\n  public java.lang.Object next();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreFlusher$1.class;;;final class org.apache.hadoop.hbase.regionserver.MemStoreFlusher$1 implements org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushQueueEntry {\n  public long getDelay(java.util.concurrent.TimeUnit);\n  public int compareTo(java.util.concurrent.Delayed);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreFlusher$2.class;;;class org.apache.hadoop.hbase.regionserver.MemStoreFlusher$2 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreFlusher$FlushHandler.class;;;class org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler extends java.lang.Thread {\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/MemStoreFlusher$FlushQueueEntry.class;;;interface org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushQueueEntry extends java.util.concurrent.Delayed {\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/MemStoreFlusher$FlushRegionEntry.class;;;class org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry implements org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushQueueEntry {\n  public boolean isMaximumWait(long);\n  public int getRequeueCount();\n  public org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker getTracker();\n  public org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushRegionEntry requeue(long);\n  public long getDelay(java.util.concurrent.TimeUnit);\n  public int compareTo(java.util.concurrent.Delayed);\n  public java.lang.String toString();\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public int compareTo(java.lang.Object);\n}\n;;;No.;;;N;;;Yes, it might be put on a task queue.;;;Y
org/apache/hadoop/hbase/regionserver/MemStoreFlusher.class;;;public class org.apache.hadoop.hbase.regionserver.MemStoreFlusher implements org.apache.hadoop.hbase.regionserver.FlushRequester {\n  public org.apache.hadoop.hbase.regionserver.MemStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public java.util.concurrent.atomic.LongAdder getUpdatesBlockedMsHighWater();\n  public boolean requestFlush(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker);\n  public boolean requestFlush(org.apache.hadoop.hbase.regionserver.HRegion, java.util.List<byte[]>, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker);\n  public boolean requestDelayedFlush(org.apache.hadoop.hbase.regionserver.HRegion, long);\n  public int getFlushQueueSize();\n  public void reclaimMemStoreMemory();\n  public java.lang.String toString();\n  public java.lang.String dumpQueue();\n  public void registerFlushRequestListener(org.apache.hadoop.hbase.regionserver.FlushRequestListener);\n  public boolean unregisterFlushRequestListener(org.apache.hadoop.hbase.regionserver.FlushRequestListener);\n  public void setGlobalMemStoreLimit(long);\n}\n;;;Yes, it is a message definition. It contains methods that can be executed on a message queue, such as requestFlush and requestDelayedFlush.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/MemStoreLAB.class;;;public interface org.apache.hadoop.hbase.regionserver.MemStoreLAB {\n  public static final java.lang.String USEMSLAB_KEY;\n  public static final boolean USEMSLAB_DEFAULT;\n  public static final java.lang.String MSLAB_CLASS_NAME;\n  public static final java.lang.String CHUNK_SIZE_KEY;\n  public static final int CHUNK_SIZE_DEFAULT;\n  public static final java.lang.String INDEX_CHUNK_SIZE_PERCENTAGE_KEY;\n  public static final float INDEX_CHUNK_SIZE_PERCENTAGE_DEFAULT;\n  public static final java.lang.String MAX_ALLOC_KEY;\n  public static final int MAX_ALLOC_DEFAULT;\n  public static final java.lang.String CHUNK_POOL_MAXSIZE_KEY;\n  public static final java.lang.String CHUNK_POOL_INITIALSIZE_KEY;\n  public static final float POOL_MAX_SIZE_DEFAULT;\n  public static final float POOL_INITIAL_SIZE_DEFAULT;\n  public abstract org.apache.hadoop.hbase.Cell copyCellInto(org.apache.hadoop.hbase.Cell);\n  public abstract org.apache.hadoop.hbase.Cell forceCopyOfBigCellInto(org.apache.hadoop.hbase.Cell);\n  public abstract void close();\n  public abstract void incScannerCount();\n  public abstract void decScannerCount();\n  public abstract org.apache.hadoop.hbase.regionserver.Chunk getNewExternalChunk(org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType);\n  public abstract org.apache.hadoop.hbase.regionserver.Chunk getNewExternalChunk(int);\n  public static org.apache.hadoop.hbase.regionserver.MemStoreLAB newInstance(org.apache.hadoop.conf.Configuration);\n  public static boolean isEnabled(org.apache.hadoop.conf.Configuration);\n  public abstract boolean isOnHeap();\n  public abstract boolean isOffHeap();\n}\n;;;No. This is an interface defining methods and constants for a memory store local allocation buffer in HBase. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreLABImpl$1.class;;;class org.apache.hadoop.hbase.regionserver.MemStoreLABImpl$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be a nested anonymous class within the MemStoreLABImpl class in the HBase region server, but it doesn't define any specific task or functionality that could be executed on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreLABImpl.class;;;public class org.apache.hadoop.hbase.regionserver.MemStoreLABImpl implements org.apache.hadoop.hbase.regionserver.MemStoreLAB {\n  public org.apache.hadoop.hbase.regionserver.MemStoreLABImpl();\n  public org.apache.hadoop.hbase.regionserver.MemStoreLABImpl(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.Cell copyCellInto(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.Cell forceCopyOfBigCellInto(org.apache.hadoop.hbase.Cell);\n  public void close();\n  public void incScannerCount();\n  public void decScannerCount();\n  public org.apache.hadoop.hbase.regionserver.Chunk getNewExternalChunk(org.apache.hadoop.hbase.regionserver.ChunkCreator$ChunkType);\n  public org.apache.hadoop.hbase.regionserver.Chunk getNewExternalChunk(int);\n  public boolean isOnHeap();\n  public boolean isOffHeap();\n}\n;;;No. This class does not define any specific message or data structure that can be sent or received on a message queue. It is a class that implements an interface and provides methods for memory management in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreMergerSegmentsIterator.class;;;public class org.apache.hadoop.hbase.regionserver.MemStoreMergerSegmentsIterator extends org.apache.hadoop.hbase.regionserver.MemStoreSegmentsIterator {\n  public org.apache.hadoop.hbase.regionserver.MemStoreMergerSegmentsIterator(java.util.List<org.apache.hadoop.hbase.regionserver.ImmutableSegment>, org.apache.hadoop.hbase.CellComparator, int) throws java.io.IOException;\n  public boolean hasNext();\n  public org.apache.hadoop.hbase.Cell next();\n  public void close();\n  public void remove();\n  public java.lang.Object next();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreSegmentsIterator.class;;;public abstract class org.apache.hadoop.hbase.regionserver.MemStoreSegmentsIterator implements java.util.Iterator<org.apache.hadoop.hbase.Cell> {\n  public org.apache.hadoop.hbase.regionserver.MemStoreSegmentsIterator(int) throws java.io.IOException;\n  public abstract void close();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreSize.class;;;public class org.apache.hadoop.hbase.regionserver.MemStoreSize {\n  public boolean isEmpty();\n  public long getDataSize();\n  public long getHeapSize();\n  public long getOffHeapSize();\n  public int getCellsCount();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreSizing$1.class;;;final class org.apache.hadoop.hbase.regionserver.MemStoreSizing$1 implements org.apache.hadoop.hbase.regionserver.MemStoreSizing {\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getMemStoreSize();\n  public long getDataSize();\n  public long getHeapSize();\n  public long getOffHeapSize();\n  public int getCellsCount();\n  public long incMemStoreSize(long, long, long, int);\n  public boolean compareAndSetDataSize(long, long);\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class definition that implements a specific interface.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreSizing.class;;;public interface org.apache.hadoop.hbase.regionserver.MemStoreSizing {\n  public static final org.apache.hadoop.hbase.regionserver.MemStoreSizing DUD;\n  public abstract long incMemStoreSize(long, long, long, int);\n  public default long incMemStoreSize(org.apache.hadoop.hbase.regionserver.MemStoreSize);\n  public default long decMemStoreSize(long, long, long, int);\n  public default long decMemStoreSize(org.apache.hadoop.hbase.regionserver.MemStoreSize);\n  public abstract boolean compareAndSetDataSize(long, long);\n  public abstract long getDataSize();\n  public abstract long getHeapSize();\n  public abstract long getOffHeapSize();\n  public abstract int getCellsCount();\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreSize getMemStoreSize();\n}\n;;;No, it is an interface definition for a class that implements memory storage sizing in the HBase region server. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MemStoreSnapshot.class;;;public class org.apache.hadoop.hbase.regionserver.MemStoreSnapshot {\n  public org.apache.hadoop.hbase.regionserver.MemStoreSnapshot(long, org.apache.hadoop.hbase.regionserver.ImmutableSegment);\n  public long getId();\n  public int getCellsCount();\n  public long getDataSize();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getMemStoreSize();\n  public org.apache.hadoop.hbase.regionserver.TimeRangeTracker getTimeRangeTracker();\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners();\n  public boolean isTagsPresent();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MetricsHeapMemoryManager.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsHeapMemoryManager {\n  public org.apache.hadoop.hbase.regionserver.MetricsHeapMemoryManager();\n  public org.apache.hadoop.hbase.regionserver.MetricsHeapMemoryManager(org.apache.hadoop.hbase.regionserver.MetricsHeapMemoryManagerSource);\n  public org.apache.hadoop.hbase.regionserver.MetricsHeapMemoryManagerSource getMetricsSource();\n  public void updateBlockedFlushCount(long);\n  public void updateUnblockedFlushCount(long);\n  public void setCurBlockCacheSizeGauge(long);\n  public void setCurMemStoreSizeGauge(long);\n  public void updateMemStoreDeltaSizeHistogram(int);\n  public void updateBlockCacheDeltaSizeHistogram(int);\n  public void increaseTunerDoNothingCounter();\n  public void increaseAboveHeapOccupancyLowWatermarkCounter();\n}\n;;;No. This is a class that provides methods for monitoring and managing heap memory in the HBase region server. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MetricsRegion.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsRegion {\n  public org.apache.hadoop.hbase.regionserver.MetricsRegion(org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper, org.apache.hadoop.conf.Configuration);\n  public void close();\n  public void updatePut();\n  public void updateDelete();\n  public void updateGet(long);\n  public void updateScanTime(long);\n  public void updateFilteredRecords();\n  public void updateAppend();\n  public void updateIncrement();\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper getRegionWrapper();\n  public void updateReadRequestCount();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/MetricsRegionServer.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsRegionServer {\n  public static final java.lang.String RS_ENABLE_TABLE_METRICS_KEY;\n  public static final boolean RS_ENABLE_TABLE_METRICS_DEFAULT;\n  public static final java.lang.String RS_ENABLE_SERVER_QUERY_METER_METRICS_KEY;\n  public static final boolean RS_ENABLE_SERVER_QUERY_METER_METRICS_KEY_DEFAULT;\n  public static final java.lang.String RS_ENABLE_TABLE_QUERY_METER_METRICS_KEY;\n  public static final boolean RS_ENABLE_TABLE_QUERY_METER_METRICS_KEY_DEFAULT;\n  public static final java.lang.String SLOW_METRIC_TIME;\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionServer(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.MetricsTable);\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource getMetricsSource();\n  public org.apache.hadoop.hbase.regionserver.MetricsUserAggregate getMetricsUserAggregate();\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper getRegionServerWrapper();\n  public void updatePutBatch(org.apache.hadoop.hbase.TableName, long);\n  public void updatePut(org.apache.hadoop.hbase.TableName, long);\n  public void updateDelete(org.apache.hadoop.hbase.TableName, long);\n  public void updateDeleteBatch(org.apache.hadoop.hbase.TableName, long);\n  public void updateCheckAndDelete(org.apache.hadoop.hbase.TableName, long);\n  public void updateCheckAndPut(org.apache.hadoop.hbase.TableName, long);\n  public void updateCheckAndMutate(org.apache.hadoop.hbase.TableName, long);\n  public void updateGet(org.apache.hadoop.hbase.TableName, long);\n  public void updateIncrement(org.apache.hadoop.hbase.TableName, long);\n  public void updateAppend(org.apache.hadoop.hbase.TableName, long);\n  public void updateReplay(long);\n  public void updateScanSize(org.apache.hadoop.hbase.TableName, long);\n  public void updateScanTime(org.apache.hadoop.hbase.TableName, long);\n  public void updateSplitTime(long);\n  public void incrSplitRequest();\n  public void incrSplitSuccess();\n  public void updateFlush(java.lang.String, long, long, long);\n  public void updateCompaction(java.lang.String, boolean, long, int, int, long, long);\n  public void updateBulkLoad(long);\n  public void incrementNumRegionSizeReportsSent(long);\n  public void incrementRegionSizeReportingChoreTime(long);\n  public void updateReadQueryMeter(org.apache.hadoop.hbase.TableName, long);\n  public void updateWriteQueryMeter(org.apache.hadoop.hbase.TableName, long);\n  public void updateWriteQueryMeter(org.apache.hadoop.hbase.TableName);\n  public void incrScannerLeaseExpired();\n}\n;;;No. It is a class representing metrics for a HBase region server, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl$RegionServerMetricsWrapperRunnable.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl$RegionServerMetricsWrapperRunnable implements java.lang.Runnable {\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl$RegionServerMetricsWrapperRunnable(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl);\n  public synchronized void run();\n}\n;;;No.;;;N;;;Yes, it is a task definition that might be put on a task queue. The class implements the Runnable interface, which means it defines a task that can be executed asynchronously.;;;Y
org/apache/hadoop/hbase/regionserver/MetricsRegionServerWrapperImpl.class;;;class org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl implements org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper {\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl(org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public java.lang.String getClusterId();\n  public long getStartCode();\n  public java.lang.String getZookeeperQuorum();\n  public java.lang.String getCoprocessors();\n  public java.lang.String getServerName();\n  public long getNumOnlineRegions();\n  public long getTotalRequestCount();\n  public long getTotalRowActionRequestCount();\n  public int getSplitQueueSize();\n  public int getCompactionQueueSize();\n  public int getSmallCompactionQueueSize();\n  public int getLargeCompactionQueueSize();\n  public int getFlushQueueSize();\n  public long getBlockCacheCount();\n  public long getMemStoreLimit();\n  public long getOnHeapMemStoreLimit();\n  public long getOffHeapMemStoreLimit();\n  public long getBlockCacheSize();\n  public long getBlockCacheFreeSize();\n  public long getBlockCacheHitCount();\n  public long getBlockCachePrimaryHitCount();\n  public long getBlockCacheMissCount();\n  public long getBlockCachePrimaryMissCount();\n  public long getBlockCacheEvictedCount();\n  public long getBlockCachePrimaryEvictedCount();\n  public double getBlockCacheHitPercent();\n  public double getBlockCacheHitCachingPercent();\n  public long getBlockCacheFailedInsertions();\n  public long getL1CacheHitCount();\n  public long getL1CacheMissCount();\n  public double getL1CacheHitRatio();\n  public double getL1CacheMissRatio();\n  public long getL2CacheHitCount();\n  public long getL2CacheMissCount();\n  public double getL2CacheHitRatio();\n  public double getL2CacheMissRatio();\n  public void forceRecompute();\n  public long getNumStores();\n  public long getNumWALFiles();\n  public long getWALFileSize();\n  public java.util.List<java.lang.String> getWALExcludeDNs();\n  public long getNumWALSlowAppend();\n  public long getNumStoreFiles();\n  public long getMaxStoreFileAge();\n  public long getMinStoreFileAge();\n  public long getAvgStoreFileAge();\n  public long getNumReferenceFiles();\n  public long getMemStoreSize();\n  public long getOnHeapMemStoreSize();\n  public long getOffHeapMemStoreSize();\n  public long getStoreFileSize();\n  public double getStoreFileSizeGrowthRate();\n  public double getRequestsPerSecond();\n  public long getReadRequestsCount();\n  public long getCpRequestsCount();\n  public double getReadRequestsRatePerSecond();\n  public long getFilteredReadRequestsCount();\n  public long getWriteRequestsCount();\n  public double getWriteRequestsRatePerSecond();\n  public long getRpcGetRequestsCount();\n  public long getRpcScanRequestsCount();\n  public long getRpcFullScanRequestsCount();\n  public long getRpcMultiRequestsCount();\n  public long getRpcMutateRequestsCount();\n  public long getCheckAndMutateChecksFailed();\n  public long getCheckAndMutateChecksPassed();\n  public long getStoreFileIndexSize();\n  public long getTotalStaticIndexSize();\n  public long getTotalStaticBloomSize();\n  public long getNumMutationsWithoutWAL();\n  public long getDataInMemoryWithoutWAL();\n  public double getPercentFileLocal();\n  public double getPercentFileLocalSecondaryRegions();\n  public long getUpdatesBlockedTime();\n  public long getFlushedCellsCount();\n  public long getCompactedCellsCount();\n  public long getMajorCompactedCellsCount();\n  public long getFlushedCellsSize();\n  public long getCompactedCellsSize();\n  public long getMajorCompactedCellsSize();\n  public long getCellsCountCompactedFromMob();\n  public long getCellsCountCompactedToMob();\n  public long getCellsSizeCompactedFromMob();\n  public long getCellsSizeCompactedToMob();\n  public long getMobFlushCount();\n  public long getMobFlushedCellsCount();\n  public long getMobFlushedCellsSize();\n  public long getMobScanCellsCount();\n  public long getMobScanCellsSize();\n  public long getMobFileCacheAccessCount();\n  public long getMobFileCacheMissCount();\n  public long getMobFileCacheCount();\n  public long getMobFileCacheEvictedCount();\n  public double getMobFileCacheHitPercent();\n  public int getActiveScanners();\n  public long getHedgedReadOps();\n  public long getHedgedReadWins();\n  public long getHedgedReadOpsInCurThread();\n  public long getTotalBytesRead();\n  public long getLocalBytesRead();\n  public long getShortCircuitBytesRead();\n  public long getZeroCopyBytesRead();\n  public long getBlockedRequestsCount();\n  public long getAverageRegionSize();\n  public long getDataMissCount();\n  public long getLeafIndexMissCount();\n  public long getBloomChunkMissCount();\n  public long getMetaMissCount();\n  public long getRootIndexMissCount();\n  public long getIntermediateIndexMissCount();\n  public long getFileInfoMissCount();\n  public long getGeneralBloomMetaMissCount();\n  public long getDeleteFamilyBloomMissCount();\n  public long getTrailerMissCount();\n  public long getDataHitCount();\n  public long getLeafIndexHitCount();\n  public long getBloomChunkHitCount();\n  public long getMetaHitCount();\n  public long getRootIndexHitCount();\n  public long getIntermediateIndexHitCount();\n  public long getFileInfoHitCount();\n  public long getGeneralBloomMetaHitCount();\n  public long getDeleteFamilyBloomHitCount();\n  public long getTrailerHitCount();\n  public long getByteBuffAllocatorHeapAllocationBytes();\n  public long getByteBuffAllocatorPoolAllocationBytes();\n  public double getByteBuffAllocatorHeapAllocRatio();\n  public long getByteBuffAllocatorTotalBufferCount();\n  public long getByteBuffAllocatorUsedBufferCount();\n}\n;;;Yes, this class could be a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/MetricsRegionWrapperImpl$HRegionMetricsWrapperRunnable.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl$HRegionMetricsWrapperRunnable implements java.lang.Runnable {\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl$HRegionMetricsWrapperRunnable(org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl);\n  public void run();\n}\n;;;No. Even though this class implements the Runnable interface, it does not contain any information or data that would be considered a message. It only defines a way to run certain metrics for a region in HBase.;;;N;;;Yes, it is a task definition that might be put on a task queue because it implements the `java.lang.Runnable` interface, indicating that it can be executed as a background task.;;;Y
org/apache/hadoop/hbase/regionserver/MetricsRegionWrapperImpl.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl implements org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper,java.io.Closeable {\n  public static final int PERIOD;\n  public static final java.lang.String UNKNOWN;\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl(org.apache.hadoop.hbase.regionserver.HRegion);\n  public java.lang.String getTableName();\n  public java.lang.String getNamespace();\n  public java.lang.String getRegionName();\n  public long getNumStores();\n  public long getNumStoreFiles();\n  public long getMemStoreSize();\n  public long getStoreFileSize();\n  public long getStoreRefCount();\n  public long getMaxCompactedStoreFileRefCount();\n  public long getReadRequestCount();\n  public long getCpRequestCount();\n  public long getFilteredReadRequestCount();\n  public long getWriteRequestCount();\n  public long getNumFilesCompacted();\n  public long getNumBytesCompacted();\n  public long getNumCompactionsCompleted();\n  public long getLastMajorCompactionAge();\n  public long getTotalRequestCount();\n  public long getNumCompactionsFailed();\n  public long getNumCompactionsQueued();\n  public long getNumFlushesQueued();\n  public long getMaxCompactionQueueSize();\n  public long getMaxFlushQueueSize();\n  public long getMaxStoreFileAge();\n  public long getMinStoreFileAge();\n  public long getAvgStoreFileAge();\n  public long getNumReferenceFiles();\n  public int getRegionHashCode();\n  public java.util.Map<java.lang.String, java.lang.Long> getMemstoreOnlyRowReadsCount();\n  public java.util.Map<java.lang.String, java.lang.Long> getMixedRowReadsCount();\n  public void close() throws java.io.IOException;\n  public int getReplicaId();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MetricsTable.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsTable {\n  public org.apache.hadoop.hbase.regionserver.MetricsTable(org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregate);\n  public org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregate getTableWrapperAgg();\n  public org.apache.hadoop.hbase.regionserver.MetricsTableAggregateSource getTableSourceAgg();\n  public void incrSplitRequest(java.lang.String);\n  public void incrSplitSuccess(java.lang.String);\n  public void updateSplitTime(java.lang.String, long);\n  public void updateFlushTime(java.lang.String, long);\n  public void updateFlushMemstoreSize(java.lang.String, long);\n  public void updateFlushOutputSize(java.lang.String, long);\n  public void updateCompactionTime(java.lang.String, boolean, long);\n  public void updateCompactionInputFileCount(java.lang.String, boolean, long);\n  public void updateCompactionInputSize(java.lang.String, boolean, long);\n  public void updateCompactionOutputFileCount(java.lang.String, boolean, long);\n  public void updateCompactionOutputSize(java.lang.String, boolean, long);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl$1.class;;;class org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregateImpl$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl$MetricsTableValues.class;;;class org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregateImpl$MetricsTableValues {\n}\n;;;no;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl$TableMetricsWrapperRunnable.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregateImpl$TableMetricsWrapperRunnable implements java.lang.Runnable {\n  public org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregateImpl$TableMetricsWrapperRunnable(org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregateImpl);\n  public void run();\n}\n;;;Yes, it is a message definition for a Runnable object that could potentially be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/MetricsTableWrapperAggregateImpl.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregateImpl implements org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregate,java.io.Closeable {\n  public org.apache.hadoop.hbase.regionserver.MetricsTableWrapperAggregateImpl(org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public long getReadRequestCount(java.lang.String);\n  public java.util.Map<java.lang.String, java.lang.Long> getMemstoreOnlyRowReadsCount(java.lang.String);\n  public java.util.Map<java.lang.String, java.lang.Long> getMixedRowReadsCount(java.lang.String);\n  public long getCpRequestsCount(java.lang.String);\n  public long getFilteredReadRequestCount(java.lang.String);\n  public long getWriteRequestCount(java.lang.String);\n  public long getTotalRequestsCount(java.lang.String);\n  public long getMemStoreSize(java.lang.String);\n  public long getStoreFileSize(java.lang.String);\n  public long getTableSize(java.lang.String);\n  public long getNumRegions(java.lang.String);\n  public long getNumStores(java.lang.String);\n  public long getNumStoreFiles(java.lang.String);\n  public long getMaxStoreFileAge(java.lang.String);\n  public long getMinStoreFileAge(java.lang.String);\n  public long getAvgStoreFileAge(java.lang.String);\n  public long getNumReferenceFiles(java.lang.String);\n  public long getAvgRegionSize(java.lang.String);\n  public long getCpRequestCount(java.lang.String);\n  public void close() throws java.io.IOException;\n}\n;;;No. This class provides implementation details for metrics related to a specific table in HBase, but it is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/MetricsUserAggregate.class;;;public interface org.apache.hadoop.hbase.regionserver.MetricsUserAggregate {\n  public abstract org.apache.hadoop.hbase.regionserver.MetricsUserAggregateSource getSource();\n  public abstract void updatePut(long);\n  public abstract void updateDelete(long);\n  public abstract void updateGet(long);\n  public abstract void updateIncrement(long);\n  public abstract void updateAppend(long);\n  public abstract void updateReplay(long);\n  public abstract void updateScanTime(long);\n  public abstract void updateFilteredReadRequests();\n  public abstract void updateReadRequestCount();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MetricsUserAggregateFactory$1.class;;;final class org.apache.hadoop.hbase.regionserver.MetricsUserAggregateFactory$1 implements org.apache.hadoop.hbase.regionserver.MetricsUserAggregate {\n  public org.apache.hadoop.hbase.regionserver.MetricsUserAggregateSource getSource();\n  public void updatePut(long);\n  public void updateDelete(long);\n  public void updateGet(long);\n  public void updateIncrement(long);\n  public void updateAppend(long);\n  public void updateReplay(long);\n  public void updateScanTime(long);\n  public void updateFilteredReadRequests();\n  public void updateReadRequestCount();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MetricsUserAggregateFactory.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsUserAggregateFactory {\n  public static final java.lang.String METRIC_USER_ENABLED_CONF;\n  public static final boolean DEFAULT_METRIC_USER_ENABLED_CONF;\n  public static org.apache.hadoop.hbase.regionserver.MetricsUserAggregate getMetricsUserAggregate(org.apache.hadoop.conf.Configuration);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a Java class that can be used to create and manage metrics related to HBase regionservers.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MetricsUserAggregateImpl.class;;;public class org.apache.hadoop.hbase.regionserver.MetricsUserAggregateImpl implements org.apache.hadoop.hbase.regionserver.MetricsUserAggregate {\n  public org.apache.hadoop.hbase.regionserver.MetricsUserAggregateImpl(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.regionserver.MetricsUserAggregateSource getSource();\n  public void updatePut(long);\n  public void updateDelete(long);\n  public void updateGet(long);\n  public void updateIncrement(long);\n  public void updateAppend(long);\n  public void updateReplay(long);\n  public void updateScanTime(long);\n  public void updateFilteredReadRequests();\n  public void updateReadRequestCount();\n}\n;;;No. This class defines an implementation of an interface for collecting metrics in HBase region servers. It does not define a message that could be put on a message queue.;;;N;;;No, it is not a task definition, as it does not represent a specific unit of work to be executed asynchronously.;;;N
org/apache/hadoop/hbase/regionserver/MiniBatchOperationInProgress.class;;;public class org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<T> {\n  public org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress(T[], org.apache.hadoop.hbase.regionserver.OperationStatus[], org.apache.hadoop.hbase.wal.WALEdit[], int, int, int);\n  public int size();\n  public T getOperation(int);\n  public void setOperationStatus(int, org.apache.hadoop.hbase.regionserver.OperationStatus);\n  public org.apache.hadoop.hbase.regionserver.OperationStatus getOperationStatus(int);\n  public void setWalEdit(int, org.apache.hadoop.hbase.wal.WALEdit);\n  public org.apache.hadoop.hbase.wal.WALEdit getWalEdit(int);\n  public void addOperationsFromCP(int, org.apache.hadoop.hbase.client.Mutation[]);\n  public org.apache.hadoop.hbase.client.Mutation[] getOperationsFromCoprocessors(int);\n  public int getReadyToWriteCount();\n  public int getLastIndexExclusive();\n  public int getCellCount();\n  public void addCellCount(int);\n  public int getNumOfPuts();\n  public void incrementNumOfPuts();\n  public int getNumOfDeletes();\n  public void incrementNumOfDeletes();\n  public int getNumOfIncrements();\n  public void incrementNumOfIncrements();\n  public int getNumOfAppends();\n  public void incrementNumOfAppends();\n  public org.apache.hadoop.hbase.wal.WALEdit getWalEditForReplicateIfExistsSkipWAL();\n  public void setWalEditForReplicateIfExistsSkipWAL(org.apache.hadoop.hbase.wal.WALEdit);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/MobReferenceOnlyFilter.class;;;class org.apache.hadoop.hbase.regionserver.MobReferenceOnlyFilter extends org.apache.hadoop.hbase.filter.FilterBase {\n  public org.apache.hadoop.hbase.filter.Filter$ReturnCode filterCell(org.apache.hadoop.hbase.Cell);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/MobStoreScanner.class;;;public class org.apache.hadoop.hbase.regionserver.MobStoreScanner extends org.apache.hadoop.hbase.regionserver.StoreScanner {\n  public org.apache.hadoop.hbase.regionserver.MobStoreScanner(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, long) throws java.io.IOException;\n  public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException;\n  public void shipped() throws java.io.IOException;\n  public void close();\n}\n;;;No. This is a class definition for a MobStoreScanner in the HBase region server. It is not a message definition and is not related to message queues.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl$1.class;;;class org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$1 {\n}\n;;;No. This class does not contain any attributes or methods that represent a message or information exchange, therefore it is not a message definition for a message queue.;;;N;;;No, it is not a task or message definition. It appears to be a nested class within the HBase regionserver MultiVersionConcurrencyControl class, which may not be directly related to message or task processing.;;;N
org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl$WriteEntry.class;;;public final class org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry {\n  public void attachCompletionAction(java.lang.Runnable);\n  public java.util.Optional<java.lang.Runnable> getCompletionAction();\n  public long getWriteNumber();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/MultiVersionConcurrencyControl.class;;;public class org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl {\n  public static final long NONE;\n  public static final long FIXED_SIZE;\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl();\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl(java.lang.String);\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl(long);\n  public void advanceTo(long);\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry begin();\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry begin(java.lang.Runnable);\n  public void await();\n  public void completeAndWait(org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry);\n  public boolean complete(org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry);\n  public java.lang.String toString();\n  public long getReadPoint();\n  public long getWritePoint();\n}\n;;;No, this class does not define any messages to be put on a message queue. It provides methods for managing concurrency control in a Hadoop/HBase application.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/MutableOnlineRegions.class;;;public interface org.apache.hadoop.hbase.regionserver.MutableOnlineRegions extends org.apache.hadoop.hbase.regionserver.OnlineRegions {\n  public abstract void addRegion(org.apache.hadoop.hbase.regionserver.HRegion);\n  public abstract boolean removeRegion(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/MutableSegment.class;;;public class org.apache.hadoop.hbase.regionserver.MutableSegment extends org.apache.hadoop.hbase.regionserver.Segment {\n  public static final long DEEP_OVERHEAD;\n  public void add(org.apache.hadoop.hbase.Cell, boolean, org.apache.hadoop.hbase.regionserver.MemStoreSizing, boolean);\n  public void upsert(org.apache.hadoop.hbase.Cell, long, org.apache.hadoop.hbase.regionserver.MemStoreSizing, boolean);\n  public boolean setInMemoryFlushed();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/NoLimitScannerContext.class;;;public class org.apache.hadoop.hbase.regionserver.NoLimitScannerContext extends org.apache.hadoop.hbase.regionserver.ScannerContext {\n  public org.apache.hadoop.hbase.regionserver.NoLimitScannerContext();\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext getInstance();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/NoRegionSplitRestriction.class;;;public class org.apache.hadoop.hbase.regionserver.NoRegionSplitRestriction extends org.apache.hadoop.hbase.regionserver.RegionSplitRestriction {\n  public org.apache.hadoop.hbase.regionserver.NoRegionSplitRestriction();\n  public void initialize(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public byte[] getRestrictedSplitPoint(byte[]);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/NoTagByteBufferChunkKeyValue.class;;;public class org.apache.hadoop.hbase.regionserver.NoTagByteBufferChunkKeyValue extends org.apache.hadoop.hbase.NoTagsByteBufferKeyValue {\n  public org.apache.hadoop.hbase.regionserver.NoTagByteBufferChunkKeyValue(java.nio.ByteBuffer, int, int);\n  public org.apache.hadoop.hbase.regionserver.NoTagByteBufferChunkKeyValue(java.nio.ByteBuffer, int, int, long);\n  public int getChunkId();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/NonLazyKeyValueScanner.class;;;public abstract class org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner implements org.apache.hadoop.hbase.regionserver.KeyValueScanner {\n  public org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner();\n  public boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException;\n  public boolean realSeekDone();\n  public void enforceSeek() throws java.io.IOException;\n  public static boolean doRealSeek(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.Cell, boolean) throws java.io.IOException;\n  public boolean shouldUseScanner(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.HStore, long);\n  public boolean isFileScanner();\n  public org.apache.hadoop.fs.Path getFilePath();\n  public org.apache.hadoop.hbase.Cell getNextIndexedKey();\n  public void shipped() throws java.io.IOException;\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/NonReversedNonLazyKeyValueScanner.class;;;public abstract class org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner extends org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner {\n  public org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner();\n  public boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean seekToLastRow() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/NonThreadSafeMemStoreSizing.class;;;class org.apache.hadoop.hbase.regionserver.NonThreadSafeMemStoreSizing implements org.apache.hadoop.hbase.regionserver.MemStoreSizing {\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getMemStoreSize();\n  public long incMemStoreSize(long, long, long, int);\n  public boolean compareAndSetDataSize(long, long);\n  public long getDataSize();\n  public long getHeapSize();\n  public long getOffHeapSize();\n  public int getCellsCount();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No. It is not a task definition but a class definition for measuring memory usage in HBase.;;;N
org/apache/hadoop/hbase/regionserver/OffheapChunk.class;;;public class org.apache.hadoop.hbase.regionserver.OffheapChunk extends org.apache.hadoop.hbase.regionserver.Chunk {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/OnheapChunk.class;;;public class org.apache.hadoop.hbase.regionserver.OnheapChunk extends org.apache.hadoop.hbase.regionserver.Chunk {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/OnlineRegions.class;;;public interface org.apache.hadoop.hbase.regionserver.OnlineRegions {\n  public abstract org.apache.hadoop.hbase.regionserver.Region getRegion(java.lang.String);\n  public abstract java.util.List<? extends org.apache.hadoop.hbase.regionserver.Region> getRegions(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public abstract java.util.List<? extends org.apache.hadoop.hbase.regionserver.Region> getRegions();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface defining some methods.;;;N
org/apache/hadoop/hbase/regionserver/OperationStatus.class;;;public class org.apache.hadoop.hbase.regionserver.OperationStatus {\n  public static final org.apache.hadoop.hbase.regionserver.OperationStatus SUCCESS;\n  public static final org.apache.hadoop.hbase.regionserver.OperationStatus FAILURE;\n  public static final org.apache.hadoop.hbase.regionserver.OperationStatus NOT_RUN;\n  public org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode);\n  public org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode, org.apache.hadoop.hbase.client.Result);\n  public org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode, java.lang.String);\n  public org.apache.hadoop.hbase.regionserver.OperationStatus(org.apache.hadoop.hbase.HConstants$OperationStatusCode, java.lang.Exception);\n  public org.apache.hadoop.hbase.HConstants$OperationStatusCode getOperationStatusCode();\n  public org.apache.hadoop.hbase.client.Result getResult();\n  public java.lang.String getExceptionMsg();\n}\n;;;Yes, this class might be put on a message queue as a message definition.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RSAnnotationReadingPriorityFunction.class;;;class org.apache.hadoop.hbase.regionserver.RSAnnotationReadingPriorityFunction extends org.apache.hadoop.hbase.ipc.AnnotationReadingPriorityFunction<org.apache.hadoop.hbase.regionserver.RSRpcServices> {\n  public static final java.lang.String SCAN_VTIME_WEIGHT_CONF_KEY;\n  public long getDeadline(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RSRpcServices$1.class;;;final class org.apache.hadoop.hbase.regionserver.RSRpcServices$1 extends java.io.IOException {\n  public synchronized java.lang.Throwable fillInStackTrace();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RSRpcServices$2.class;;;class org.apache.hadoop.hbase.regionserver.RSRpcServices$2 {\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/RSRpcServices$RegionScannerCloseCallBack.class;;;final class org.apache.hadoop.hbase.regionserver.RSRpcServices$RegionScannerCloseCallBack implements org.apache.hadoop.hbase.ipc.RpcCallback {\n  public org.apache.hadoop.hbase.regionserver.RSRpcServices$RegionScannerCloseCallBack(org.apache.hadoop.hbase.regionserver.RegionScanner);\n  public void run() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RSRpcServices$RegionScannerHolder.class;;;final class org.apache.hadoop.hbase.regionserver.RSRpcServices$RegionScannerHolder {\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RSRpcServices$RegionScannerShippedCallBack.class;;;class org.apache.hadoop.hbase.regionserver.RSRpcServices$RegionScannerShippedCallBack implements org.apache.hadoop.hbase.ipc.RpcCallback {\n  public org.apache.hadoop.hbase.regionserver.RSRpcServices$RegionScannerShippedCallBack(org.apache.hadoop.hbase.regionserver.RSRpcServices, java.lang.String, org.apache.hadoop.hbase.regionserver.Shipper, org.apache.hadoop.hbase.regionserver.LeaseManager$Lease);\n  public void run() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RSRpcServices$RegionScannersCloseCallBack.class;;;class org.apache.hadoop.hbase.regionserver.RSRpcServices$RegionScannersCloseCallBack implements org.apache.hadoop.hbase.ipc.RpcCallback {\n  public void addScanner(org.apache.hadoop.hbase.regionserver.RegionScanner);\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RSRpcServices$ScannerListener.class;;;class org.apache.hadoop.hbase.regionserver.RSRpcServices$ScannerListener implements org.apache.hadoop.hbase.regionserver.LeaseListener {\n  public void leaseExpired();\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/regionserver/RSRpcServices.class;;;public class org.apache.hadoop.hbase.regionserver.RSRpcServices extends org.apache.hadoop.hbase.HBaseRpcServicesBase<org.apache.hadoop.hbase.regionserver.HRegionServer> implements org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ClientService$BlockingInterface, org.apache.hadoop.hbase.shaded.protobuf.generated.BootstrapNodeProtos$BootstrapNodeService$BlockingInterface {\n  public static final java.lang.String REGION_SERVER_RPC_SCHEDULER_FACTORY_CLASS;\n  public static final java.lang.String REGIONSERVER_ADMIN_SERVICE_CONFIG;\n  public static final java.lang.String REGIONSERVER_CLIENT_SERVICE_CONFIG;\n  public static final java.lang.String REGIONSERVER_CLIENT_META_SERVICE_CONFIG;\n  public org.apache.hadoop.hbase.regionserver.RSRpcServices(org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException;\n  public int getScannersCount();\n  public java.lang.String getScanDetailsWithId(long);\n  public java.lang.String getScanDetailsWithRequest(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ScanRequest);\n  public org.apache.hadoop.hbase.regionserver.HRegion getRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$RegionSpecifier) throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CloseRegionResponse closeRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CloseRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactRegionResponse compactRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactionSwitchResponse compactionSwitch(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactionSwitchRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionResponse flushRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetOnlineRegionResponse getOnlineRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetOnlineRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionInfoResponse getRegionInfo(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionInfoRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionLoadResponse getRegionLoad(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionLoadRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearCompactionQueuesResponse clearCompactionQueues(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearCompactionQueuesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetServerInfoResponse getServerInfo(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetServerInfoRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetStoreFileResponse getStoreFile(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetStoreFileRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$OpenRegionResponse openRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$OpenRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WarmupRegionResponse warmupRegion(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WarmupRegionRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse replay(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse replicateToReplica(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse replicateWALEntry(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$RollWALWriterResponse rollWALWriter(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$RollWALWriterRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$StopServerResponse stopServer(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$StopServerRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateFavoredNodesResponse updateFavoredNodes(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateFavoredNodesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$BulkLoadHFileResponse bulkLoadHFile(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$BulkLoadHFileRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$PrepareBulkLoadResponse prepareBulkLoad(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$PrepareBulkLoadRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CleanupBulkLoadResponse cleanupBulkLoad(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CleanupBulkLoadRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CoprocessorServiceResponse execService(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CoprocessorServiceRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$GetResponse get(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$GetRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MultiResponse multi(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MultiRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateResponse mutate(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutateRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ScanResponse scan(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ScanRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CoprocessorServiceResponse execRegionServerService(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CoprocessorServiceRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaSnapshotsResponse getSpaceQuotaSnapshots(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaSnapshotsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearRegionBlockCacheResponse clearRegionBlockCache(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearRegionBlockCacheRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ExecuteProceduresResponse executeProcedures(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ExecuteProceduresRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.BootstrapNodeProtos$GetAllBootstrapNodesResponse getAllBootstrapNodes(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.BootstrapNodeProtos$GetAllBootstrapNodesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n}\n;;;Yes, the class contains method definitions that could be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RSSnapshotVerifier$1.class;;;class org.apache.hadoop.hbase.regionserver.RSSnapshotVerifier$1 implements org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor {\n  public void storeFile(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RSSnapshotVerifier$SnapshotManifestCacheLoader.class;;;final class org.apache.hadoop.hbase.regionserver.RSSnapshotVerifier$SnapshotManifestCacheLoader extends org.apache.hbase.thirdparty.com.google.common.cache.CacheLoader<org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.FileSystem, java.util.Map<java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest>>> {\n  public org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.FileSystem, java.util.Map<java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest>> load(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription) throws java.lang.Exception;\n  public java.lang.Object load(java.lang.Object) throws java.lang.Exception;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a cache loader in HBase, but it is not describing a specific task to be performed.;;;N
org/apache/hadoop/hbase/regionserver/RSSnapshotVerifier.class;;;public class org.apache.hadoop.hbase.regionserver.RSSnapshotVerifier {\n  public org.apache.hadoop.hbase.regionserver.RSSnapshotVerifier(org.apache.hadoop.conf.Configuration);\n  public void verifyRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/Region$Operation.class;;;public final class org.apache.hadoop.hbase.regionserver.Region$Operation extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.Region$Operation> {\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation ANY;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation GET;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation PUT;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation DELETE;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation SCAN;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation APPEND;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation INCREMENT;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation SPLIT_REGION;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation MERGE_REGION;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation BATCH_MUTATE;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation REPLAY_BATCH_MUTATE;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation COMPACT_REGION;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation REPLAY_EVENT;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation SNAPSHOT;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation COMPACT_SWITCH;\n  public static final org.apache.hadoop.hbase.regionserver.Region$Operation CHECK_AND_MUTATE;\n  public static org.apache.hadoop.hbase.regionserver.Region$Operation[] values();\n  public static org.apache.hadoop.hbase.regionserver.Region$Operation valueOf(java.lang.String);\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a Java enum class defining the possible operations that can be performed on a region in the HBase database.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/Region$RowLock.class;;;public interface org.apache.hadoop.hbase.regionserver.Region$RowLock {\n  public abstract void release();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/Region.class;;;public interface org.apache.hadoop.hbase.regionserver.Region extends org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public abstract org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public abstract org.apache.hadoop.hbase.client.TableDescriptor getTableDescriptor();\n  public abstract boolean isAvailable();\n  public abstract boolean isClosed();\n  public abstract boolean isClosing();\n  public abstract boolean isReadOnly();\n  public abstract boolean isSplittable();\n  public abstract boolean isMergeable();\n  public abstract java.util.List<? extends org.apache.hadoop.hbase.regionserver.Store> getStores();\n  public abstract org.apache.hadoop.hbase.regionserver.Store getStore(byte[]);\n  public abstract java.util.List<java.lang.String> getStoreFileList(byte[][]);\n  public abstract boolean refreshStoreFiles() throws java.io.IOException;\n  public abstract long getMaxFlushedSeqId();\n  public abstract long getOldestHfileTs(boolean) throws java.io.IOException;\n  public abstract java.util.Map<byte[], java.lang.Long> getMaxStoreSeqId();\n  public abstract long getEarliestFlushTimeForAllStores();\n  public abstract long getReadRequestsCount();\n  public abstract long getCpRequestsCount();\n  public abstract long getFilteredReadRequestsCount();\n  public abstract long getWriteRequestsCount();\n  public abstract long getMemStoreDataSize();\n  public abstract long getMemStoreHeapSize();\n  public abstract long getMemStoreOffHeapSize();\n  public abstract long getNumMutationsWithoutWAL();\n  public abstract long getDataInMemoryWithoutWAL();\n  public abstract long getBlockedRequestsCount();\n  public abstract long getCheckAndMutateChecksPassed();\n  public abstract long getCheckAndMutateChecksFailed();\n  public abstract void startRegionOperation() throws java.io.IOException;\n  public abstract void startRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException;\n  public abstract void closeRegionOperation() throws java.io.IOException;\n  public abstract void closeRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.Region$RowLock getRowLock(byte[], boolean) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.Result append(org.apache.hadoop.hbase.client.Append) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.OperationStatus[] batchMutate(org.apache.hadoop.hbase.client.Mutation[]) throws java.io.IOException;\n  public default boolean checkAndMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException;\n  public abstract boolean checkAndMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.io.TimeRange, org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException;\n  public default boolean checkAndMutate(byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException;\n  public abstract boolean checkAndMutate(byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.io.TimeRange, org.apache.hadoop.hbase.client.Mutation) throws java.io.IOException;\n  public default boolean checkAndRowMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException;\n  public abstract boolean checkAndRowMutate(byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.io.TimeRange, org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException;\n  public default boolean checkAndRowMutate(byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException;\n  public abstract boolean checkAndRowMutate(byte[], org.apache.hadoop.hbase.filter.Filter, org.apache.hadoop.hbase.io.TimeRange, org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.CheckAndMutateResult checkAndMutate(org.apache.hadoop.hbase.client.CheckAndMutate) throws java.io.IOException;\n  public abstract void delete(org.apache.hadoop.hbase.client.Delete) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.Result get(org.apache.hadoop.hbase.client.Get) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.Cell> get(org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.RegionScanner getScanner(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.RegionScanner getScanner(org.apache.hadoop.hbase.client.Scan, java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner>) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.CellComparator getCellComparator();\n  public abstract org.apache.hadoop.hbase.client.Result increment(org.apache.hadoop.hbase.client.Increment) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.Result mutateRow(org.apache.hadoop.hbase.client.RowMutations) throws java.io.IOException;\n  public abstract void mutateRowsWithLocks(java.util.Collection<org.apache.hadoop.hbase.client.Mutation>, java.util.Collection<byte[]>, long, long) throws java.io.IOException;\n  public abstract void put(org.apache.hadoop.hbase.client.Put) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.CompactionState getCompactionState();\n  public abstract void requestCompaction(java.lang.String, int, boolean, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker) throws java.io.IOException;\n  public abstract void requestCompaction(byte[], java.lang.String, int, boolean, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker) throws java.io.IOException;\n  public abstract void requestFlush(org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public abstract boolean waitForFlushes(long);\n  public abstract org.apache.hadoop.conf.Configuration getReadOnlyConfiguration();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$1.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$1 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$10.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$10 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a Java class definition and not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$11.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$11 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.regionserver.InternalScanner> {\n  public org.apache.hadoop.hbase.regionserver.InternalScanner call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$12.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$12 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$13.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$13 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$14.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$14 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$15.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$15 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$16.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$16 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.regionserver.InternalScanner> {\n  public org.apache.hadoop.hbase.regionserver.InternalScanner call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$17.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$17 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$18.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$18 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$19.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$19 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$2.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$2 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$20.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$20 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$21.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$21 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, java.lang.Boolean> {\n  public java.lang.Boolean call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No. It is a class definition for a region coprocessor host, and does not represent a task that can be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$22.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$22 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, java.lang.Boolean> {\n  public java.lang.Boolean call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$23.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$23 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$24.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$24 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$25.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$25 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$26.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$26 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a RegionCoprocessorHost$26 object, which extends RegionCoprocessorHost$RegionObserverOperationWithoutResult class and has two methods call(RegionObserver) and call(Object), which both throw IOException. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$27.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$27 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$28.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$28 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$29.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$29 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$3.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$3 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class is not a message definition that might be put on a message queue. It is a method definition for a class and would likely be part of a larger application or system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$30.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$30 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$31.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$31 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.client.CheckAndMutateResult> {\n  public org.apache.hadoop.hbase.client.CheckAndMutateResult call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$32.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$32 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.client.CheckAndMutateResult> {\n  public org.apache.hadoop.hbase.client.CheckAndMutateResult call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$33.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$33 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.client.CheckAndMutateResult> {\n  public org.apache.hadoop.hbase.client.CheckAndMutateResult call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$34.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$34 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.client.Result> {\n  public org.apache.hadoop.hbase.client.Result call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$35.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$35 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.client.Result> {\n  public org.apache.hadoop.hbase.client.Result call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$36.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$36 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.client.Result> {\n  public org.apache.hadoop.hbase.client.Result call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue as it extends a class that is specifically designed for defining coprocessor hosts and operations in Hadoop HBase.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$37.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$37 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.client.Result> {\n  public org.apache.hadoop.hbase.client.Result call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue.;;;N;;;No, it is not a task definition, but rather a class definition for a specific type of observer operation in the HBase region server.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$38.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$38 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.client.Result> {\n  public org.apache.hadoop.hbase.client.Result call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$39.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$39 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.client.Result> {\n  public org.apache.hadoop.hbase.client.Result call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$4.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$4 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void postEnvCall();\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, this class is not a message definition that might be put on a message queue. It appears to be a class definition for a RegionObserverOperation within the HBase region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$40.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$40 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class is a Java code definition of a method that extends another class. It is not a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$41.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$41 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.regionserver.RegionScanner> {\n  public org.apache.hadoop.hbase.regionserver.RegionScanner call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. It is a class definition for a specific implementation of a method call in the HBase library, but it is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition, but a message definition.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$42.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$42 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, java.lang.Boolean> {\n  public java.lang.Boolean call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a region coprocessor host observer operation with result.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$43.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$43 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, java.lang.Boolean> {\n  public java.lang.Boolean call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$44.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$44 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, java.lang.Boolean> {\n  public java.lang.Boolean call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, the class is not a message definition that might be put on a message queue. It is a class definition for a specific purpose within the Hadoop framework.;;;N;;;It is not a task definition that might be put on a task queue.;;;?
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$45.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$45 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$46.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$46 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$47.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$47 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$48.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$48 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$49.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$49 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$5.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$5 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$50.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$50 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a region coprocessor in the Apache Hadoop ecosystem. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$51.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$51 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$52.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$52 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$53.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$53 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$54.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$54 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is a class definition for a RegionCoprocessorHost in the Hadoop framework, but it is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$55.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$55 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$56.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$56 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. The class defines a method call for a RegionObserver in HBase and is not a message definition for a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$57.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$57 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class is not a message definition that might be put on a message queue. It appears to be a class definition for a specific implementation in the Apache HBase library.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$58.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$58 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.regionserver.StoreFileReader> {\n  public org.apache.hadoop.hbase.regionserver.StoreFileReader call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No. This is not a task definition that might be put on a task queue. It is a class definition for a specific type of observer operation with a result, defined in the Hadoop HBase library.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$59.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$59 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.regionserver.StoreFileReader> {\n  public org.apache.hadoop.hbase.regionserver.StoreFileReader call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$6.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$6 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class is a Java class and does not represent a message definition that can be put on a message queue directly. It may be used as a part of a message definition or as a component of a larger system that utilizes message queues.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$60.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$60 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>> {\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This appears to be a class definition for a RegionCoprocessorHost, which inherits from a CoprocessorHost and implements the ObserverOperationWithResult interface. It contains two methods, call with a RegionObserver parameter and call with an Object parameter, both of which throw an IOException. It is not clear from this code snippet what it is used for or how it might be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue because it extends a class that is specific to message passing and does not implement any task-related interfaces or classes.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$61.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$61 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>> {\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$62.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$62 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$63.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$63 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.EndpointObserver, org.apache.hbase.thirdparty.com.google.protobuf.Message> {\n  public org.apache.hbase.thirdparty.com.google.protobuf.Message call(org.apache.hadoop.hbase.coprocessor.EndpointObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$64.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$64 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithoutResult<org.apache.hadoop.hbase.coprocessor.EndpointObserver> {\n  public void call(org.apache.hadoop.hbase.coprocessor.EndpointObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$65.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$65 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker> {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No. It is a class definition for a RegionCoprocessor, which is a component of the HBase distributed database system. It does not represent a task that can be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$66.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$66 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$BulkLoadObserverOperation {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$67.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$67 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$BulkLoadObserverOperation {\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$7.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$7 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$8.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$8 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionObserver, org.apache.hadoop.hbase.regionserver.InternalScanner> {\n  public org.apache.hadoop.hbase.regionserver.InternalScanner call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$9.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$9 extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This is not a message definition that can be put on a message queue. It is a class definition that extends another class and has two methods defined within it.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$BulkLoadObserverOperation.class;;;abstract class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$BulkLoadObserverOperation extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithoutResult<org.apache.hadoop.hbase.coprocessor.BulkLoadObserver> {\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$BulkLoadObserverOperation(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost, org.apache.hadoop.hbase.security.User);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$RegionEnvironment.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment extends org.apache.hadoop.hbase.coprocessor.BaseEnvironment<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor> implements org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment {\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment(org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.RegionServerServices, java.util.concurrent.ConcurrentMap<java.lang.String, java.lang.Object>);\n  public org.apache.hadoop.hbase.regionserver.Region getRegion();\n  public org.apache.hadoop.hbase.regionserver.OnlineRegions getOnlineRegions();\n  public org.apache.hadoop.hbase.client.Connection getConnection();\n  public org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public void shutdown();\n  public java.util.concurrent.ConcurrentMap<java.lang.String, java.lang.Object> getSharedData();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public org.apache.hadoop.hbase.metrics.MetricRegistry getMetricRegistryForRegionServer();\n  public org.apache.hadoop.hbase.RawCellBuilder getCellBuilder();\n}\n;;;No, this is not a message definition. It is a class definition that defines a specific implementation of an interface, and it cannot be put directly on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$RegionEnvironmentForCoreCoprocessors.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironmentForCoreCoprocessors extends org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment implements org.apache.hadoop.hbase.coprocessor.HasRegionServerServices {\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironmentForCoreCoprocessors(org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.regionserver.RegionServerServices, java.util.concurrent.ConcurrentMap<java.lang.String, java.lang.Object>);\n  public org.apache.hadoop.hbase.regionserver.RegionServerServices getRegionServerServices();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$RegionObserverOperationWithoutResult.class;;;abstract class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>.ObserverOperationWithoutResult<org.apache.hadoop.hbase.coprocessor.RegionObserver> {\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost);\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost, org.apache.hadoop.hbase.security.User);\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost, boolean);\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionObserverOperationWithoutResult(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost, org.apache.hadoop.hbase.security.User, boolean);\n}\n;;;No. It is a class definition for a Region Observer operation in HBase, but it is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost$TableCoprocessorAttribute.class;;;class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$TableCoprocessorAttribute {\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$TableCoprocessorAttribute(org.apache.hadoop.fs.Path, java.lang.String, int, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.fs.Path getPath();\n  public java.lang.String getClassName();\n  public int getPriority();\n  public org.apache.hadoop.conf.Configuration getConf();\n}\n;;;No.;;;N;;;No. It is not a task definition, but rather a data structure for storing information about a table coprocessor attribute in HBase.;;;N
org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.class;;;public class org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment> {\n  public boolean hasCustomPostScannerFilterRow();\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration);\n  public static void testTableCoprocessorAttrs(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionEnvironment createEnvironment(org.apache.hadoop.hbase.coprocessor.RegionCoprocessor, int, int, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.coprocessor.RegionCoprocessor checkAndGetInstance(java.lang.Class<?>) throws java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public void preOpen() throws java.io.IOException;\n  public void postOpen();\n  public void preClose(boolean) throws java.io.IOException;\n  public void postClose(boolean);\n  public boolean preCompactSelection(org.apache.hadoop.hbase.regionserver.HStore, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompactSelection(org.apache.hadoop.hbase.regionserver.HStore, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.ScanInfo preCompactScannerOpen(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.InternalScanner preCompact(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void postCompact(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.HStoreFile, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.ScanInfo preFlushScannerOpen(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.InternalScanner preFlush(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public void preFlush(org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public void postFlush(org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public void preMemStoreCompaction(org.apache.hadoop.hbase.regionserver.HStore) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.ScanInfo preMemStoreCompactionCompactScannerOpen(org.apache.hadoop.hbase.regionserver.HStore) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.InternalScanner preMemStoreCompactionCompact(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public void postMemStoreCompaction(org.apache.hadoop.hbase.regionserver.HStore) throws java.io.IOException;\n  public void postFlush(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.HStoreFile, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public boolean preGet(org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public void postGet(org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public java.lang.Boolean preExists(org.apache.hadoop.hbase.client.Get) throws java.io.IOException;\n  public boolean postExists(org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException;\n  public boolean prePut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public boolean prePrepareTimeStampForDeleteVersion(org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.client.Get) throws java.io.IOException;\n  public void postPut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public boolean preDelete(org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void postDelete(org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void preBatchMutate(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public void postBatchMutate(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public void postBatchMutateIndispensably(org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.CheckAndMutateResult preCheckAndMutate(org.apache.hadoop.hbase.client.CheckAndMutate) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.CheckAndMutateResult preCheckAndMutateAfterRowLock(org.apache.hadoop.hbase.client.CheckAndMutate) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.CheckAndMutateResult postCheckAndMutate(org.apache.hadoop.hbase.client.CheckAndMutate, org.apache.hadoop.hbase.client.CheckAndMutateResult) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result preAppend(org.apache.hadoop.hbase.client.Append, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result preAppendAfterRowLock(org.apache.hadoop.hbase.client.Append) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result preIncrement(org.apache.hadoop.hbase.client.Increment, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result preIncrementAfterRowLock(org.apache.hadoop.hbase.client.Increment) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result postAppend(org.apache.hadoop.hbase.client.Append, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result postIncrement(org.apache.hadoop.hbase.client.Increment, org.apache.hadoop.hbase.client.Result, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void preScannerOpen(org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.RegionScanner postScannerOpen(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException;\n  public java.lang.Boolean preScannerNext(org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int) throws java.io.IOException;\n  public boolean postScannerNext(org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException;\n  public boolean postScannerFilterRow(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean preScannerClose(org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public void postScannerClose(org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.ScanInfo preStoreScannerOpen(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public void preReplayWALs(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void postReplayWALs(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean preWALRestore(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void postWALRestore(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void preBulkLoadHFile(java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>) throws java.io.IOException;\n  public boolean preCommitStoreFile(byte[], java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path>>) throws java.io.IOException;\n  public void postCommitStoreFile(byte[], org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void postBulkLoadHFile(java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>>) throws java.io.IOException;\n  public void postStartRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException;\n  public void postCloseRegionOperation(org.apache.hadoop.hbase.regionserver.Region$Operation) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileReader preStoreFileReaderOpen(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileReader postStoreFileReaderOpen(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.io.FSDataInputStreamWrapper, long, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.regionserver.StoreFileReader) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> postIncrementBeforeWAL(org.apache.hadoop.hbase.client.Mutation, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> postAppendBeforeWAL(org.apache.hadoop.hbase.client.Mutation, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>) throws java.io.IOException;\n  public void preWALAppend(org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public org.apache.hbase.thirdparty.com.google.protobuf.Message preEndpointInvocation(org.apache.hbase.thirdparty.com.google.protobuf.Service, java.lang.String, org.apache.hbase.thirdparty.com.google.protobuf.Message) throws java.io.IOException;\n  public void postEndpointInvocation(org.apache.hbase.thirdparty.com.google.protobuf.Service, java.lang.String, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hbase.thirdparty.com.google.protobuf.Message$Builder) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker postInstantiateDeleteTracker(org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker) throws java.io.IOException;\n  public void prePrepareBulkLoad(org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preCleanupBulkLoad(org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Coprocessor checkAndGetInstance(java.lang.Class) throws java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public org.apache.hadoop.hbase.CoprocessorEnvironment createEnvironment(org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration);\n}\n;;;No. It is a class definition for a region server coprocessor in HBase. It does not contain any message or message-related elements.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionScanner.class;;;public interface org.apache.hadoop.hbase.regionserver.RegionScanner extends org.apache.hadoop.hbase.regionserver.InternalScanner {\n  public abstract org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public abstract boolean isFilterDone() throws java.io.IOException;\n  public abstract boolean reseek(byte[]) throws java.io.IOException;\n  public abstract long getMaxResultSize();\n  public abstract long getMvccReadPoint();\n  public abstract int getBatch();\n  public default java.lang.String getOperationId();\n  public abstract boolean nextRaw(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public abstract boolean nextRaw(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an interface definition for a region scanner in Apache HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionScannerImpl$1.class;;;final class org.apache.hadoop.hbase.regionserver.RegionScannerImpl$1 extends java.util.AbstractList<org.apache.hadoop.hbase.Cell> {\n  public void add(int, org.apache.hadoop.hbase.Cell);\n  public boolean addAll(int, java.util.Collection<? extends org.apache.hadoop.hbase.Cell>);\n  public org.apache.hadoop.hbase.KeyValue get(int);\n  public int size();\n  public void add(int, java.lang.Object);\n  public java.lang.Object get(int);\n}\n;;;No. This class appears to be an implementation of a region scanner for HBase and does not define any message formats or structures.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionScannerImpl.class;;;class org.apache.hadoop.hbase.regionserver.RegionScannerImpl implements org.apache.hadoop.hbase.regionserver.RegionScanner,org.apache.hadoop.hbase.regionserver.Shipper,org.apache.hadoop.hbase.ipc.RpcCallback {\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public long getMaxResultSize();\n  public long getMvccReadPoint();\n  public int getBatch();\n  public java.lang.String getOperationId();\n  public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public synchronized boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException;\n  public boolean nextRaw(java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public boolean nextRaw(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException;\n  public synchronized boolean isFilterDone() throws java.io.IOException;\n  public synchronized void close();\n  public synchronized boolean reseek(byte[]) throws java.io.IOException;\n  public void shipped() throws java.io.IOException;\n  public void run() throws java.io.IOException;\n}\n;;;No, this is not a message definition. It is a class definition for an implementation class in the HBase system, which defines various methods and interfaces that the class implements. It does not define any specific message formats for communication between systems or components.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerAccounting.class;;;public class org.apache.hadoop.hbase.regionserver.RegionServerAccounting {\n  public org.apache.hadoop.hbase.regionserver.RegionServerAccounting(org.apache.hadoop.conf.Configuration);\n  public long getGlobalMemStoreDataSize();\n  public long getGlobalMemStoreHeapSize();\n  public long getGlobalMemStoreOffHeapSize();\n  public void incGlobalMemStoreSize(long, long, long);\n  public void decGlobalMemStoreSize(long, long, long);\n  public org.apache.hadoop.hbase.regionserver.FlushType isAboveHighWaterMark();\n  public org.apache.hadoop.hbase.regionserver.FlushType isAboveLowWaterMark();\n  public double getFlushPressure();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$1.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$1 extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public void postEnvCall();\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$10.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$10 extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$2.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$2 extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$3.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$3 extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$4.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$4 extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$5.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$5 extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$6.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$6 extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>.ObserverOperationWithResult<org.apache.hadoop.hbase.coprocessor.RegionServerObserver, org.apache.hadoop.hbase.replication.ReplicationEndpoint> {\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public java.lang.Object call(java.lang.Object) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$7.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$7 extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No. This class defines a method (or two variants of a method) and extends another class, but it does not define any message or data structure that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$8.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$8 extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$9.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$9 extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.RegionServerObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$RegionServerEnvironment.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment extends org.apache.hadoop.hbase.coprocessor.BaseEnvironment<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor> implements org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment {\n  public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment(org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public org.apache.hadoop.hbase.regionserver.OnlineRegions getOnlineRegions();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public org.apache.hadoop.hbase.client.Connection getConnection();\n  public org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.metrics.MetricRegistry getMetricRegistryForRegionServer();\n  public void shutdown();\n}\n;;;No, this is not a message definition, but rather a class definition for a region server environment in the HBase framework.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$RegionServerEnvironmentForCoreCoprocessors.class;;;class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironmentForCoreCoprocessors extends org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment implements org.apache.hadoop.hbase.coprocessor.HasRegionServerServices {\n  public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironmentForCoreCoprocessors(org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor, int, int, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public org.apache.hadoop.hbase.regionserver.RegionServerServices getRegionServerServices();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost$RegionServerObserverOperation.class;;;abstract class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>.ObserverOperationWithoutResult<org.apache.hadoop.hbase.coprocessor.RegionServerObserver> {\n  public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation(org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost);\n  public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerObserverOperation(org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost, org.apache.hadoop.hbase.security.User);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is an abstract class that extends another abstract class and has constructors with parameters. It is used for defining operations for a region server observer in HBase.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerCoprocessorHost.class;;;public class org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor, org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment> {\n  public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost$RegionServerEnvironment createEnvironment(org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor, int, int, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor checkAndGetInstance(java.lang.Class<?>) throws java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public void preStop(java.lang.String, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void preRollWALWriterRequest() throws java.io.IOException;\n  public void postRollWALWriterRequest() throws java.io.IOException;\n  public void preReplicateLogEntries() throws java.io.IOException;\n  public void postReplicateLogEntries() throws java.io.IOException;\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint postCreateReplicationEndPoint(org.apache.hadoop.hbase.replication.ReplicationEndpoint) throws java.io.IOException;\n  public void preClearCompactionQueues() throws java.io.IOException;\n  public void postClearCompactionQueues() throws java.io.IOException;\n  public void preExecuteProcedures() throws java.io.IOException;\n  public void postExecuteProcedures() throws java.io.IOException;\n  public org.apache.hadoop.hbase.Coprocessor checkAndGetInstance(java.lang.Class) throws java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public org.apache.hadoop.hbase.CoprocessorEnvironment createEnvironment(org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration);\n}\n;;;No. This is a class definition for a RegionServerCoprocessorHost in HBase, but it is not a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerServices$PostOpenDeployContext.class;;;public class org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext {\n  public org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext(org.apache.hadoop.hbase.regionserver.HRegion, long, long);\n  public org.apache.hadoop.hbase.regionserver.HRegion getRegion();\n  public long getOpenProcId();\n  public long getMasterSystemTime();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RegionServerServices$RegionStateTransitionContext.class;;;public class org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext {\n  public org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext(org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode, long, long, org.apache.hadoop.hbase.client.RegionInfo...);\n  public org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext(org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode, long, long, long, org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos$RegionStateTransition$TransitionCode getCode();\n  public long getOpenSeqNum();\n  public long getMasterSystemTime();\n  public org.apache.hadoop.hbase.client.RegionInfo[] getHris();\n  public long[] getProcIds();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerServices.class;;;public interface org.apache.hadoop.hbase.regionserver.RegionServerServices extends org.apache.hadoop.hbase.Server,org.apache.hadoop.hbase.regionserver.MutableOnlineRegions,org.apache.hadoop.hbase.regionserver.FavoredNodesForRegion {\n  public abstract org.apache.hadoop.hbase.wal.WAL getWAL(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.wal.WAL> getWALs() throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.FlushRequester getFlushRequester();\n  public abstract org.apache.hadoop.hbase.regionserver.compactions.CompactionRequester getCompactionRequestor();\n  public abstract org.apache.hadoop.hbase.regionserver.RegionServerAccounting getRegionServerAccounting();\n  public abstract org.apache.hadoop.hbase.quotas.RegionServerRpcQuotaManager getRegionServerRpcQuotaManager();\n  public abstract org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager getSecureBulkLoadManager();\n  public abstract org.apache.hadoop.hbase.quotas.RegionServerSpaceQuotaManager getRegionServerSpaceQuotaManager();\n  public abstract void postOpenDeployTasks(org.apache.hadoop.hbase.regionserver.RegionServerServices$PostOpenDeployContext) throws java.io.IOException;\n  public abstract boolean reportRegionStateTransition(org.apache.hadoop.hbase.regionserver.RegionServerServices$RegionStateTransitionContext);\n  public abstract org.apache.hadoop.hbase.ipc.RpcServerInterface getRpcServer();\n  public abstract java.util.concurrent.ConcurrentMap<byte[], java.lang.Boolean> getRegionsInTransitionInRS();\n  public abstract org.apache.hadoop.hbase.regionserver.LeaseManager getLeaseManager();\n  public abstract org.apache.hadoop.hbase.executor.ExecutorService getExecutorService();\n  public abstract org.apache.hadoop.hbase.regionserver.ServerNonceManager getNonceManager();\n  public abstract boolean registerService(org.apache.hbase.thirdparty.com.google.protobuf.Service);\n  public abstract org.apache.hadoop.hbase.regionserver.HeapMemoryManager getHeapMemoryManager();\n  public abstract double getCompactionPressure();\n  public abstract org.apache.hadoop.hbase.regionserver.throttle.ThroughputController getFlushThroughputController();\n  public abstract double getFlushPressure();\n  public abstract org.apache.hadoop.hbase.regionserver.MetricsRegionServer getMetrics();\n  public abstract org.apache.hadoop.hbase.client.locking.EntityLock regionLock(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, java.lang.String, org.apache.hadoop.hbase.Abortable) throws java.io.IOException;\n  public abstract void unassign(byte[]) throws java.io.IOException;\n  public abstract boolean reportRegionSizesForQuotas(org.apache.hadoop.hbase.quotas.RegionSizeStore);\n  public abstract boolean reportFileArchivalForQuotas(org.apache.hadoop.hbase.TableName, java.util.Collection<java.util.Map$Entry<java.lang.String, java.lang.Long>>);\n  public abstract boolean isClusterUp();\n  public abstract org.apache.hadoop.hbase.regionserver.ReplicationSourceService getReplicationSourceService();\n  public abstract org.apache.hadoop.hbase.TableDescriptors getTableDescriptors();\n  public abstract java.util.Optional<org.apache.hadoop.hbase.io.hfile.BlockCache> getBlockCache();\n  public abstract java.util.Optional<org.apache.hadoop.hbase.mob.MobFileCache> getMobFileCache();\n  public abstract org.apache.hadoop.hbase.security.access.AccessChecker getAccessChecker();\n  public abstract org.apache.hadoop.hbase.security.access.ZKPermissionWatcher getZKPermissionWatcher();\n  public abstract org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationBufferManager getRegionReplicationBufferManager();\n  public abstract org.apache.hadoop.hbase.regionserver.HRegion getRegion(java.lang.String);\n  public abstract java.util.List<org.apache.hadoop.hbase.regionserver.HRegion> getRegions(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.regionserver.HRegion> getRegions();\n  public default org.apache.hadoop.hbase.regionserver.Region getRegion(java.lang.String);\n}\n;;;No. While this class has many methods, it is an interface and does not define a concrete message. It might be used to define the methods that could be invoked on an object put on a message queue, but it is not a message definition itself.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServerTableMetrics.class;;;public class org.apache.hadoop.hbase.regionserver.RegionServerTableMetrics {\n  public org.apache.hadoop.hbase.regionserver.RegionServerTableMetrics(boolean);\n  public void updatePut(org.apache.hadoop.hbase.TableName, long);\n  public void updatePutBatch(org.apache.hadoop.hbase.TableName, long);\n  public void updateGet(org.apache.hadoop.hbase.TableName, long);\n  public void updateIncrement(org.apache.hadoop.hbase.TableName, long);\n  public void updateAppend(org.apache.hadoop.hbase.TableName, long);\n  public void updateDelete(org.apache.hadoop.hbase.TableName, long);\n  public void updateDeleteBatch(org.apache.hadoop.hbase.TableName, long);\n  public void updateCheckAndDelete(org.apache.hadoop.hbase.TableName, long);\n  public void updateCheckAndPut(org.apache.hadoop.hbase.TableName, long);\n  public void updateCheckAndMutate(org.apache.hadoop.hbase.TableName, long);\n  public void updateScanTime(org.apache.hadoop.hbase.TableName, long);\n  public void updateScanSize(org.apache.hadoop.hbase.TableName, long);\n  public void updateTableReadQueryMeter(org.apache.hadoop.hbase.TableName, long);\n  public void updateTableWriteQueryMeter(org.apache.hadoop.hbase.TableName, long);\n  public void updateTableWriteQueryMeter(org.apache.hadoop.hbase.TableName);\n}\n;;;No. This class contains methods for updating metrics related to table operations but it does not define any specific message format or structure that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionServicesForStores.class;;;public class org.apache.hadoop.hbase.regionserver.RegionServicesForStores {\n  public org.apache.hadoop.hbase.regionserver.RegionServicesForStores(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public void addMemStoreSize(long, long, long, int);\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public org.apache.hadoop.hbase.wal.WAL getWAL();\n  public org.apache.hadoop.hbase.io.ByteBuffAllocator getByteBuffAllocator();\n  public long getMemStoreFlushSize();\n  public int getNumStores();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.class;;;public abstract class org.apache.hadoop.hbase.regionserver.RegionSplitPolicy extends org.apache.hadoop.conf.Configured {\n  public org.apache.hadoop.hbase.regionserver.RegionSplitPolicy();\n  public static org.apache.hadoop.hbase.regionserver.RegionSplitPolicy create(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.RegionSplitPolicy> getSplitPolicyClass(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n}\n;;;No. This class provides functionality for the HBase region server and is not a message definition that can be put on a message queue.;;;N;;;No. This is not a task definition, but rather a class definition.;;;N
org/apache/hadoop/hbase/regionserver/RegionSplitRestriction.class;;;public abstract class org.apache.hadoop.hbase.regionserver.RegionSplitRestriction {\n  public static final java.lang.String RESTRICTION_TYPE_KEY;\n  public static final java.lang.String RESTRICTION_TYPE_NONE;\n  public static final java.lang.String RESTRICTION_TYPE_KEY_PREFIX;\n  public static final java.lang.String RESTRICTION_TYPE_DELIMITED_KEY_PREFIX;\n  public org.apache.hadoop.hbase.regionserver.RegionSplitRestriction();\n  public static org.apache.hadoop.hbase.regionserver.RegionSplitRestriction create(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract void initialize(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract byte[] getRestrictedSplitPoint(byte[]);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RemoteProcedureResultReporter.class;;;class org.apache.hadoop.hbase.regionserver.RemoteProcedureResultReporter extends java.lang.Thread {\n  public org.apache.hadoop.hbase.regionserver.RemoteProcedureResultReporter(org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public void complete(long, java.lang.Throwable);\n  public void run();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/ReplicationService.class;;;public interface org.apache.hadoop.hbase.regionserver.ReplicationService {\n  public abstract void initialize(org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.wal.WALFactory) throws java.io.IOException;\n  public abstract void startReplicationService() throws java.io.IOException;\n  public abstract void stopReplicationService();\n  public abstract org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad refreshAndGetReplicationLoad();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/ReplicationSinkService.class;;;public interface org.apache.hadoop.hbase.regionserver.ReplicationSinkService extends org.apache.hadoop.hbase.regionserver.ReplicationService {\n  public abstract void replicateLogEntries(java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/ReplicationSourceService.class;;;public interface org.apache.hadoop.hbase.regionserver.ReplicationSourceService extends org.apache.hadoop.hbase.regionserver.ReplicationService {\n  public abstract org.apache.hadoop.hbase.replication.regionserver.SyncReplicationPeerInfoProvider getSyncReplicationPeerInfoProvider();\n  public abstract org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandler getPeerProcedureHandler();\n  public abstract org.apache.hadoop.hbase.replication.ReplicationPeers getReplicationPeers();\n  public abstract org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager getReplicationManager();\n}\n;;;No. This is an interface definition for a service in the HBase region server and does not represent a specific message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap$ReversedKVScannerComparator.class;;;class org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap$ReversedKVScannerComparator extends org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator {\n  public org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap$ReversedKVScannerComparator(org.apache.hadoop.hbase.CellComparator);\n  public int compare(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.regionserver.KeyValueScanner);\n  public int compareRows(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ReversedKeyValueHeap.class;;;public class org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap extends org.apache.hadoop.hbase.regionserver.KeyValueHeap {\n  public org.apache.hadoop.hbase.regionserver.ReversedKeyValueHeap(java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.CellComparator) throws java.io.IOException;\n  public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException;\n  public boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell next() throws java.io.IOException;\n  public boolean seekToLastRow() throws java.io.IOException;\n}\n;;;No, this is a class definition for a Java class in the Apache Hadoop project. It is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ReversedMobStoreScanner.class;;;public class org.apache.hadoop.hbase.regionserver.ReversedMobStoreScanner extends org.apache.hadoop.hbase.regionserver.ReversedStoreScanner {\n  public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException;\n  public void shipped() throws java.io.IOException;\n  public void close();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ReversedRegionScannerImpl.class;;;class org.apache.hadoop.hbase.regionserver.ReversedRegionScannerImpl extends org.apache.hadoop.hbase.regionserver.RegionScannerImpl {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ReversedStoreScanner.class;;;public class org.apache.hadoop.hbase.regionserver.ReversedStoreScanner extends org.apache.hadoop.hbase.regionserver.StoreScanner implements org.apache.hadoop.hbase.regionserver.KeyValueScanner {\n  public org.apache.hadoop.hbase.regionserver.ReversedStoreScanner(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, long) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.ReversedStoreScanner(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.NavigableSet<byte[]>, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>) throws java.io.IOException;\n  public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RowTooBigException.class;;;public class org.apache.hadoop.hbase.regionserver.RowTooBigException extends org.apache.hadoop.hbase.client.RowTooBigException {\n  public org.apache.hadoop.hbase.regionserver.RowTooBigException(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/RpcSchedulerFactory.class;;;public interface org.apache.hadoop.hbase.regionserver.RpcSchedulerFactory {\n  public abstract org.apache.hadoop.hbase.ipc.RpcScheduler create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.Abortable);\n}\n;;;No. This is an interface definition for a class that creates an RPC scheduler, but it is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ScanInfo.class;;;public class org.apache.hadoop.hbase.regionserver.ScanInfo {\n  public static final long FIXED_OVERHEAD;\n  public org.apache.hadoop.hbase.regionserver.ScanInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, long, long, org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.regionserver.ScanInfo(org.apache.hadoop.conf.Configuration, byte[], int, int, long, org.apache.hadoop.hbase.KeepDeletedCells, long, long, org.apache.hadoop.hbase.CellComparator, boolean);\n  public byte[] getFamily();\n  public int getMinVersions();\n  public int getMaxVersions();\n  public long getTtl();\n  public org.apache.hadoop.hbase.KeepDeletedCells getKeepDeletedCells();\n  public long getTimeToPurgeDeletes();\n  public org.apache.hadoop.hbase.CellComparator getComparator();\n  public boolean isNewVersionBehavior();\n  public java.lang.String toString();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a class definition for a data model used in Apache HBase.;;;N;;;No;;;N
org/apache/hadoop/hbase/regionserver/ScanOptions.class;;;public interface org.apache.hadoop.hbase.regionserver.ScanOptions {\n  public abstract int getMaxVersions();\n  public abstract void setMaxVersions(int);\n  public default void readAllVersions();\n  public abstract long getTTL();\n  public abstract void setTTL(long);\n  public abstract void setKeepDeletedCells(org.apache.hadoop.hbase.KeepDeletedCells);\n  public abstract org.apache.hadoop.hbase.KeepDeletedCells getKeepDeletedCells();\n  public abstract int getMinVersions();\n  public abstract void setMinVersions(int);\n  public abstract long getTimeToPurgeDeletes();\n  public abstract void setTimeToPurgeDeletes(long);\n  public abstract org.apache.hadoop.hbase.client.Scan getScan();\n}\n;;;No. While this class defines methods related to scanning options in HBase, it is not specifically designed as a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition, but rather a definition for scan options in the HBase region server.;;;N
org/apache/hadoop/hbase/regionserver/ScanType.class;;;public final class org.apache.hadoop.hbase.regionserver.ScanType extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.ScanType> {\n  public static final org.apache.hadoop.hbase.regionserver.ScanType COMPACT_DROP_DELETES;\n  public static final org.apache.hadoop.hbase.regionserver.ScanType COMPACT_RETAIN_DELETES;\n  public static final org.apache.hadoop.hbase.regionserver.ScanType USER_SCAN;\n  public static org.apache.hadoop.hbase.regionserver.ScanType[] values();\n  public static org.apache.hadoop.hbase.regionserver.ScanType valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ScannerContext$1.class;;;class org.apache.hadoop.hbase.regionserver.ScannerContext$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ScannerContext$Builder.class;;;public final class org.apache.hadoop.hbase.regionserver.ScannerContext$Builder {\n  public org.apache.hadoop.hbase.regionserver.ScannerContext$Builder setKeepProgress(boolean);\n  public org.apache.hadoop.hbase.regionserver.ScannerContext$Builder setTrackMetrics(boolean);\n  public org.apache.hadoop.hbase.regionserver.ScannerContext$Builder setSizeLimit(org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope, long, long);\n  public org.apache.hadoop.hbase.regionserver.ScannerContext$Builder setTimeLimit(org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope, long);\n  public org.apache.hadoop.hbase.regionserver.ScannerContext$Builder setBatchLimit(int);\n  public org.apache.hadoop.hbase.regionserver.ScannerContext build();\n}\n;;;No, this is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ScannerContext$LimitFields.class;;;class org.apache.hadoop.hbase.regionserver.ScannerContext$LimitFields {\n  public java.lang.String toString();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ScannerContext$LimitScope.class;;;public final class org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope> {\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope BETWEEN_ROWS;\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope BETWEEN_CELLS;\n  public static org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope[] values();\n  public static org.apache.hadoop.hbase.regionserver.ScannerContext$LimitScope valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ScannerContext$NextState.class;;;public final class org.apache.hadoop.hbase.regionserver.ScannerContext$NextState extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.ScannerContext$NextState> {\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext$NextState MORE_VALUES;\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext$NextState NO_MORE_VALUES;\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext$NextState SIZE_LIMIT_REACHED;\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext$NextState SIZE_LIMIT_REACHED_MID_ROW;\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext$NextState TIME_LIMIT_REACHED;\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext$NextState TIME_LIMIT_REACHED_MID_ROW;\n  public static final org.apache.hadoop.hbase.regionserver.ScannerContext$NextState BATCH_LIMIT_REACHED;\n  public static org.apache.hadoop.hbase.regionserver.ScannerContext$NextState[] values();\n  public static org.apache.hadoop.hbase.regionserver.ScannerContext$NextState valueOf(java.lang.String);\n  public boolean hasMoreValues();\n  public boolean limitReached();\n  public static boolean isValidState(org.apache.hadoop.hbase.regionserver.ScannerContext$NextState);\n  public static boolean hasMoreValues(org.apache.hadoop.hbase.regionserver.ScannerContext$NextState);\n}\n;;;No. This class defines constants and methods related to the state of a scanner, but it does not define a message that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ScannerContext$ProgressFields.class;;;class org.apache.hadoop.hbase.regionserver.ScannerContext$ProgressFields {\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ScannerContext.class;;;public class org.apache.hadoop.hbase.regionserver.ScannerContext {\n  public boolean isTrackingMetrics();\n  public org.apache.hadoop.hbase.client.metrics.ServerSideScanMetrics getMetrics();\n  public java.lang.String toString();\n  public static org.apache.hadoop.hbase.regionserver.ScannerContext$Builder newBuilder();\n  public static org.apache.hadoop.hbase.regionserver.ScannerContext$Builder newBuilder(boolean);\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class definition for the ScannerContext class in the HBase regionserver library. It defines methods for tracking metrics and creating new instances of the class.;;;N
org/apache/hadoop/hbase/regionserver/ScannerIdGenerator.class;;;public class org.apache.hadoop.hbase.regionserver.ScannerIdGenerator {\n  public org.apache.hadoop.hbase.regionserver.ScannerIdGenerator(org.apache.hadoop.hbase.ServerName);\n  public long generateNewScannerId();\n  public static void main(java.lang.String[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager$1.class;;;class org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager$1 implements java.security.PrivilegedAction<java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>>> {\n  public java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>> run();\n  public java.lang.Object run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager$SecureBulkLoadListener.class;;;class org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager$SecureBulkLoadListener implements org.apache.hadoop.hbase.regionserver.HRegion$BulkLoadListener {\n  public org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager$SecureBulkLoadListener(org.apache.hadoop.fs.FileSystem, java.lang.String, org.apache.hadoop.conf.Configuration);\n  public java.lang.String prepareBulkLoad(byte[], java.lang.String, boolean, java.lang.String) throws java.io.IOException;\n  public void doneBulkLoad(byte[], java.lang.String) throws java.io.IOException;\n  public void failedBulkLoad(byte[], java.lang.String) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/SecureBulkLoadManager.class;;;public class org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager {\n  public static final long VERSION;\n  public void start() throws java.io.IOException;\n  public void stop() throws java.io.IOException;\n  public java.lang.String prepareBulkLoad(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$PrepareBulkLoadRequest) throws java.io.IOException;\n  public void cleanupBulkLoad(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$CleanupBulkLoadRequest) throws java.io.IOException;\n  public java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>> secureBulkLoadHFiles(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$BulkLoadHFileRequest) throws java.io.IOException;\n  public java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>> secureBulkLoadHFiles(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$BulkLoadHFileRequest, java.util.List<java.lang.String>) throws java.io.IOException;\n}\n;;;No. This class defines a SecureBulkLoadManager with methods to start, stop, prepareBulkLoad, cleanupBulkLoad, and secureBulkLoadHFiles. However, it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/Segment.class;;;public abstract class org.apache.hadoop.hbase.regionserver.Segment implements org.apache.hadoop.hbase.regionserver.MemStoreSizing {\n  public static final long FIXED_OVERHEAD;\n  public static final long DEEP_OVERHEAD;\n  public java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner> getScanners(long);\n  public boolean isEmpty();\n  public void close();\n  public org.apache.hadoop.hbase.Cell maybeCloneWithAllocator(org.apache.hadoop.hbase.Cell, boolean);\n  public boolean shouldSeek(org.apache.hadoop.hbase.io.TimeRange, long);\n  public boolean isTagsPresent();\n  public void incScannerCount();\n  public void decScannerCount();\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getMemStoreSize();\n  public long getDataSize();\n  public long getHeapSize();\n  public long getOffHeapSize();\n  public int getCellsCount();\n  public long incMemStoreSize(long, long, long, int);\n  public boolean sharedLock();\n  public void sharedUnlock();\n  public void waitForUpdates();\n  public boolean compareAndSetDataSize(long, long);\n  public long getMinSequenceId();\n  public org.apache.hadoop.hbase.regionserver.TimeRangeTracker getTimeRangeTracker();\n  public org.apache.hadoop.hbase.Cell last();\n  public java.util.Iterator<org.apache.hadoop.hbase.Cell> iterator();\n  public java.util.SortedSet<org.apache.hadoop.hbase.Cell> headSet(org.apache.hadoop.hbase.Cell);\n  public int compare(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public int compareRows(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/SegmentFactory$1.class;;;class org.apache.hadoop.hbase.regionserver.SegmentFactory$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SegmentFactory.class;;;public final class org.apache.hadoop.hbase.regionserver.SegmentFactory {\n  public static org.apache.hadoop.hbase.regionserver.SegmentFactory instance();\n  public org.apache.hadoop.hbase.regionserver.CompositeImmutableSegment createCompositeImmutableSegment(org.apache.hadoop.hbase.CellComparator, java.util.List<org.apache.hadoop.hbase.regionserver.ImmutableSegment>);\n  public org.apache.hadoop.hbase.regionserver.ImmutableSegment createImmutableSegmentByCompaction(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.regionserver.MemStoreSegmentsIterator, int, org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType, org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.ImmutableSegment createImmutableSegment(org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.regionserver.ImmutableSegment createImmutableSegment(org.apache.hadoop.hbase.regionserver.MutableSegment, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public org.apache.hadoop.hbase.regionserver.MutableSegment createMutableSegment(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.regionserver.MemStoreSizing);\n  public org.apache.hadoop.hbase.regionserver.ImmutableSegment createImmutableSegmentByMerge(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.regionserver.MemStoreSegmentsIterator, int, java.util.List<org.apache.hadoop.hbase.regionserver.ImmutableSegment>, org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType, org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.ImmutableSegment createImmutableSegmentByFlattening(org.apache.hadoop.hbase.regionserver.CSLMImmutableSegment, org.apache.hadoop.hbase.regionserver.CompactingMemStore$IndexType, org.apache.hadoop.hbase.regionserver.MemStoreSizing, org.apache.hadoop.hbase.regionserver.MemStoreCompactionStrategy$Action);\n}\n;;;No. This class provides factory methods for creating different types of segments in HBase, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SegmentScanner.class;;;public class org.apache.hadoop.hbase.regionserver.SegmentScanner implements org.apache.hadoop.hbase.regionserver.KeyValueScanner {\n  public org.apache.hadoop.hbase.Cell peek();\n  public org.apache.hadoop.hbase.Cell next() throws java.io.IOException;\n  public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean seekToLastRow() throws java.io.IOException;\n  public void close();\n  public boolean shouldUseScanner(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.HStore, long);\n  public boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException;\n  public boolean realSeekDone();\n  public void enforceSeek() throws java.io.IOException;\n  public boolean isFileScanner();\n  public org.apache.hadoop.fs.Path getFilePath();\n  public org.apache.hadoop.hbase.Cell getNextIndexedKey();\n  public void shipped() throws java.io.IOException;\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/SequenceId.class;;;public interface org.apache.hadoop.hbase.regionserver.SequenceId {\n  public static final long NO_SEQUENCE_ID;\n  public default long getSequenceId();\n}\n;;;No. This is an interface for a class in the Apache HBase library that defines a method for getting a sequence ID. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ServerNonceManager$1.class;;;class org.apache.hadoop.hbase.regionserver.ServerNonceManager$1 extends org.apache.hadoop.hbase.ScheduledChore {\n}\n;;;No.;;;N;;;No, it is not a task definition, it is a class definition for a scheduled chore in the HBase region server.;;;N
org/apache/hadoop/hbase/regionserver/ServerNonceManager$OperationContext.class;;;class org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext {\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.regionserver.ServerNonceManager$OperationContext();\n  public void setState(int);\n  public int getState();\n  public void setHasWait();\n  public boolean hasWait();\n  public void reportActivity();\n  public boolean isExpired(long);\n  public void setMvcc(long);\n  public long getMvcc();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ServerNonceManager.class;;;public class org.apache.hadoop.hbase.regionserver.ServerNonceManager {\n  public static final java.lang.String HASH_NONCE_GRACE_PERIOD_KEY;\n  public org.apache.hadoop.hbase.regionserver.ServerNonceManager(org.apache.hadoop.conf.Configuration);\n  public void setConflictWaitIterationMs(int);\n  public boolean startOperation(long, long, org.apache.hadoop.hbase.Stoppable) throws java.lang.InterruptedException;\n  public void endOperation(long, long, boolean);\n  public void addMvccToOperationContext(long, long, long);\n  public long getMvccFromOperationContext(long, long);\n  public void reportOperationFromWal(long, long, long);\n  public org.apache.hadoop.hbase.ScheduledChore createCleanupScheduledChore(org.apache.hadoop.hbase.Stoppable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/Shipper.class;;;public interface org.apache.hadoop.hbase.regionserver.Shipper {\n  public abstract void shipped() throws java.io.IOException;\n}\n;;;No, it is an interface definition for a class named "Shipper" in the Apache Hadoop HBase library. It does not define a message that could be put on a message queue.;;;N;;;No, it is not a task definition, but rather an interface defining a method for HBase region server.;;;N
org/apache/hadoop/hbase/regionserver/ShipperListener.class;;;public interface org.apache.hadoop.hbase.regionserver.ShipperListener {\n  public abstract void beforeShipped() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ShutdownHook$DoNothingStoppable.class;;;class org.apache.hadoop.hbase.regionserver.ShutdownHook$DoNothingStoppable implements org.apache.hadoop.hbase.Stoppable {\n  public boolean isStopped();\n  public void stop(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ShutdownHook$DoNothingThread.class;;;class org.apache.hadoop.hbase.regionserver.ShutdownHook$DoNothingThread extends java.lang.Thread {\n  public void run();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition that extends the Thread class and defines the behavior of a thread when it is run.;;;N
org/apache/hadoop/hbase/regionserver/ShutdownHook$ShutdownHookThread.class;;;class org.apache.hadoop.hbase.regionserver.ShutdownHook$ShutdownHookThread extends java.lang.Thread {\n  public void run();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ShutdownHook.class;;;public class org.apache.hadoop.hbase.regionserver.ShutdownHook {\n  public static final java.lang.String RUN_SHUTDOWN_HOOK;\n  public static final java.lang.String FS_SHUTDOWN_HOOK_WAIT;\n  public org.apache.hadoop.hbase.regionserver.ShutdownHook();\n  public static void install(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.Stoppable, java.lang.Thread);\n  public static void main(java.lang.String[]) throws java.io.IOException;\n}\n;;;No. This class is not a message definition. It is a class that provides methods for shutting down a Hadoop HBase region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SimpleRpcSchedulerFactory.class;;;public class org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory implements org.apache.hadoop.hbase.regionserver.RpcSchedulerFactory {\n  public org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory();\n  public org.apache.hadoop.hbase.ipc.RpcScheduler create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.Abortable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SnapshotRegionCallable.class;;;public class org.apache.hadoop.hbase.regionserver.SnapshotRegionCallable extends org.apache.hadoop.hbase.procedure2.BaseRSProcedureCallable {\n  public org.apache.hadoop.hbase.regionserver.SnapshotRegionCallable();\n  public org.apache.hadoop.hbase.executor.EventType getEventType();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/SnapshotSegmentScanner.class;;;public class org.apache.hadoop.hbase.regionserver.SnapshotSegmentScanner extends org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner {\n  public org.apache.hadoop.hbase.regionserver.SnapshotSegmentScanner(org.apache.hadoop.hbase.regionserver.ImmutableSegment);\n  public org.apache.hadoop.hbase.Cell peek();\n  public org.apache.hadoop.hbase.Cell next();\n  public boolean seek(org.apache.hadoop.hbase.Cell);\n  public boolean reseek(org.apache.hadoop.hbase.Cell);\n  public long getScannerOrder();\n  public void close();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SnapshotVerifyCallable.class;;;public class org.apache.hadoop.hbase.regionserver.SnapshotVerifyCallable extends org.apache.hadoop.hbase.procedure2.BaseRSProcedureCallable {\n  public org.apache.hadoop.hbase.regionserver.SnapshotVerifyCallable();\n  public org.apache.hadoop.hbase.executor.EventType getEventType();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/SplitLogWorker$TaskExecutor$Status.class;;;public final class org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status> {\n  public static final org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status DONE;\n  public static final org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status ERR;\n  public static final org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status RESIGNED;\n  public static final org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status PREEMPTED;\n  public static org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status[] values();\n  public static org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status valueOf(java.lang.String);\n}\n;;;No. This is a Java enumeration class and not a message definition.;;;N;;;No, it is not a task definition. It is a class that defines the possible status values for a task executor in the HBase region server.;;;N
org/apache/hadoop/hbase/regionserver/SplitLogWorker$TaskExecutor.class;;;public interface org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor {\n  public abstract org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor$Status exec(java.lang.String, org.apache.hadoop.hbase.util.CancelableProgressable);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/SplitLogWorker.class;;;public class org.apache.hadoop.hbase.regionserver.SplitLogWorker implements java.lang.Runnable {\n  public org.apache.hadoop.hbase.regionserver.SplitLogWorker(org.apache.hadoop.hbase.Server, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor);\n  public org.apache.hadoop.hbase.regionserver.SplitLogWorker(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.regionserver.LastSequenceId, org.apache.hadoop.hbase.wal.WALFactory);\n  public void run();\n  public void stopTask();\n  public void start();\n  public void stop();\n  public int getTaskReadySeq();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/SplitRequest.class;;;class org.apache.hadoop.hbase.regionserver.SplitRequest implements java.lang.Runnable {\n  public java.lang.String toString();\n  public void run();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/SplitWALCallable$1.class;;;class org.apache.hadoop.hbase.regionserver.SplitWALCallable$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SplitWALCallable$ErrorWALSplitException.class;;;public class org.apache.hadoop.hbase.regionserver.SplitWALCallable$ErrorWALSplitException extends org.apache.hadoop.hbase.HBaseIOException {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SplitWALCallable$PreemptedWALSplitException.class;;;public class org.apache.hadoop.hbase.regionserver.SplitWALCallable$PreemptedWALSplitException extends org.apache.hadoop.hbase.HBaseIOException {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SplitWALCallable$ResignedWALSplitException.class;;;public class org.apache.hadoop.hbase.regionserver.SplitWALCallable$ResignedWALSplitException extends org.apache.hadoop.hbase.HBaseIOException {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SplitWALCallable.class;;;public class org.apache.hadoop.hbase.regionserver.SplitWALCallable extends org.apache.hadoop.hbase.procedure2.BaseRSProcedureCallable {\n  public org.apache.hadoop.hbase.regionserver.SplitWALCallable();\n  public org.apache.hadoop.hbase.executor.EventType getEventType();\n  public java.lang.String getWalPath();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/SteppingSplitPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy extends org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy {\n  public org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy();\n  public java.lang.String toString();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/Store.class;;;public interface org.apache.hadoop.hbase.regionserver.Store {\n  public static final int PRIORITY_USER;\n  public static final int NO_PRIORITY;\n  public abstract org.apache.hadoop.hbase.CellComparator getComparator();\n  public abstract java.util.Collection<? extends org.apache.hadoop.hbase.regionserver.StoreFile> getStorefiles();\n  public abstract java.util.Collection<? extends org.apache.hadoop.hbase.regionserver.StoreFile> getCompactedFiles();\n  public abstract long timeOfOldestEdit();\n  public abstract org.apache.hadoop.fs.FileSystem getFileSystem();\n  public abstract boolean shouldPerformMajorCompaction() throws java.io.IOException;\n  public abstract boolean needsCompaction();\n  public abstract int getCompactPriority();\n  public abstract boolean canSplit();\n  public abstract boolean hasReferences();\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreSize getMemStoreSize();\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreSize getFlushableSize();\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreSize getSnapshotSize();\n  public abstract org.apache.hadoop.hbase.client.ColumnFamilyDescriptor getColumnFamilyDescriptor();\n  public abstract java.util.OptionalLong getMaxSequenceId();\n  public abstract java.util.OptionalLong getMaxMemStoreTS();\n  public abstract long getLastCompactSize();\n  public abstract long getSize();\n  public abstract int getStorefilesCount();\n  public abstract int getCompactedFilesCount();\n  public abstract java.util.OptionalLong getMaxStoreFileAge();\n  public abstract java.util.OptionalLong getMinStoreFileAge();\n  public abstract java.util.OptionalDouble getAvgStoreFileAge();\n  public abstract long getNumReferenceFiles();\n  public abstract long getNumHFiles();\n  public abstract long getStoreSizeUncompressed();\n  public abstract long getStorefilesSize();\n  public abstract long getHFilesSize();\n  public abstract long getStorefilesRootLevelIndexSize();\n  public abstract long getTotalStaticIndexSize();\n  public abstract long getTotalStaticBloomSize();\n  public abstract org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public abstract boolean areWritesEnabled();\n  public abstract long getSmallestReadPoint();\n  public abstract java.lang.String getColumnFamilyName();\n  public abstract org.apache.hadoop.hbase.TableName getTableName();\n  public abstract long getFlushedCellsCount();\n  public abstract long getFlushedCellsSize();\n  public abstract long getFlushedOutputFileSize();\n  public abstract long getCompactedCellsCount();\n  public abstract long getCompactedCellsSize();\n  public abstract long getMajorCompactedCellsCount();\n  public abstract long getMajorCompactedCellsSize();\n  public abstract boolean hasTooManyStoreFiles();\n  public abstract void refreshStoreFiles() throws java.io.IOException;\n  public abstract double getCompactionPressure();\n  public abstract boolean isPrimaryReplicaStore();\n  public abstract boolean isSloppyMemStore();\n  public abstract int getCurrentParallelPutCount();\n  public abstract long getMemstoreOnlyRowReadsCount();\n  public abstract long getMixedRowReadsCount();\n  public abstract org.apache.hadoop.conf.Configuration getReadOnlyConfiguration();\n}\n;;;Yes, the above class might be put on a message queue as a message definition.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StoreConfigInformation.class;;;public interface org.apache.hadoop.hbase.regionserver.StoreConfigInformation {\n  public abstract long getMemStoreFlushSize();\n  public abstract long getStoreFileTtl();\n  public abstract long getCompactionCheckMultiplier();\n  public abstract long getBlockingFileCount();\n  public abstract org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public abstract java.lang.String getColumnFamilyName();\n}\n;;;No, this is not a message definition. It is an interface definition for accessing various configuration information for a data storage system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreContext$1.class;;;class org.apache.hadoop.hbase.regionserver.StoreContext$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreContext$Builder.class;;;public class org.apache.hadoop.hbase.regionserver.StoreContext$Builder {\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder();\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withBlockSize(int);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withEncryptionContext(org.apache.hadoop.hbase.io.crypto.Encryption$Context);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withCacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withRegionFileSystem(org.apache.hadoop.hbase.regionserver.HRegionFileSystem);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withCellComparator(org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withBloomType(org.apache.hadoop.hbase.regionserver.BloomType);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withCompactedFilesSupplier(java.util.function.Supplier<java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>>);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withFavoredNodesSupplier(java.util.function.Supplier<java.net.InetSocketAddress[]>);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withColumnFamilyDescriptor(org.apache.hadoop.hbase.client.ColumnFamilyDescriptor);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withFamilyStoreDirectoryPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.hbase.regionserver.StoreContext$Builder withRegionCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost);\n  public org.apache.hadoop.hbase.regionserver.StoreContext build();\n}\n;;;No. This class is a builder class used to create an instance of the `org.apache.hadoop.hbase.regionserver.StoreContext` class. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreContext.class;;;public final class org.apache.hadoop.hbase.regionserver.StoreContext implements org.apache.hadoop.hbase.io.HeapSize {\n  public static final long FIXED_OVERHEAD;\n  public int getBlockSize();\n  public org.apache.hadoop.hbase.io.crypto.Encryption$Context getEncryptionContext();\n  public org.apache.hadoop.hbase.io.hfile.CacheConfig getCacheConf();\n  public org.apache.hadoop.hbase.regionserver.HRegionFileSystem getRegionFileSystem();\n  public org.apache.hadoop.hbase.CellComparator getComparator();\n  public org.apache.hadoop.hbase.regionserver.BloomType getBloomFilterType();\n  public java.util.function.Supplier<java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>> getCompactedFilesSupplier();\n  public java.net.InetSocketAddress[] getFavoredNodes();\n  public org.apache.hadoop.hbase.client.ColumnFamilyDescriptor getFamily();\n  public org.apache.hadoop.fs.Path getFamilyStoreDirectoryPath();\n  public org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost getCoprocessorHost();\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public boolean isPrimaryReplicaStore();\n  public static org.apache.hadoop.hbase.regionserver.StoreContext$Builder getBuilder();\n  public long heapSize();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/StoreEngine$IOExceptionRunnable.class;;;public interface org.apache.hadoop.hbase.regionserver.StoreEngine$IOExceptionRunnable {\n  public abstract void run() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreEngine.class;;;public abstract class org.apache.hadoop.hbase.regionserver.StoreEngine<SF extends org.apache.hadoop.hbase.regionserver.StoreFlusher, CP extends org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy, C extends org.apache.hadoop.hbase.regionserver.compactions.Compactor<?>, SFM extends org.apache.hadoop.hbase.regionserver.StoreFileManager> {\n  public static final java.lang.String STORE_ENGINE_CLASS_KEY;\n  public org.apache.hadoop.hbase.regionserver.StoreEngine();\n  public void readLock();\n  public void readUnlock();\n  public void writeLock();\n  public void writeUnlock();\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy getCompactionPolicy();\n  public org.apache.hadoop.hbase.regionserver.compactions.Compactor<?> getCompactor();\n  public org.apache.hadoop.hbase.regionserver.StoreFileManager getStoreFileManager();\n  public org.apache.hadoop.hbase.regionserver.StoreFlusher getStoreFlusher();\n  public abstract boolean needsCompaction(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public abstract org.apache.hadoop.hbase.regionserver.compactions.CompactionContext createCompaction() throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter(org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HStoreFile createStoreFileAndReader(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HStoreFile createStoreFileAndReader(org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException;\n  public void validateStoreFile(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void initialize(boolean) throws java.io.IOException;\n  public void refreshStoreFiles() throws java.io.IOException;\n  public void refreshStoreFiles(java.util.Collection<java.lang.String>) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> commitStoreFiles(java.util.List<org.apache.hadoop.fs.Path>, boolean) throws java.io.IOException;\n  public void addStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, org.apache.hadoop.hbase.regionserver.StoreEngine$IOExceptionRunnable) throws java.io.IOException;\n  public void replaceStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, org.apache.hadoop.hbase.regionserver.StoreEngine$IOExceptionRunnable, java.lang.Runnable) throws java.io.IOException;\n  public void removeCompactedFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public static org.apache.hadoop.hbase.regionserver.StoreEngine<?, ?, ?, ?> create(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.CellComparator) throws java.io.IOException;\n  public boolean requireWritingToTmpDirFirst();\n}\n;;;No. This class provides implementation details for a StoreEngine in HBase, but it is not a message definition that carries information between distributed systems.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreFile.class;;;public interface org.apache.hadoop.hbase.regionserver.StoreFile {\n  public abstract java.util.Optional<org.apache.hadoop.hbase.Cell> getFirstKey();\n  public abstract java.util.Optional<org.apache.hadoop.hbase.Cell> getLastKey();\n  public abstract org.apache.hadoop.hbase.CellComparator getComparator();\n  public abstract long getMaxMemStoreTS();\n  public abstract org.apache.hadoop.fs.Path getPath();\n  public abstract org.apache.hadoop.fs.Path getEncodedPath();\n  public abstract org.apache.hadoop.fs.Path getQualifiedPath();\n  public abstract boolean isReference();\n  public abstract boolean isHFile();\n  public abstract boolean isMajorCompactionResult();\n  public abstract boolean excludeFromMinorCompaction();\n  public abstract long getMaxSequenceId();\n  public abstract long getModificationTimestamp() throws java.io.IOException;\n  public abstract boolean isBulkLoadResult();\n  public abstract java.util.OptionalLong getBulkLoadTimestamp();\n  public abstract java.lang.String toStringDetailed();\n  public abstract java.util.OptionalLong getMinimumTimestamp();\n  public abstract java.util.OptionalLong getMaximumTimestamp();\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an interface defining methods for accessing properties of a specific object in the Apache HBase library.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileComparators$1.class;;;class org.apache.hadoop.hbase.regionserver.StoreFileComparators$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileComparators$GetBulkTime.class;;;class org.apache.hadoop.hbase.regionserver.StoreFileComparators$GetBulkTime implements java.util.function.ToLongFunction<org.apache.hadoop.hbase.regionserver.HStoreFile> {\n  public long applyAsLong(org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public long applyAsLong(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileComparators$GetFileSize.class;;;class org.apache.hadoop.hbase.regionserver.StoreFileComparators$GetFileSize implements java.util.function.ToLongFunction<org.apache.hadoop.hbase.regionserver.HStoreFile> {\n  public long applyAsLong(org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public long applyAsLong(java.lang.Object);\n}\n;;;No.;;;N;;;No. It is a functional interface implementation in Java and does not represent a task that can be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileComparators$GetMaxTimestamp.class;;;class org.apache.hadoop.hbase.regionserver.StoreFileComparators$GetMaxTimestamp implements java.util.function.ToLongFunction<org.apache.hadoop.hbase.regionserver.HStoreFile> {\n  public long applyAsLong(org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public long applyAsLong(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileComparators$GetPathName.class;;;class org.apache.hadoop.hbase.regionserver.StoreFileComparators$GetPathName implements java.util.function.Function<org.apache.hadoop.hbase.regionserver.HStoreFile, java.lang.String> {\n  public java.lang.String apply(org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public java.lang.Object apply(java.lang.Object);\n}\n;;;No.;;;N;;;No. This is a function definition in Java and is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileComparators.class;;;final class org.apache.hadoop.hbase.regionserver.StoreFileComparators {\n  public static final java.util.Comparator<org.apache.hadoop.hbase.regionserver.HStoreFile> SEQ_ID;\n  public static final java.util.Comparator<org.apache.hadoop.hbase.regionserver.HStoreFile> SEQ_ID_MAX_TIMESTAMP;\n}\n;;;No. This is a utility class that provides comparators for sorting HBase store files. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileInfo.class;;;public class org.apache.hadoop.hbase.regionserver.StoreFileInfo implements org.apache.hadoop.conf.Configurable {\n  public static final java.lang.String HFILE_NAME_REGEX;\n  public static final java.lang.String STORE_FILE_READER_NO_READAHEAD;\n  public static final boolean DEFAULT_STORE_FILE_READER_NO_READAHEAD;\n  public org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.hbase.io.HFileLink);\n  public org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.hbase.io.Reference);\n  public org.apache.hadoop.hbase.regionserver.StoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.hbase.io.Reference, org.apache.hadoop.hbase.io.HFileLink);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public long getSize();\n  public void setRegionCoprocessorHost(org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost);\n  public org.apache.hadoop.hbase.io.Reference getReference();\n  public boolean isReference();\n  public boolean isTopReference();\n  public boolean isLink();\n  public org.apache.hadoop.hbase.HDFSBlocksDistribution getHDFSBlockDistribution();\n  public org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileStatus getReferencedFileStatus(org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getPath();\n  public org.apache.hadoop.fs.FileStatus getFileStatus() throws java.io.IOException;\n  public long getModificationTime() throws java.io.IOException;\n  public java.lang.String toString();\n  public static boolean isHFile(org.apache.hadoop.fs.Path);\n  public static boolean isHFile(java.lang.String);\n  public static boolean isMobFile(org.apache.hadoop.fs.Path);\n  public static boolean isMobRefFile(org.apache.hadoop.fs.Path);\n  public static boolean isReference(org.apache.hadoop.fs.Path);\n  public static boolean isReference(java.lang.String);\n  public long getCreatedTimestamp();\n  public static org.apache.hadoop.fs.Path getReferredToFile(org.apache.hadoop.fs.Path);\n  public static org.apache.hadoop.hbase.util.Pair<java.lang.String, java.lang.String> getReferredToRegionAndFile(java.lang.String);\n  public static boolean validateStoreFileName(java.lang.String);\n  public static boolean isValid(org.apache.hadoop.fs.FileStatus) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public java.lang.String getActiveFileName();\n  public void initHFileInfo(org.apache.hadoop.hbase.io.hfile.ReaderContext) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StoreFileManager.class;;;public interface org.apache.hadoop.hbase.regionserver.StoreFileManager {\n  public abstract void loadFiles(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public abstract void insertNewFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public abstract void addCompactionResults(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public abstract void removeCompactedFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public abstract org.apache.hbase.thirdparty.com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.HStoreFile> clearFiles();\n  public abstract java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> clearCompactedFiles();\n  public abstract java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getStorefiles();\n  public abstract java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getCompactedfiles();\n  public abstract int getStorefileCount();\n  public abstract int getCompactedFilesCount();\n  public abstract java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getFilesForScan(byte[], boolean, byte[], boolean);\n  public abstract java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile> getCandidateFilesForRowKeyBefore(org.apache.hadoop.hbase.KeyValue);\n  public abstract java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile> updateCandidateFilesForRowKeyBefore(java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile>, org.apache.hadoop.hbase.KeyValue, org.apache.hadoop.hbase.Cell);\n  public abstract java.util.Optional<byte[]> getSplitPoint() throws java.io.IOException;\n  public abstract int getStoreCompactionPriority();\n  public abstract java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getUnneededFiles(long, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public abstract double getCompactionPressure();\n  public abstract java.util.Comparator<org.apache.hadoop.hbase.regionserver.HStoreFile> getStoreFileComparator();\n}\n;;;No. This is an interface definition in Java, not a message definition for a queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileReader$1.class;;;class org.apache.hadoop.hbase.regionserver.StoreFileReader$1 {\n}\n;;;No. This class is not a message definition and cannot be put on a message queue. It appears to be a nested class within the org.apache.hadoop.hbase.regionserver.StoreFileReader class.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileReader.class;;;public class org.apache.hadoop.hbase.regionserver.StoreFileReader {\n  public org.apache.hadoop.hbase.regionserver.StoreFileReader(org.apache.hadoop.hbase.io.hfile.ReaderContext, org.apache.hadoop.hbase.io.hfile.HFileInfo, org.apache.hadoop.hbase.io.hfile.CacheConfig, java.util.concurrent.atomic.AtomicInteger, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public boolean isPrimaryReplicaReader();\n  public org.apache.hadoop.hbase.CellComparator getComparator();\n  public org.apache.hadoop.hbase.regionserver.StoreFileScanner getStoreFileScanner(boolean, boolean, boolean, long, long, boolean);\n  public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(boolean, boolean);\n  public org.apache.hadoop.hbase.io.hfile.HFileScanner getScanner(boolean, boolean, boolean);\n  public void close(boolean) throws java.io.IOException;\n  public boolean passesDeleteFamilyBloomFilter(byte[], int, int);\n  public boolean passesGeneralRowColBloomFilter(org.apache.hadoop.hbase.Cell);\n  public boolean passesKeyRangeFilter(org.apache.hadoop.hbase.client.Scan);\n  public java.util.Map<byte[], byte[]> loadFileInfo() throws java.io.IOException;\n  public void loadBloomfilter();\n  public void loadBloomfilter(org.apache.hadoop.hbase.io.hfile.BlockType);\n  public long getFilterEntries();\n  public void setGeneralBloomFilterFaulty();\n  public void setDeleteFamilyBloomFilterFaulty();\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> getLastKey();\n  public java.util.Optional<byte[]> getLastRowKey();\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> midKey() throws java.io.IOException;\n  public long length();\n  public long getTotalUncompressedBytes();\n  public long getEntries();\n  public long getDeleteFamilyCnt();\n  public java.util.Optional<org.apache.hadoop.hbase.Cell> getFirstKey();\n  public long indexSize();\n  public org.apache.hadoop.hbase.regionserver.BloomType getBloomFilterType();\n  public long getSequenceID();\n  public void setSequenceID(long);\n  public void setBulkLoaded(boolean);\n  public boolean isBulkLoaded();\n  public long getTotalBloomSize();\n  public int getHFileVersion();\n  public int getHFileMinorVersion();\n  public org.apache.hadoop.hbase.io.hfile.HFile$Reader getHFileReader();\n  public long getMaxTimestamp();\n  public int getPrefixLength();\n  public org.apache.hadoop.hbase.io.hfile.ReaderContext getReaderContext();\n}\n;;;No. This class provides methods to interact with a specific type of file reader used in the HBase region server, but it does not define a message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileScanner.class;;;public class org.apache.hadoop.hbase.regionserver.StoreFileScanner implements org.apache.hadoop.hbase.regionserver.KeyValueScanner {\n  public org.apache.hadoop.hbase.regionserver.StoreFileScanner(org.apache.hadoop.hbase.regionserver.StoreFileReader, org.apache.hadoop.hbase.io.hfile.HFileScanner, boolean, boolean, long, long, boolean);\n  public static java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner> getScannersForStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean, boolean, long) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner> getScannersForStoreFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean, boolean, org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher, long) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner> getScannersForCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, long) throws java.io.IOException;\n  public java.lang.String toString();\n  public org.apache.hadoop.hbase.Cell peek();\n  public org.apache.hadoop.hbase.Cell next() throws java.io.IOException;\n  public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public void close();\n  public static boolean seekAtOrAfter(org.apache.hadoop.hbase.io.hfile.HFileScanner, org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public long getScannerOrder();\n  public boolean requestSeek(org.apache.hadoop.hbase.Cell, boolean, boolean) throws java.io.IOException;\n  public boolean realSeekDone();\n  public void enforceSeek() throws java.io.IOException;\n  public boolean isFileScanner();\n  public org.apache.hadoop.fs.Path getFilePath();\n  public boolean shouldUseScanner(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.HStore, long);\n  public boolean seekToPreviousRow(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean seekToLastRow() throws java.io.IOException;\n  public boolean backwardSeek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell getNextIndexedKey();\n  public void shipped() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StoreFileWriter$1.class;;;class org.apache.hadoop.hbase.regionserver.StoreFileWriter$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue, as it does not contain any information about specific tasks that need to be performed.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileWriter$Builder.class;;;public class org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder {\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withOutputDir(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withFilePath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withFavoredNodes(java.net.InetSocketAddress[]);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withBloomType(org.apache.hadoop.hbase.regionserver.BloomType);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withMaxKeyCount(long);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withFileContext(org.apache.hadoop.hbase.io.hfile.HFileContext);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withShouldDropCacheBehind(boolean);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withCompactedFilesSupplier(java.util.function.Supplier<java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>>);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withFileStoragePolicy(java.lang.String);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter$Builder withWriterCreationTracker(java.util.function.Consumer<org.apache.hadoop.fs.Path>);\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter build() throws java.io.IOException;\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a builder class used for creating instances of the StoreFileWriter class in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StoreFileWriter.class;;;public class org.apache.hadoop.hbase.regionserver.StoreFileWriter implements org.apache.hadoop.hbase.regionserver.CellSink,org.apache.hadoop.hbase.regionserver.ShipperListener {\n  public long getPos() throws java.io.IOException;\n  public void appendMetadata(long, boolean) throws java.io.IOException;\n  public void appendMetadata(long, boolean, java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public void appendMetadata(long, boolean, long) throws java.io.IOException;\n  public void appendMobMetadata(org.apache.hbase.thirdparty.com.google.common.collect.SetMultimap<org.apache.hadoop.hbase.TableName, java.lang.String>) throws java.io.IOException;\n  public void appendTrackedTimestampsToMetadata() throws java.io.IOException;\n  public void trackTimestamps(org.apache.hadoop.hbase.Cell);\n  public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public void beforeShipped() throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getPath();\n  public boolean hasGeneralBloom();\n  public void close() throws java.io.IOException;\n  public void appendFileInfo(byte[], byte[]) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StoreFlushContext.class;;;interface org.apache.hadoop.hbase.regionserver.StoreFlushContext {\n  public abstract org.apache.hadoop.hbase.regionserver.MemStoreSize prepare();\n  public abstract void flushCache(org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.io.IOException;\n  public abstract boolean commit(org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.io.IOException;\n  public abstract void replayFlush(java.util.List<java.lang.String>, boolean) throws java.io.IOException;\n  public abstract void abort() throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.fs.Path> getCommittedFiles();\n  public abstract long getOutputFileSize();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StoreFlusher.class;;;abstract class org.apache.hadoop.hbase.regionserver.StoreFlusher {\n  public org.apache.hadoop.hbase.regionserver.StoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HStore);\n  public abstract java.util.List<org.apache.hadoop.fs.Path> flushSnapshot(org.apache.hadoop.hbase.regionserver.MemStoreSnapshot, long, org.apache.hadoop.hbase.monitoring.MonitoredTask, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StoreScanner$1.class;;;class org.apache.hadoop.hbase.regionserver.StoreScanner$1 {\n}\n;;;No.;;;N;;;No, it is not a task or message definition. It appears to be an anonymous inner class in the HBase framework, which is not intended to be used as a message or task.;;;N
org/apache/hadoop/hbase/regionserver/StoreScanner.class;;;public class org.apache.hadoop.hbase.regionserver.StoreScanner extends org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner implements org.apache.hadoop.hbase.regionserver.KeyValueScanner,org.apache.hadoop.hbase.regionserver.InternalScanner,org.apache.hadoop.hbase.regionserver.ChangedReadersObserver {\n  public static final java.lang.String STORESCANNER_PARALLEL_SEEK_ENABLE;\n  public static final java.lang.String HBASE_CELLS_SCANNED_PER_HEARTBEAT_CHECK;\n  public static final long DEFAULT_HBASE_CELLS_SCANNED_PER_HEARTBEAT_CHECK;\n  public static final java.lang.String STORESCANNER_PREAD_MAX_BYTES;\n  public org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.client.Scan, java.util.NavigableSet<byte[]>, long) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, org.apache.hadoop.hbase.regionserver.ScanType, long, long) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>, long, long, byte[], byte[]) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreScanner(org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.regionserver.ScanType, java.util.List<? extends org.apache.hadoop.hbase.regionserver.KeyValueScanner>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell peek();\n  public org.apache.hadoop.hbase.KeyValue next();\n  public void close();\n  public boolean seek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean next(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.regionserver.ScannerContext) throws java.io.IOException;\n  public long getReadPoint();\n  public void updateReaders(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.KeyValueScanner>) throws java.io.IOException;\n  public boolean reseek(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public long getEstimatedNumberOfKvsScanned();\n  public org.apache.hadoop.hbase.Cell getNextIndexedKey();\n  public void shipped() throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell next() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StoreUtils.class;;;public final class org.apache.hadoop.hbase.regionserver.StoreUtils {\n  public static java.util.OptionalInt getDeterministicRandomSeed(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public static boolean hasReferences(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public static long getLowestTimestamp(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public static java.util.OptionalLong getMaxMemStoreTSInList(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public static java.util.OptionalLong getMaxSequenceIdInList(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public static org.apache.hadoop.hbase.util.ChecksumType getChecksumType(org.apache.hadoop.conf.Configuration);\n  public static int getBytesPerChecksum(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.conf.Configuration createStoreConfiguration(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor);\n  public static java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileInfo> toStoreFileInfo(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public static long getTotalUncompressedBytes(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public static long getStorefilesSize(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.function.Predicate<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public static long getStorefileFieldSize(org.apache.hadoop.hbase.regionserver.HStoreFile, java.util.function.ToLongFunction<org.apache.hadoop.hbase.regionserver.StoreFileReader>);\n}\n;;;No. This class contains utility methods for working with HBase stores and does not define a message structure.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StorefileRefresherChore.class;;;public class org.apache.hadoop.hbase.regionserver.StorefileRefresherChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public static final java.lang.String REGIONSERVER_STOREFILE_REFRESH_PERIOD;\n  public static final java.lang.String REGIONSERVER_META_STOREFILE_REFRESH_PERIOD;\n  public org.apache.hadoop.hbase.regionserver.StorefileRefresherChore(int, boolean, org.apache.hadoop.hbase.regionserver.HRegionServer, org.apache.hadoop.hbase.Stoppable);\n}\n;;;No, it is a class definition for a chore in the HBase system.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter$BoundaryMultiWriter.class;;;public class org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$BoundaryMultiWriter extends org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter {\n  public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$BoundaryMultiWriter(org.apache.hadoop.hbase.CellComparator, java.util.List<byte[]>, byte[], byte[]) throws java.io.IOException;\n  public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter$SizeMultiWriter.class;;;public class org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$SizeMultiWriter extends org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter {\n  public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter$SizeMultiWriter(org.apache.hadoop.hbase.CellComparator, int, long, byte[], byte[]);\n  public void append(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeMultiFileWriter.class;;;public abstract class org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter extends org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter {\n  public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter(org.apache.hadoop.hbase.CellComparator);\n  public void setNoStripeMetadata();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreConfig.class;;;public class org.apache.hadoop.hbase.regionserver.StripeStoreConfig {\n  public static final java.lang.String MAX_FILES_KEY;\n  public static final java.lang.String MIN_FILES_KEY;\n  public static final java.lang.String MIN_FILES_L0_KEY;\n  public static final java.lang.String SIZE_TO_SPLIT_KEY;\n  public static final java.lang.String SPLIT_PARTS_KEY;\n  public static final java.lang.String INITIAL_STRIPE_COUNT_KEY;\n  public static final java.lang.String FLUSH_TO_L0_KEY;\n  public static final java.lang.String MAX_REGION_SPLIT_IMBALANCE_KEY;\n  public org.apache.hadoop.hbase.regionserver.StripeStoreConfig(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation);\n  public float getMaxSplitImbalance();\n  public int getLevel0MinFiles();\n  public int getStripeCompactMinFiles();\n  public int getStripeCompactMaxFiles();\n  public boolean isUsingL0Flush();\n  public long getSplitSize();\n  public int getInitialCount();\n  public float getSplitCount();\n  public long getSplitPartSize();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreEngine$1.class;;;class org.apache.hadoop.hbase.regionserver.StripeStoreEngine$1 {\n}\n;;;No.;;;N;;;No, it is not a message or a task definition that might be put on a message or task queue. It is an anonymous inner class in the Apache HBase library.;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreEngine$StripeCompaction.class;;;class org.apache.hadoop.hbase.regionserver.StripeStoreEngine$StripeCompaction extends org.apache.hadoop.hbase.regionserver.compactions.CompactionContext {\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> preSelect(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public boolean select(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean) throws java.io.IOException;\n  public void forceSelect(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n  public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreEngine.class;;;public class org.apache.hadoop.hbase.regionserver.StripeStoreEngine extends org.apache.hadoop.hbase.regionserver.StoreEngine<org.apache.hadoop.hbase.regionserver.StripeStoreFlusher, org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy, org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, org.apache.hadoop.hbase.regionserver.StripeStoreFileManager> {\n  public org.apache.hadoop.hbase.regionserver.StripeStoreEngine();\n  public boolean needsCompaction(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionContext createCompaction();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFileManager$1.class;;;class org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$1 {\n}\n;;;No. This class is not a message definition and cannot be put on a message queue. It is an inner anonymous class within the StripeStoreFileManager class in the Apache HBase package.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFileManager$CompactionOrFlushMergeCopy.class;;;class org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$CompactionOrFlushMergeCopy {\n  public org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$CompactionOrFlushMergeCopy(org.apache.hadoop.hbase.regionserver.StripeStoreFileManager, boolean);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFileManager$KeyBeforeConcatenatedLists$Iterator.class;;;public class org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists$Iterator extends org.apache.hadoop.hbase.util.ConcatenatedLists<org.apache.hadoop.hbase.regionserver.HStoreFile>.Iterator {\n  public org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists$Iterator(org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists);\n  public java.util.ArrayList<java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>> getComponents();\n  public void removeComponents(int);\n  public void remove();\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFileManager$KeyBeforeConcatenatedLists.class;;;class org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$KeyBeforeConcatenatedLists extends org.apache.hadoop.hbase.util.ConcatenatedLists<org.apache.hadoop.hbase.regionserver.HStoreFile> {\n  public java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile> iterator();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFileManager$State.class;;;class org.apache.hadoop.hbase.regionserver.StripeStoreFileManager$State {\n  public byte[][] stripeEndRows;\n  public java.util.ArrayList<org.apache.hbase.thirdparty.com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.HStoreFile>> stripeFiles;\n  public org.apache.hbase.thirdparty.com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.HStoreFile> level0Files;\n  public org.apache.hbase.thirdparty.com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.HStoreFile> allFilesCached;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFileManager.class;;;public class org.apache.hadoop.hbase.regionserver.StripeStoreFileManager implements org.apache.hadoop.hbase.regionserver.StoreFileManager,org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider {\n  public static final byte[] STRIPE_START_KEY;\n  public static final byte[] STRIPE_END_KEY;\n  public static final byte[] OPEN_KEY;\n  public org.apache.hadoop.hbase.regionserver.StripeStoreFileManager(org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StripeStoreConfig);\n  public void loadFiles(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getStorefiles();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getCompactedfiles();\n  public int getCompactedFilesCount();\n  public void insertNewFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public org.apache.hbase.thirdparty.com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.HStoreFile> clearFiles();\n  public org.apache.hbase.thirdparty.com.google.common.collect.ImmutableCollection<org.apache.hadoop.hbase.regionserver.HStoreFile> clearCompactedFiles();\n  public int getStorefileCount();\n  public java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile> getCandidateFilesForRowKeyBefore(org.apache.hadoop.hbase.KeyValue);\n  public java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile> updateCandidateFilesForRowKeyBefore(java.util.Iterator<org.apache.hadoop.hbase.regionserver.HStoreFile>, org.apache.hadoop.hbase.KeyValue, org.apache.hadoop.hbase.Cell);\n  public java.util.Optional<byte[]> getSplitPoint() throws java.io.IOException;\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getFilesForScan(byte[], boolean, byte[], boolean);\n  public void addCompactionResults(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public void removeCompactedFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public int getStoreCompactionPriority();\n  public final byte[] getStartRow(int);\n  public final byte[] getEndRow(int);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> getLevel0Files();\n  public java.util.List<byte[]> getStripeBoundaries();\n  public java.util.ArrayList<org.apache.hbase.thirdparty.com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.HStoreFile>> getStripes();\n  public int getStripeCount();\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getUnneededFiles(long, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public double getCompactionPressure();\n  public java.util.Comparator<org.apache.hadoop.hbase.regionserver.HStoreFile> getStoreFileComparator();\n  public java.util.Collection clearCompactedFiles();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFlusher$1.class;;;class org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$1 implements org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter$WriterFactory {\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter() throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue, as it defines a writer factory for a specific HBase component rather than a task to be executed.;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFlusher$BoundaryStripeFlushRequest.class;;;public class org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$BoundaryStripeFlushRequest extends org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest {\n  public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$BoundaryStripeFlushRequest(org.apache.hadoop.hbase.CellComparator, java.util.List<byte[]>);\n  public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter createWriter() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFlusher$SizeStripeFlushRequest.class;;;public class org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$SizeStripeFlushRequest extends org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest {\n  public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$SizeStripeFlushRequest(org.apache.hadoop.hbase.CellComparator, int, long);\n  public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter createWriter() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFlusher$StripeFlushRequest.class;;;public class org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest {\n  public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest(org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter createWriter() throws java.io.IOException;\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/StripeStoreFlusher.class;;;public class org.apache.hadoop.hbase.regionserver.StripeStoreFlusher extends org.apache.hadoop.hbase.regionserver.StoreFlusher {\n  public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HStore, org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy, org.apache.hadoop.hbase.regionserver.StripeStoreFileManager);\n  public java.util.List<org.apache.hadoop.fs.Path> flushSnapshot(org.apache.hadoop.hbase.regionserver.MemStoreSnapshot, long, org.apache.hadoop.hbase.monitoring.MonitoredTask, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class definition for a Java class.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/ThreadSafeMemStoreSizing.class;;;class org.apache.hadoop.hbase.regionserver.ThreadSafeMemStoreSizing implements org.apache.hadoop.hbase.regionserver.MemStoreSizing {\n  public org.apache.hadoop.hbase.regionserver.MemStoreSize getMemStoreSize();\n  public long incMemStoreSize(long, long, long, int);\n  public boolean compareAndSetDataSize(long, long);\n  public long getDataSize();\n  public long getHeapSize();\n  public long getOffHeapSize();\n  public int getCellsCount();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/TimeRangeTracker$1.class;;;class org.apache.hadoop.hbase.regionserver.TimeRangeTracker$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/TimeRangeTracker$NonSyncTimeRangeTracker.class;;;public class org.apache.hadoop.hbase.regionserver.TimeRangeTracker$NonSyncTimeRangeTracker extends org.apache.hadoop.hbase.regionserver.TimeRangeTracker {\n  public long getMin();\n  public long getMax();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/TimeRangeTracker$SyncTimeRangeTracker.class;;;public class org.apache.hadoop.hbase.regionserver.TimeRangeTracker$SyncTimeRangeTracker extends org.apache.hadoop.hbase.regionserver.TimeRangeTracker {\n  public long getMin();\n  public long getMax();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/TimeRangeTracker$Type.class;;;public final class org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type> {\n  public static final org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type NON_SYNC;\n  public static final org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type SYNC;\n  public static org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type[] values();\n  public static org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type valueOf(java.lang.String);\n}\n;;;No, this class does not contain any message fields or definitions. It is an enum that defines types of time range trackers in a HBase region server. It is not meant to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/TimeRangeTracker.class;;;public abstract class org.apache.hadoop.hbase.regionserver.TimeRangeTracker {\n  public org.apache.hadoop.hbase.regionserver.TimeRangeTracker();\n  public static org.apache.hadoop.hbase.regionserver.TimeRangeTracker create(org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type);\n  public static org.apache.hadoop.hbase.regionserver.TimeRangeTracker create(org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type, org.apache.hadoop.hbase.regionserver.TimeRangeTracker);\n  public static org.apache.hadoop.hbase.regionserver.TimeRangeTracker create(org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type, long, long);\n  public void includeTimestamp(org.apache.hadoop.hbase.Cell);\n  public boolean includesTimeRange(org.apache.hadoop.hbase.io.TimeRange);\n  public abstract long getMin();\n  public abstract long getMax();\n  public java.lang.String toString();\n  public static org.apache.hadoop.hbase.regionserver.TimeRangeTracker parseFrom(byte[]) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.TimeRangeTracker parseFrom(byte[], org.apache.hadoop.hbase.regionserver.TimeRangeTracker$Type) throws java.io.IOException;\n  public static byte[] toByteArray(org.apache.hadoop.hbase.regionserver.TimeRangeTracker) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/VersionedSegmentsList.class;;;public class org.apache.hadoop.hbase.regionserver.VersionedSegmentsList {\n  public org.apache.hadoop.hbase.regionserver.VersionedSegmentsList(java.util.List<org.apache.hadoop.hbase.regionserver.ImmutableSegment>, long);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.ImmutableSegment> getStoreSegments();\n  public long getVersion();\n  public int getNumOfCells();\n  public int getNumOfSegments();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/AbstractMultiOutputCompactor$1.class;;;class org.apache.hadoop.hbase.regionserver.compactions.AbstractMultiOutputCompactor$1 implements org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter$WriterFactory {\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter() throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriterWithStoragePolicy(java.lang.String) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/compactions/AbstractMultiOutputCompactor.class;;;public abstract class org.apache.hadoop.hbase.regionserver.compactions.AbstractMultiOutputCompactor<T extends org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter> extends org.apache.hadoop.hbase.regionserver.compactions.Compactor<T> {\n  public org.apache.hadoop.hbase.regionserver.compactions.AbstractMultiOutputCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HStore);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CloseChecker.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.CloseChecker {\n  public static final java.lang.String SIZE_LIMIT_KEY;\n  public static final java.lang.String TIME_LIMIT_KEY;\n  public org.apache.hadoop.hbase.regionserver.compactions.CloseChecker(org.apache.hadoop.conf.Configuration, long);\n  public boolean isSizeLimit(org.apache.hadoop.hbase.regionserver.Store, long);\n  public boolean isTimeLimit(org.apache.hadoop.hbase.regionserver.Store, long);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionConfiguration.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration {\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_RATIO_KEY;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_RATIO_OFFPEAK_KEY;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_MIN_KEY_OLD;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_MIN_KEY;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_MIN_SIZE_KEY;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_MAX_KEY;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_MAX_SIZE_KEY;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_MAX_SIZE_OFFPEAK_KEY;\n  public static final java.lang.String HBASE_HSTORE_OFFPEAK_END_HOUR;\n  public static final java.lang.String HBASE_HSTORE_OFFPEAK_START_HOUR;\n  public static final java.lang.String HBASE_HSTORE_MIN_LOCALITY_TO_SKIP_MAJOR_COMPACT;\n  public static final java.lang.String HBASE_HFILE_COMPACTION_DISCHARGER_THREAD_COUNT;\n  public static final java.lang.String DATE_TIERED_MAX_AGE_MILLIS_KEY;\n  public static final java.lang.String DATE_TIERED_INCOMING_WINDOW_MIN_KEY;\n  public static final java.lang.String COMPACTION_POLICY_CLASS_FOR_DATE_TIERED_WINDOWS_KEY;\n  public static final java.lang.String DATE_TIERED_SINGLE_OUTPUT_FOR_MINOR_COMPACTION_KEY;\n  public static final java.lang.String DATE_TIERED_COMPACTION_WINDOW_FACTORY_CLASS_KEY;\n  public static final java.lang.String DATE_TIERED_STORAGE_POLICY_ENABLE_KEY;\n  public static final java.lang.String DATE_TIERED_HOT_WINDOW_AGE_MILLIS_KEY;\n  public static final java.lang.String DATE_TIERED_HOT_WINDOW_STORAGE_POLICY_KEY;\n  public static final java.lang.String DATE_TIERED_WARM_WINDOW_AGE_MILLIS_KEY;\n  public static final java.lang.String DATE_TIERED_WARM_WINDOW_STORAGE_POLICY_KEY;\n  public static final java.lang.String DATE_TIERED_COLD_WINDOW_STORAGE_POLICY_KEY;\n  public java.lang.String toString();\n  public long getMinCompactSize();\n  public long getMaxCompactSize();\n  public int getMinFilesToCompact();\n  public void setMinFilesToCompact(int);\n  public int getMaxFilesToCompact();\n  public double getCompactionRatio();\n  public double getCompactionRatioOffPeak();\n  public long getThrottlePoint();\n  public long getMajorCompactionPeriod();\n  public float getMajorCompactionJitter();\n  public float getMinLocalityToForceCompact();\n  public long getOffPeakMaxCompactSize();\n  public long getMaxCompactSize(boolean);\n  public long getDateTieredMaxStoreFileAgeMillis();\n  public int getDateTieredIncomingWindowMin();\n  public java.lang.String getCompactionPolicyForDateTieredWindow();\n  public boolean useDateTieredSingleOutputForMinorCompaction();\n  public java.lang.String getDateTieredCompactionWindowFactory();\n  public boolean isDateTieredStoragePolicyEnable();\n  public long getHotWindowAgeMillis();\n  public long getWarmWindowAgeMillis();\n  public java.lang.String getHotWindowStoragePolicy();\n  public java.lang.String getWarmWindowStoragePolicy();\n  public java.lang.String getColdWindowStoragePolicy();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionContext.class;;;public abstract class org.apache.hadoop.hbase.regionserver.compactions.CompactionContext {\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionContext();\n  public abstract java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> preSelect(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public abstract boolean select(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean) throws java.io.IOException;\n  public void forceSelect(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n  public abstract java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl getRequest();\n  public boolean hasSelection();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionLifeCycleTracker$1.class;;;final class org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker$1 implements org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionLifeCycleTracker.class;;;public interface org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker {\n  public static final org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker DUMMY;\n  public default void notExecuted(org.apache.hadoop.hbase.regionserver.Store, java.lang.String);\n  public default void beforeExecution(org.apache.hadoop.hbase.regionserver.Store);\n  public default void afterExecution(org.apache.hadoop.hbase.regionserver.Store);\n  public default void completed();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionPolicy.class;;;public abstract class org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy {\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation);\n  public abstract boolean shouldPerformMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public abstract boolean throttleCompaction(long);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration getConf();\n}\n;;;No.;;;N;;;No. It is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionProgress.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress {\n  public long totalCompactingKVs;\n  public long currentCompactedKVs;\n  public long totalCompactedSize;\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress(long);\n  public float getProgressPct();\n  public void cancel();\n  public void complete();\n  public long getTotalCompactingKVs();\n  public long getCurrentCompactedKvs();\n  public long getTotalCompactedSize();\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.class;;;public interface org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest {\n  public abstract java.util.Collection<? extends org.apache.hadoop.hbase.regionserver.StoreFile> getFiles();\n  public abstract long getSize();\n  public abstract boolean isAllFiles();\n  public abstract boolean isMajor();\n  public abstract int getPriority();\n  public abstract boolean isOffPeak();\n  public abstract long getSelectionTime();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionRequestImpl$DisplayCompactionType.class;;;final class org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl$DisplayCompactionType extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl$DisplayCompactionType> {\n  public static final org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl$DisplayCompactionType MINOR;\n  public static final org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl$DisplayCompactionType ALL_FILES;\n  public static final org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl$DisplayCompactionType MAJOR;\n  public static org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl$DisplayCompactionType[] values();\n  public static org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl$DisplayCompactionType valueOf(java.lang.String);\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an enum class that defines three constant values and methods for working with these values.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionRequestImpl.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl implements org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest {\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public void updateFiles(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getFiles();\n  public void setDescription(java.lang.String, java.lang.String);\n  public long getSize();\n  public boolean isAllFiles();\n  public boolean isMajor();\n  public int getPriority();\n  public void setPriority(int);\n  public boolean isOffPeak();\n  public void setOffPeak(boolean);\n  public long getSelectionTime();\n  public void setIsMajor(boolean, boolean);\n  public void setTracker(org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker getTracker();\n  public java.util.function.Consumer<org.apache.hadoop.fs.Path> getWriterCreationTracker();\n  public void setWriterCreationTracker(java.util.function.Consumer<org.apache.hadoop.fs.Path>);\n  public boolean isAfterSplit();\n  public void setAfterSplit(boolean);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionRequester.class;;;public interface org.apache.hadoop.hbase.regionserver.compactions.CompactionRequester {\n  public abstract void requestCompaction(org.apache.hadoop.hbase.regionserver.HRegion, java.lang.String, int, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public abstract void requestCompaction(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HStore, java.lang.String, int, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public abstract void requestSystemCompaction(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.regionserver.HStore, java.lang.String, boolean) throws java.io.IOException;\n  public abstract void switchCompaction(boolean);\n}\n;;;No. This is an interface for a compaction requester in the HBase region server, which defines methods for requesting and switching compactions. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionWindow.class;;;public abstract class org.apache.hadoop.hbase.regionserver.compactions.CompactionWindow {\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionWindow();\n  public abstract int compareToTimestamp(long);\n  public abstract org.apache.hadoop.hbase.regionserver.compactions.CompactionWindow nextEarlierWindow();\n  public abstract long startMillis();\n  public abstract long endMillis();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CompactionWindowFactory.class;;;public abstract class org.apache.hadoop.hbase.regionserver.compactions.CompactionWindowFactory {\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionWindowFactory();\n  public abstract org.apache.hadoop.hbase.regionserver.compactions.CompactionWindow newIncomingWindow(long);\n}\n;;;no;;;N;;;No, it is not a task definition, but rather a class definition.;;;N
org/apache/hadoop/hbase/regionserver/compactions/Compactor$1.class;;;class org.apache.hadoop.hbase.regionserver.compactions.Compactor$1 implements org.apache.hadoop.hbase.regionserver.compactions.Compactor$InternalScannerFactory {\n  public org.apache.hadoop.hbase.regionserver.ScanType getScanType(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n  public org.apache.hadoop.hbase.regionserver.InternalScanner createScanner(org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner>, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, long) throws java.io.IOException;\n}\n;;;No. It is a class definition for an inner class of the org.apache.hadoop.hbase.regionserver.compactions.Compactor class, which implements the org.apache.hadoop.hbase.regionserver.compactions.Compactor$InternalScannerFactory interface. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/Compactor$CellSinkFactory.class;;;public interface org.apache.hadoop.hbase.regionserver.compactions.Compactor$CellSinkFactory<S> {\n  public abstract S createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/Compactor$FileDetails.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails {\n  public long maxKeyCount;\n  public long earliestPutTs;\n  public long latestPutTs;\n  public long maxSeqId;\n  public long maxMVCCReadpoint;\n  public int maxTagsLength;\n  public long minSeqIdToKeep;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/Compactor$InternalScannerFactory.class;;;public interface org.apache.hadoop.hbase.regionserver.compactions.Compactor$InternalScannerFactory {\n  public abstract org.apache.hadoop.hbase.regionserver.ScanType getScanType(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n  public abstract org.apache.hadoop.hbase.regionserver.InternalScanner createScanner(org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner>, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, long) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/compactions/Compactor.class;;;public abstract class org.apache.hadoop.hbase.regionserver.compactions.Compactor<T extends org.apache.hadoop.hbase.regionserver.CellSink> {\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress getProgress();\n  public boolean isCompacting();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/CurrentHourProvider$Tick.class;;;final class org.apache.hadoop.hbase.regionserver.compactions.CurrentHourProvider$Tick {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/CurrentHourProvider.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.CurrentHourProvider {\n  public static int getCurrentHour();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionPolicy extends org.apache.hadoop.hbase.regionserver.compactions.SortedCompactionPolicy {\n  public org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation) throws java.io.IOException;\n  public boolean needsCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public boolean shouldPerformMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl selectMajorCompaction(java.util.ArrayList<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl selectMinorCompaction(java.util.ArrayList<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean) throws java.io.IOException;\n}\n;;;No. This is a class definition for a compaction policy in the HBase region server. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactionRequest.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionRequest extends org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl {\n  public org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<java.lang.Long>, java.util.Map<java.lang.Long, java.lang.String>);\n  public java.util.List<java.lang.Long> getBoundaries();\n  public java.util.Map<java.lang.Long, java.lang.String> getBoundariesPolicies();\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactor$1.class;;;class org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactor$1 implements org.apache.hadoop.hbase.regionserver.compactions.Compactor$CellSinkFactory<org.apache.hadoop.hbase.regionserver.DateTieredMultiFileWriter> {\n  public org.apache.hadoop.hbase.regionserver.DateTieredMultiFileWriter createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n  public java.lang.Object createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class definition for a Java class.;;;N;;;No;;;N
org/apache/hadoop/hbase/regionserver/compactions/DateTieredCompactor.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactor extends org.apache.hadoop.hbase.regionserver.compactions.AbstractMultiOutputCompactor<org.apache.hadoop.hbase.regionserver.DateTieredMultiFileWriter> {\n  public org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HStore);\n  public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl, java.util.List<java.lang.Long>, java.util.Map<java.lang.Long, java.lang.String>, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/DefaultCompactor$1.class;;;class org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor$1 implements org.apache.hadoop.hbase.regionserver.compactions.Compactor$CellSinkFactory<org.apache.hadoop.hbase.regionserver.StoreFileWriter> {\n  public org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n  public java.lang.Object createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/DefaultCompactor.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor extends org.apache.hadoop.hbase.regionserver.compactions.Compactor<org.apache.hadoop.hbase.regionserver.StoreFileWriter> {\n  public org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HStore);\n  public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/ExploringCompactionPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy extends org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy {\n  public org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> applyCompactionPolicy(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, int, int);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> selectCompactFiles(java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, int, boolean);\n}\n;;;No. The class contains methods and constructors, but it does not define a message or specify any information to be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/compactions/ExponentialCompactionWindowFactory$Window.class;;;final class org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory$Window extends org.apache.hadoop.hbase.regionserver.compactions.CompactionWindow {\n  public org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory$Window(org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory, long, long, long);\n  public int compareToTimestamp(long);\n  public org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory$Window nextEarlierWindow();\n  public long startMillis();\n  public long endMillis();\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionWindow nextEarlierWindow();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/ExponentialCompactionWindowFactory.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory extends org.apache.hadoop.hbase.regionserver.compactions.CompactionWindowFactory {\n  public static final java.lang.String BASE_WINDOW_MILLIS_KEY;\n  public static final java.lang.String WINDOWS_PER_TIER_KEY;\n  public static final java.lang.String MAX_TIER_AGE_MILLIS_KEY;\n  public org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory(org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionWindow newIncomingWindow(long);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/FIFOCompactionPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.FIFOCompactionPolicy extends org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy {\n  public org.apache.hadoop.hbase.regionserver.compactions.FIFOCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl selectCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean) throws java.io.IOException;\n  public boolean shouldPerformMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public boolean needsCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n}\n;;;No, this is a class definition for a compaction policy used in HBase. It does not define a message to be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/compactions/ForbidMajorCompactionChecker.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.ForbidMajorCompactionChecker implements java.util.function.BiPredicate<org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState> {\n  public org.apache.hadoop.hbase.regionserver.compactions.ForbidMajorCompactionChecker();\n  public boolean test(org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState);\n  public static org.apache.hadoop.hbase.regionserver.compactions.ForbidMajorCompactionChecker get();\n  public boolean test(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/compactions/OffPeakHours$1.class;;;final class org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours$1 extends org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours {\n  public boolean isOffPeakHour();\n  public boolean isOffPeakHour(int);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/OffPeakHours$OffPeakHoursImpl.class;;;class org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours$OffPeakHoursImpl extends org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours {\n  public boolean isOffPeakHour();\n  public boolean isOffPeakHour(int);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/OffPeakHours.class;;;public abstract class org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours {\n  public static final org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours DISABLED;\n  public org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours();\n  public static org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours getInstance(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.hbase.regionserver.compactions.OffPeakHours getInstance(int, int);\n  public abstract boolean isOffPeakHour(int);\n  public abstract boolean isOffPeakHour();\n}\n;;;No. It is an abstract class with methods and fields related to off-peak hours for HBase compactions. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/RatioBasedCompactionPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy extends org.apache.hadoop.hbase.regionserver.compactions.SortedCompactionPolicy {\n  public org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation);\n  public boolean shouldPerformMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public boolean needsCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public void setMinThreshold(int);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/SortedCompactionPolicy.class;;;public abstract class org.apache.hadoop.hbase.regionserver.compactions.SortedCompactionPolicy extends org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy {\n  public org.apache.hadoop.hbase.regionserver.compactions.SortedCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> preSelectCompactionForCoprocessor(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl selectCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean, boolean, boolean) throws java.io.IOException;\n  public abstract boolean shouldPerformMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public long getNextMajorCompactTime(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public boolean throttleCompaction(long);\n  public abstract boolean needsCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy$BoundaryStripeCompactionRequest.class;;;class org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest extends org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest {\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl, java.util.List<byte[]>);\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, java.util.List<byte[]>);\n  public java.util.List<org.apache.hadoop.fs.Path> execute(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n}\n;;;Yes, the class org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$BoundaryStripeCompactionRequest is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy$SplitStripeCompactionRequest.class;;;class org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest extends org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest {\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl, byte[], byte[], int, long);\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, byte[], byte[], long);\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$SplitStripeCompactionRequest(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>, byte[], byte[], int, long);\n  public java.util.List<org.apache.hadoop.fs.Path> execute(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public void setMajorRangeFull();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy$StripeCompactionRequest.class;;;public abstract class org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest {\n  public java.util.List<org.apache.hadoop.fs.Path> execute(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.fs.Path> execute(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n  public void setMajorRange(byte[], byte[]);\n  public org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl getRequest();\n  public void setRequest(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy$StripeInformationProvider.class;;;public interface org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider {\n  public abstract java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile> getStorefiles();\n  public abstract byte[] getStartRow(int);\n  public abstract byte[] getEndRow(int);\n  public abstract java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> getLevel0Files();\n  public abstract java.util.List<byte[]> getStripeBoundaries();\n  public abstract java.util.ArrayList<org.apache.hbase.thirdparty.com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.regionserver.HStoreFile>> getStripes();\n  public abstract int getStripeCount();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/StripeCompactionPolicy.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy extends org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy {\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.StoreConfigInformation, org.apache.hadoop.hbase.regionserver.StripeStoreConfig);\n  public java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile> preSelectFilesForCoprocessor(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest createEmptyRequest(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n  public org.apache.hadoop.hbase.regionserver.StripeStoreFlusher$StripeFlushRequest selectFlush(org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, int);\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeCompactionRequest selectCompaction(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>, boolean) throws java.io.IOException;\n  public boolean needsCompactions(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy$StripeInformationProvider, java.util.List<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n  public boolean shouldPerformMajorCompaction(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public boolean throttleCompaction(long);\n  public static long getTotalFileSize(java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor$1.class;;;class org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor$1 implements org.apache.hadoop.hbase.regionserver.compactions.Compactor$CellSinkFactory<org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter> {\n  public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n  public java.lang.Object createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor$2.class;;;class org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor$2 implements org.apache.hadoop.hbase.regionserver.compactions.Compactor$CellSinkFactory<org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter> {\n  public org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n  public java.lang.Object createWriter(org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, boolean, boolean, java.util.function.Consumer) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class definition for a factory that creates instances of the StripeMultiFileWriter class.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor$StripeInternalScannerFactory.class;;;final class org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor$StripeInternalScannerFactory implements org.apache.hadoop.hbase.regionserver.compactions.Compactor$InternalScannerFactory {\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor$StripeInternalScannerFactory(org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor, byte[], byte[]);\n  public org.apache.hadoop.hbase.regionserver.ScanType getScanType(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl);\n  public org.apache.hadoop.hbase.regionserver.InternalScanner createScanner(org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileScanner>, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.Compactor$FileDetails, long) throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/compactions/StripeCompactor.class;;;public class org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor extends org.apache.hadoop.hbase.regionserver.compactions.AbstractMultiOutputCompactor<org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter> {\n  public org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.HStore);\n  public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl, java.util.List<byte[]>, byte[], byte[], org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.Path> compact(org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl, int, long, byte[], byte[], byte[], byte[], org.apache.hadoop.hbase.regionserver.throttle.ThroughputController, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class definition for a compactor in the HBase region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/handler/AssignRegionHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.AssignRegionHandler extends org.apache.hadoop.hbase.executor.EventHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.AssignRegionHandler(org.apache.hadoop.hbase.regionserver.HRegionServer, org.apache.hadoop.hbase.client.RegionInfo, long, org.apache.hadoop.hbase.client.TableDescriptor, long, org.apache.hadoop.hbase.executor.EventType);\n  public void process() throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.handler.AssignRegionHandler create(org.apache.hadoop.hbase.regionserver.HRegionServer, org.apache.hadoop.hbase.client.RegionInfo, long, org.apache.hadoop.hbase.client.TableDescriptor, long);\n}\n;;;No. This is a class definition for a handler in the HBase region server, but it is not a message definition that would be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/handler/CloseMetaHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler extends org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.client.RegionInfo, boolean);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler extends org.apache.hadoop.hbase.executor.EventHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.client.RegionInfo, boolean, org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public void process() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/handler/HandlerUtil.class;;;final class org.apache.hadoop.hbase.regionserver.handler.HandlerUtil {\n  public static org.apache.hadoop.hbase.util.RetryCounter getRetryCounter();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/handler/OpenMetaHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.OpenMetaHandler extends org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.OpenMetaHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, long);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/handler/OpenPriorityRegionHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.OpenPriorityRegionHandler extends org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.OpenPriorityRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, long);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler$1.class;;;class org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$1 implements org.apache.hadoop.hbase.util.CancelableProgressable {\n  public boolean progress();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler$PostOpenDeployTasksThread.class;;;class org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler$PostOpenDeployTasksThread extends java.lang.Thread {\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler extends org.apache.hadoop.hbase.executor.EventHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, long);\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public void process() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/handler/ParallelSeekHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.ParallelSeekHandler extends org.apache.hadoop.hbase.executor.EventHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.ParallelSeekHandler(org.apache.hadoop.hbase.regionserver.KeyValueScanner, org.apache.hadoop.hbase.Cell, long, java.util.concurrent.CountDownLatch);\n  public void process();\n  public java.lang.Throwable getErr();\n  public void setErr(java.lang.Throwable);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/handler/RSProcedureHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.RSProcedureHandler extends org.apache.hadoop.hbase.executor.EventHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.RSProcedureHandler(org.apache.hadoop.hbase.regionserver.HRegionServer, long, org.apache.hadoop.hbase.procedure2.RSProcedureCallable);\n  public void process();\n}\n;;;No. This is a class definition for a handler in the HBase region server. It is not a message definition that might be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/handler/RegionReplicaFlushHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.RegionReplicaFlushHandler extends org.apache.hadoop.hbase.executor.EventHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.RegionReplicaFlushHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.regionserver.HRegion);\n  public void process() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/handler/UnassignRegionHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.UnassignRegionHandler extends org.apache.hadoop.hbase.executor.EventHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.UnassignRegionHandler(org.apache.hadoop.hbase.regionserver.HRegionServer, java.lang.String, long, boolean, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.executor.EventType);\n  public void process() throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.handler.UnassignRegionHandler create(org.apache.hadoop.hbase.regionserver.HRegionServer, java.lang.String, long, boolean, org.apache.hadoop.hbase.ServerName);\n}\n;;;No. While the class contains methods and parameters, it is not a clear and concise message definition that can be put on a message queue. It appears to be a class that handles unassigning a region in a HBase region server.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/handler/WALSplitterHandler$1.class;;;class org.apache.hadoop.hbase.regionserver.handler.WALSplitterHandler$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/handler/WALSplitterHandler.class;;;public class org.apache.hadoop.hbase.regionserver.handler.WALSplitterHandler extends org.apache.hadoop.hbase.executor.EventHandler {\n  public org.apache.hadoop.hbase.regionserver.handler.WALSplitterHandler(org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails, org.apache.hadoop.hbase.util.CancelableProgressable, java.util.concurrent.atomic.AtomicInteger, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor);\n  public void process() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/http/RSDumpServlet.class;;;public class org.apache.hadoop.hbase.regionserver.http.RSDumpServlet extends org.apache.hadoop.hbase.monitoring.StateDumpServlet {\n  public org.apache.hadoop.hbase.regionserver.http.RSDumpServlet();\n  public void doGet(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) throws java.io.IOException;\n  public static void dumpQueue(org.apache.hadoop.hbase.regionserver.HRegionServer, java.io.PrintWriter);\n  public static void dumpCallQueues(org.apache.hadoop.hbase.regionserver.HRegionServer, java.io.PrintWriter);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/http/RSStatusServlet.class;;;public class org.apache.hadoop.hbase.regionserver.http.RSStatusServlet extends javax.servlet.http.HttpServlet {\n  public org.apache.hadoop.hbase.regionserver.http.RSStatusServlet();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/ColumnCount.class;;;class org.apache.hadoop.hbase.regionserver.querymatcher.ColumnCount {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ColumnCount(byte[]);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ColumnCount(byte[], int);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ColumnCount(byte[], int, int, int);\n  public byte[] getBuffer();\n  public int getOffset();\n  public int getLength();\n  public int decrement();\n  public int increment();\n  public void setCount(int);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/ColumnTracker.class;;;public interface org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker extends org.apache.hadoop.hbase.regionserver.ShipperListener {\n  public abstract org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode checkColumn(org.apache.hadoop.hbase.Cell, byte) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode checkVersions(org.apache.hadoop.hbase.Cell, long, byte, boolean) throws java.io.IOException;\n  public abstract void reset();\n  public abstract boolean done();\n  public abstract org.apache.hadoop.hbase.regionserver.querymatcher.ColumnCount getColumnHint();\n  public abstract org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode getNextRowOrNextColumn(org.apache.hadoop.hbase.Cell);\n  public abstract boolean isDone(long);\n  public default void doneWithColumn(org.apache.hadoop.hbase.Cell);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/CompactionScanQueryMatcher.class;;;public abstract class org.apache.hadoop.hbase.regionserver.querymatcher.CompactionScanQueryMatcher extends org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher {\n  public void beforeShipped() throws java.io.IOException;\n  public boolean hasNullColumnInQuery();\n  public boolean isUserScan();\n  public boolean moreRowsMayExistAfter(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.filter.Filter getFilter();\n  public org.apache.hadoop.hbase.Cell getNextKeyHint(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.CompactionScanQueryMatcher create(org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.regionserver.ScanType, long, long, long, long, byte[], byte[], org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost) throws java.io.IOException;\n}\n;;;No, the class is not a message definition that might be put on a message queue. It is an abstract class with methods that can be called to manipulate data in a Hadoop cluster.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/DeleteTracker$DeleteResult.class;;;public final class org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult> {\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult FAMILY_DELETED;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult FAMILY_VERSION_DELETED;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult COLUMN_DELETED;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult VERSION_DELETED;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult NOT_DELETED;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult VERSION_MASKED;\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult[] values();\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an enum class definition with static fields and methods.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/DeleteTracker.class;;;public interface org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker extends org.apache.hadoop.hbase.regionserver.ShipperListener {\n  public abstract void add(org.apache.hadoop.hbase.Cell);\n  public abstract org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult isDeleted(org.apache.hadoop.hbase.Cell);\n  public abstract boolean isEmpty();\n  public abstract void update();\n  public abstract void reset();\n  public abstract org.apache.hadoop.hbase.CellComparator getCellComparator();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/DropDeletesCompactionScanQueryMatcher.class;;;public abstract class org.apache.hadoop.hbase.regionserver.querymatcher.DropDeletesCompactionScanQueryMatcher extends org.apache.hadoop.hbase.regionserver.querymatcher.CompactionScanQueryMatcher {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a Java class that extends another class, used in the HBase application for querying and matching data. Task definitions for a task queue typically involve a specific action or task to be performed, such as running a particular function or executing a certain workflow.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/ExplicitColumnTracker.class;;;public class org.apache.hadoop.hbase.regionserver.querymatcher.ExplicitColumnTracker implements org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ExplicitColumnTracker(java.util.NavigableSet<byte[]>, int, int, long);\n  public boolean done();\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ColumnCount getColumnHint();\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode checkColumn(org.apache.hadoop.hbase.Cell, byte);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode checkVersions(org.apache.hadoop.hbase.Cell, long, byte, boolean) throws java.io.IOException;\n  public void reset();\n  public void doneWithColumn(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode getNextRowOrNextColumn(org.apache.hadoop.hbase.Cell);\n  public boolean isDone(long);\n  public void beforeShipped() throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/IncludeAllCompactionQueryMatcher.class;;;public class org.apache.hadoop.hbase.regionserver.querymatcher.IncludeAllCompactionQueryMatcher extends org.apache.hadoop.hbase.regionserver.querymatcher.MinorCompactionScanQueryMatcher {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.IncludeAllCompactionQueryMatcher(org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker, org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker, long, long, long);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode match(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/MajorCompactionScanQueryMatcher.class;;;public class org.apache.hadoop.hbase.regionserver.querymatcher.MajorCompactionScanQueryMatcher extends org.apache.hadoop.hbase.regionserver.querymatcher.DropDeletesCompactionScanQueryMatcher {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.MajorCompactionScanQueryMatcher(org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker, org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker, long, long, long, long);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode match(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;No. This is a class definition for a Java program and is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/MinorCompactionScanQueryMatcher.class;;;public class org.apache.hadoop.hbase.regionserver.querymatcher.MinorCompactionScanQueryMatcher extends org.apache.hadoop.hbase.regionserver.querymatcher.CompactionScanQueryMatcher {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.MinorCompactionScanQueryMatcher(org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker, org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker, long, long, long);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode match(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;No. This is a class definition for a query matcher used in the HBase region server, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/NewVersionBehaviorTracker$1.class;;;class org.apache.hadoop.hbase.regionserver.querymatcher.NewVersionBehaviorTracker$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/NewVersionBehaviorTracker$DeleteVersionsNode.class;;;public class org.apache.hadoop.hbase.regionserver.querymatcher.NewVersionBehaviorTracker$DeleteVersionsNode {\n  public long ts;\n  public long mvcc;\n  public void addVersionDelete(org.apache.hadoop.hbase.Cell);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/NewVersionBehaviorTracker.class;;;public class org.apache.hadoop.hbase.regionserver.querymatcher.NewVersionBehaviorTracker implements org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker,org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.NewVersionBehaviorTracker(java.util.NavigableSet<byte[]>, org.apache.hadoop.hbase.CellComparator, int, int, int, long);\n  public void beforeShipped() throws java.io.IOException;\n  public void add(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult isDeleted(org.apache.hadoop.hbase.Cell);\n  public boolean isEmpty();\n  public void update();\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode checkColumn(org.apache.hadoop.hbase.Cell, byte) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode checkVersions(org.apache.hadoop.hbase.Cell, long, byte, boolean) throws java.io.IOException;\n  public void reset();\n  public boolean done();\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ColumnCount getColumnHint();\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode getNextRowOrNextColumn(org.apache.hadoop.hbase.Cell);\n  public boolean isDone(long);\n  public org.apache.hadoop.hbase.CellComparator getCellComparator();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/NormalUserScanQueryMatcher$1.class;;;final class org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher$1 extends org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/NormalUserScanQueryMatcher$2.class;;;final class org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher$2 extends org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/NormalUserScanQueryMatcher$3.class;;;final class org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher$3 extends org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher {\n}\n;;;no;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition that extends another class.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/NormalUserScanQueryMatcher$4.class;;;final class org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher$4 extends org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher {\n}\n;;;No. This is a class definition and not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/NormalUserScanQueryMatcher.class;;;public abstract class org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher extends org.apache.hadoop.hbase.regionserver.querymatcher.UserScanQueryMatcher {\n  public void beforeShipped() throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode match(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.NormalUserScanQueryMatcher create(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker, org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker, boolean, long, long) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/RawScanQueryMatcher$1.class;;;final class org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher$1 extends org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/RawScanQueryMatcher$2.class;;;final class org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher$2 extends org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/RawScanQueryMatcher$3.class;;;final class org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher$3 extends org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/RawScanQueryMatcher$4.class;;;final class org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher$4 extends org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/RawScanQueryMatcher.class;;;public abstract class org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher extends org.apache.hadoop.hbase.regionserver.querymatcher.UserScanQueryMatcher {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode match(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.RawScanQueryMatcher create(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker, boolean, long, long);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/ScanDeleteTracker.class;;;public class org.apache.hadoop.hbase.regionserver.querymatcher.ScanDeleteTracker implements org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanDeleteTracker(org.apache.hadoop.hbase.CellComparator);\n  public void add(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult isDeleted(org.apache.hadoop.hbase.Cell);\n  public boolean isEmpty();\n  public void reset();\n  public void update();\n  public void beforeShipped() throws java.io.IOException;\n  public org.apache.hadoop.hbase.CellComparator getCellComparator();\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class definition for a data structure used in the HBase region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/ScanQueryMatcher$1.class;;;class org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/ScanQueryMatcher$MatchCode.class;;;public final class org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode> {\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode INCLUDE;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode SKIP;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode NEXT;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode DONE;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode SEEK_NEXT_ROW;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode SEEK_NEXT_COL;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode DONE_SCAN;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode SEEK_NEXT_USING_HINT;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode INCLUDE_AND_SEEK_NEXT_COL;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode INCLUDE_AND_SEEK_NEXT_ROW;\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode[] values();\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode valueOf(java.lang.String);\n}\n;;;No. This is a class definition for an enum, which provides a set of named values. It is not a message definition that could be placed on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/ScanQueryMatcher.class;;;public abstract class org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher implements org.apache.hadoop.hbase.regionserver.ShipperListener {\n  public abstract org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode match(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Cell getStartKey();\n  public abstract boolean hasNullColumnInQuery();\n  public org.apache.hadoop.hbase.Cell currentRow();\n  public void clearCurrentRow();\n  public void setToNewRow(org.apache.hadoop.hbase.Cell);\n  public abstract boolean isUserScan();\n  public abstract boolean moreRowsMayExistAfter(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.Cell getKeyForNextColumn(org.apache.hadoop.hbase.Cell);\n  public int compareKeyForNextRow(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public int compareKeyForNextColumn(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public abstract org.apache.hadoop.hbase.filter.Filter getFilter();\n  public abstract org.apache.hadoop.hbase.Cell getNextKeyHint(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public void beforeShipped() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/ScanWildcardColumnTracker.class;;;public class org.apache.hadoop.hbase.regionserver.querymatcher.ScanWildcardColumnTracker implements org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanWildcardColumnTracker(int, int, long, org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode checkColumn(org.apache.hadoop.hbase.Cell, byte) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode checkVersions(org.apache.hadoop.hbase.Cell, long, byte, boolean) throws java.io.IOException;\n  public void reset();\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ColumnCount getColumnHint();\n  public boolean done();\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode getNextRowOrNextColumn(org.apache.hadoop.hbase.Cell);\n  public void beforeShipped();\n  public boolean isDone(long);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/StripeCompactionScanQueryMatcher$1.class;;;class org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/StripeCompactionScanQueryMatcher$DropDeletesInOutput.class;;;final class org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher$DropDeletesInOutput extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher$DropDeletesInOutput> {\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher$DropDeletesInOutput BEFORE;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher$DropDeletesInOutput IN;\n  public static final org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher$DropDeletesInOutput AFTER;\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher$DropDeletesInOutput[] values();\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher$DropDeletesInOutput valueOf(java.lang.String);\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an enum definition in the HBase codebase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/StripeCompactionScanQueryMatcher.class;;;public class org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher extends org.apache.hadoop.hbase.regionserver.querymatcher.DropDeletesCompactionScanQueryMatcher {\n  public org.apache.hadoop.hbase.regionserver.querymatcher.StripeCompactionScanQueryMatcher(org.apache.hadoop.hbase.regionserver.ScanInfo, org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker, org.apache.hadoop.hbase.regionserver.querymatcher.ColumnTracker, long, long, long, long, byte[], byte[]);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher$MatchCode match(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher$1.class;;;class org.apache.hadoop.hbase.regionserver.querymatcher.UserScanQueryMatcher$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a nested class object within the UserScanQueryMatcher class in the HBase package.;;;N
org/apache/hadoop/hbase/regionserver/querymatcher/UserScanQueryMatcher.class;;;public abstract class org.apache.hadoop.hbase.regionserver.querymatcher.UserScanQueryMatcher extends org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher {\n  public boolean hasNullColumnInQuery();\n  public boolean isUserScan();\n  public org.apache.hadoop.hbase.filter.Filter getFilter();\n  public org.apache.hadoop.hbase.Cell getNextKeyHint(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public void beforeShipped() throws java.io.IOException;\n  public boolean moreRowsMayExistAfter(org.apache.hadoop.hbase.Cell);\n  public static org.apache.hadoop.hbase.regionserver.querymatcher.UserScanQueryMatcher create(org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.ScanInfo, java.util.NavigableSet<byte[]>, long, long, org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost) throws java.io.IOException;\n}\n;;;No. This is a class definition, not a message definition. It is a blueprint for creating objects, not a specific message to be sent on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/regionreplication/RegionReplicationBufferManager.class;;;public class org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationBufferManager {\n  public static final java.lang.String MAX_PENDING_SIZE;\n  public static final long MAX_PENDING_SIZE_DEFAULT;\n  public static final java.lang.String SOFT_LIMIT_PERCENTAGE;\n  public static final float SOFT_LIMIT_PERCENTAGE_DEFAULT;\n  public org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationBufferManager(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public boolean increase(long);\n  public void decrease(long);\n  public void stop();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/regionreplication/RegionReplicationFlushRequester.class;;;class org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationFlushRequester {\n  public static final java.lang.String MIN_INTERVAL_SECS;\n  public static final int MIN_INTERVAL_SECS_DEFAULT;\n}\n;;;No. This class only contains static constant definitions and does not define any message or data structure.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/regionreplication/RegionReplicationSink$SinkEntry.class;;;final class org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationSink$SinkEntry {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/regionreplication/RegionReplicationSink.class;;;public class org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationSink {\n  public static final java.lang.String RETRIES_NUMBER;\n  public static final int RETRIES_NUMBER_DEFAULT;\n  public static final java.lang.String RPC_TIMEOUT_MS;\n  public static final long RPC_TIMEOUT_MS_DEFAULT;\n  public static final java.lang.String OPERATION_TIMEOUT_MS;\n  public static final long OPERATION_TIMEOUT_MS_DEFAULT;\n  public static final java.lang.String META_EDIT_RPC_TIMEOUT_MS;\n  public static final long META_EDIT_RPC_TIMEOUT_MS_DEFAULT;\n  public static final java.lang.String META_EDIT_OPERATION_TIMEOUT_MS;\n  public static final long META_EDIT_OPERATION_TIMEOUT_MS_DEFAULT;\n  public static final java.lang.String BATCH_SIZE_CAPACITY;\n  public static final long BATCH_SIZE_CAPACITY_DEFAULT;\n  public static final java.lang.String BATCH_COUNT_CAPACITY;\n  public static final int BATCH_COUNT_CAPACITY_DEFAULT;\n  public org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationSink(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationBufferManager, java.lang.Runnable, org.apache.hadoop.hbase.client.AsyncClusterConnection);\n  public void add(org.apache.hadoop.hbase.wal.WALKeyImpl, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.ipc.ServerCall<?>);\n  public void stop();\n  public void waitUntilStopped() throws java.lang.InterruptedException;\n}\n;;;No. It is a class definition for a RegionReplicationSink in HBase, but it does not contain a clear definition for a message format that could be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure$RegionSnapshotTask.class;;;public class org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure$RegionSnapshotTask implements java.util.concurrent.Callable<java.lang.Void> {\n  public org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure$RegionSnapshotTask(org.apache.hadoop.hbase.regionserver.HRegion, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, boolean, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher);\n  public java.lang.Void call() throws java.lang.Exception;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No, it is not a message definition.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/snapshot/FlushSnapshotSubprocedure.class;;;public class org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure extends org.apache.hadoop.hbase.procedure.Subprocedure {\n  public org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure(org.apache.hadoop.hbase.procedure.ProcedureMember, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, long, long, java.util.List<org.apache.hadoop.hbase.regionserver.HRegion>, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedurePool);\n  public void acquireBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public byte[] insideBarrier() throws org.apache.hadoop.hbase.errorhandling.ForeignException;\n  public void cleanup(java.lang.Exception);\n  public void releaseBarrier();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager$1.class;;;class org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class definition for a nested class within the RegionServerSnapshotManager class in Apache HBase.;;;N
org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager$SnapshotSubprocedureBuilder.class;;;public class org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedureBuilder implements org.apache.hadoop.hbase.procedure.SubprocedureFactory {\n  public org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedureBuilder(org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager);\n  public org.apache.hadoop.hbase.procedure.Subprocedure buildSubprocedure(java.lang.String, byte[]);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager$SnapshotSubprocedurePool.class;;;class org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager$SnapshotSubprocedurePool {\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class that defines a pool for managing subprocedures in the HBase region server snapshot functionality. It would not be put on a task queue directly.;;;N
org/apache/hadoop/hbase/regionserver/snapshot/RegionServerSnapshotManager.class;;;public class org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager extends org.apache.hadoop.hbase.procedure.RegionServerProcedureManager {\n  public static final java.lang.String SNAPSHOT_REQUEST_THREADS_KEY;\n  public static final int SNAPSHOT_REQUEST_THREADS_DEFAULT;\n  public static final java.lang.String SNAPSHOT_TIMEOUT_MILLIS_KEY;\n  public static final long SNAPSHOT_TIMEOUT_MILLIS_DEFAULT;\n  public static final java.lang.String SNAPSHOT_REQUEST_WAKE_MILLIS_KEY;\n  public org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager();\n  public void start();\n  public void stop(boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.procedure.Subprocedure buildSubprocedure(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription);\n  public void initialize(org.apache.hadoop.hbase.regionserver.RegionServerServices) throws org.apache.zookeeper.KeeperException;\n  public java.lang.String getProcedureSignature();\n}\n;;;No, this class is not a message definition. It contains methods and properties for managing HBase snapshots on the region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/DefaultStoreFileTracker.class;;;class org.apache.hadoop.hbase.regionserver.storefiletracker.DefaultStoreFileTracker extends org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerBase {\n  public org.apache.hadoop.hbase.regionserver.storefiletracker.DefaultStoreFileTracker(org.apache.hadoop.conf.Configuration, boolean, org.apache.hadoop.hbase.regionserver.StoreContext);\n  public boolean requireWritingToTmpDirFirst();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/FileBasedStoreFileTracker.class;;;class org.apache.hadoop.hbase.regionserver.storefiletracker.FileBasedStoreFileTracker extends org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerBase {\n  public org.apache.hadoop.hbase.regionserver.storefiletracker.FileBasedStoreFileTracker(org.apache.hadoop.conf.Configuration, boolean, org.apache.hadoop.hbase.regionserver.StoreContext);\n  public boolean requireWritingToTmpDirFirst();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/InitializeStoreFileTrackerProcedure.class;;;public class org.apache.hadoop.hbase.regionserver.storefiletracker.InitializeStoreFileTrackerProcedure extends org.apache.hadoop.hbase.master.procedure.ModifyTableDescriptorProcedure {\n  public org.apache.hadoop.hbase.regionserver.storefiletracker.InitializeStoreFileTrackerProcedure();\n  public org.apache.hadoop.hbase.regionserver.storefiletracker.InitializeStoreFileTrackerProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/MigrationStoreFileTracker.class;;;class org.apache.hadoop.hbase.regionserver.storefiletracker.MigrationStoreFileTracker extends org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerBase {\n  public static final java.lang.String SRC_IMPL;\n  public static final java.lang.String DST_IMPL;\n  public org.apache.hadoop.hbase.regionserver.storefiletracker.MigrationStoreFileTracker(org.apache.hadoop.conf.Configuration, boolean, org.apache.hadoop.hbase.regionserver.StoreContext);\n  public boolean requireWritingToTmpDirFirst();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyColumnFamilyStoreFileTrackerProcedure.class;;;public class org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyColumnFamilyStoreFileTrackerProcedure extends org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure {\n  public org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyColumnFamilyStoreFileTrackerProcedure();\n  public org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyColumnFamilyStoreFileTrackerProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, byte[], java.lang.String) throws org.apache.hadoop.hbase.HBaseIOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyStoreFileTrackerProcedure$1.class;;;class org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure$1 {\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyStoreFileTrackerProcedure$StoreFileTrackerState.class;;;final class org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure$StoreFileTrackerState extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure$StoreFileTrackerState> {\n  public static final org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure$StoreFileTrackerState NEED_FINISH_PREVIOUS_MIGRATION_FIRST;\n  public static final org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure$StoreFileTrackerState NEED_START_MIGRATION;\n  public static final org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure$StoreFileTrackerState NEED_FINISH_MIGRATION;\n  public static final org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure$StoreFileTrackerState ALREADY_FINISHED;\n  public static org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure$StoreFileTrackerState[] values();\n  public static org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure$StoreFileTrackerState valueOf(java.lang.String);\n}\n;;;No. This class is an enumeration used by another class, and would not be placed on a message queue directly.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyStoreFileTrackerProcedure.class;;;public abstract class org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure extends org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure<org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos$ModifyStoreFileTrackerState> {\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public org.apache.hadoop.hbase.master.procedure.TableProcedureInterface$TableOperationType getTableOperationType();\n}\n;;;No.;;;N;;;It is not a task definition.;;;?
org/apache/hadoop/hbase/regionserver/storefiletracker/ModifyTableStoreFileTrackerProcedure.class;;;public class org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyTableStoreFileTrackerProcedure extends org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyStoreFileTrackerProcedure {\n  public org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyTableStoreFileTrackerProcedure();\n  public org.apache.hadoop.hbase.regionserver.storefiletracker.ModifyTableStoreFileTrackerProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName, java.lang.String) throws org.apache.hadoop.hbase.HBaseIOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileListFile.class;;;class org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileListFile {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileTracker.class;;;public interface org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTracker {\n  public abstract java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileInfo> load() throws java.io.IOException;\n  public abstract void add(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo>) throws java.io.IOException;\n  public abstract void replace(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo>, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo>) throws java.io.IOException;\n  public abstract void set(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileInfo>) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter(org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.TableDescriptorBuilder updateWithTrackerConfigs(org.apache.hadoop.hbase.client.TableDescriptorBuilder);\n  public abstract boolean requireWritingToTmpDirFirst();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileTrackerBase.class;;;abstract class org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerBase implements org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTracker {\n  public final java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileInfo> load() throws java.io.IOException;\n  public final void add(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo>) throws java.io.IOException;\n  public final void replace(java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo>, java.util.Collection<org.apache.hadoop.hbase.regionserver.StoreFileInfo>) throws java.io.IOException;\n  public final void set(java.util.List<org.apache.hadoop.hbase.regionserver.StoreFileInfo>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableDescriptorBuilder updateWithTrackerConfigs(org.apache.hadoop.hbase.client.TableDescriptorBuilder);\n  public final org.apache.hadoop.hbase.regionserver.StoreFileWriter createWriter(org.apache.hadoop.hbase.regionserver.CreateStoreFileWriterParams) throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition. It is a class definition for a StoreFileTracker in HBase.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileTrackerFactory$Trackers.class;;;public final class org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerFactory$Trackers extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerFactory$Trackers> {\n  public static final org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerFactory$Trackers DEFAULT;\n  public static final org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerFactory$Trackers FILE;\n  public static final org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerFactory$Trackers MIGRATION;\n  public static org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerFactory$Trackers[] values();\n  public static org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerFactory$Trackers valueOf(java.lang.String);\n}\n;;;No. This class is not a message definition and is not related to message queuing directly. It is an enum class that defines three constant values and two static methods.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileTrackerFactory.class;;;public final class org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerFactory {\n  public static final java.lang.String TRACKER_IMPL;\n  public static java.lang.String getStoreFileTrackerName(org.apache.hadoop.conf.Configuration);\n  public static java.lang.String getStoreFileTrackerName(java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTracker>);\n  public static java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTracker> getTrackerClass(org.apache.hadoop.conf.Configuration);\n  public static java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTracker> getTrackerClass(java.lang.String);\n  public static org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTracker create(org.apache.hadoop.conf.Configuration, boolean, org.apache.hadoop.hbase.regionserver.StoreContext);\n  public static org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTracker create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, org.apache.hadoop.hbase.regionserver.HRegionFileSystem);\n  public static org.apache.hadoop.hbase.client.TableDescriptor updateWithTrackerConfigs(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor);\n  public static boolean isMigration(java.lang.Class<?>);\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a utility class for creating and managing instances of the StoreFileTracker interface used by the Hadoop HBase region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/storefiletracker/StoreFileTrackerValidationUtils.class;;;public final class org.apache.hadoop.hbase.regionserver.storefiletracker.StoreFileTrackerValidationUtils {\n  public static void checkForCreateTable(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public static void checkForModifyTable(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor, boolean) throws java.io.IOException;\n  public static void validatePreRestoreSnapshot(org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.snapshot.RestoreSnapshotException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class containing static methods to perform certain validations related to store file tracking in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/throttle/CompactionThroughputControllerFactory.class;;;public final class org.apache.hadoop.hbase.regionserver.throttle.CompactionThroughputControllerFactory {\n  public static final java.lang.String HBASE_THROUGHPUT_CONTROLLER_KEY;\n  public static org.apache.hadoop.hbase.regionserver.throttle.ThroughputController create(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration);\n  public static java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.throttle.ThroughputController> getThroughputControllerClass(org.apache.hadoop.conf.Configuration);\n}\n;;;No. This class contains methods that create and retrieve instances of another class (ThroughputController). It does not define a message that would be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/throttle/FlushThroughputControllerFactory.class;;;public final class org.apache.hadoop.hbase.regionserver.throttle.FlushThroughputControllerFactory {\n  public static final java.lang.String HBASE_FLUSH_THROUGHPUT_CONTROLLER_KEY;\n  public static org.apache.hadoop.hbase.regionserver.throttle.ThroughputController create(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration);\n  public static java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.throttle.ThroughputController> getThroughputControllerClass(org.apache.hadoop.conf.Configuration);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/throttle/NoLimitThroughputController.class;;;public class org.apache.hadoop.hbase.regionserver.throttle.NoLimitThroughputController implements org.apache.hadoop.hbase.regionserver.throttle.ThroughputController {\n  public static final org.apache.hadoop.hbase.regionserver.throttle.NoLimitThroughputController INSTANCE;\n  public org.apache.hadoop.hbase.regionserver.throttle.NoLimitThroughputController();\n  public void setup(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public void start(java.lang.String);\n  public long control(java.lang.String, long) throws java.lang.InterruptedException;\n  public void finish(java.lang.String);\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public java.lang.String toString();\n}\n;;;No, it is a Java class definition that implements the "ThroughputController" interface, but it is not explicitly designed as a message definition to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/throttle/PressureAwareCompactionThroughputController$1.class;;;class org.apache.hadoop.hbase.regionserver.throttle.PressureAwareCompactionThroughputController$1 extends org.apache.hadoop.hbase.ScheduledChore {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/throttle/PressureAwareCompactionThroughputController.class;;;public class org.apache.hadoop.hbase.regionserver.throttle.PressureAwareCompactionThroughputController extends org.apache.hadoop.hbase.regionserver.throttle.PressureAwareThroughputController {\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_MAX_THROUGHPUT_HIGHER_BOUND;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_MAX_THROUGHPUT_LOWER_BOUND;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_MAX_THROUGHPUT_OFFPEAK;\n  public static final java.lang.String HBASE_HSTORE_COMPACTION_THROUGHPUT_TUNE_PERIOD;\n  public org.apache.hadoop.hbase.regionserver.throttle.PressureAwareCompactionThroughputController();\n  public void setup(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/throttle/PressureAwareFlushThroughputController$1.class;;;class org.apache.hadoop.hbase.regionserver.throttle.PressureAwareFlushThroughputController$1 extends org.apache.hadoop.hbase.ScheduledChore {\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/throttle/PressureAwareFlushThroughputController.class;;;public class org.apache.hadoop.hbase.regionserver.throttle.PressureAwareFlushThroughputController extends org.apache.hadoop.hbase.regionserver.throttle.PressureAwareThroughputController {\n  public static final java.lang.String HBASE_HSTORE_FLUSH_MAX_THROUGHPUT_UPPER_BOUND;\n  public static final java.lang.String HBASE_HSTORE_FLUSH_MAX_THROUGHPUT_LOWER_BOUND;\n  public static final java.lang.String HBASE_HSTORE_FLUSH_THROUGHPUT_TUNE_PERIOD;\n  public static final java.lang.String HBASE_HSTORE_FLUSH_THROUGHPUT_CONTROL_CHECK_INTERVAL;\n  public org.apache.hadoop.hbase.regionserver.throttle.PressureAwareFlushThroughputController();\n  public void setup(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public java.lang.String toString();\n}\n;;;No, it is a class definition for a throughput controller in the HBase region server. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController$ActiveOperation.class;;;final class org.apache.hadoop.hbase.regionserver.throttle.PressureAwareThroughputController$ActiveOperation {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/throttle/PressureAwareThroughputController.class;;;public abstract class org.apache.hadoop.hbase.regionserver.throttle.PressureAwareThroughputController extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.hbase.regionserver.throttle.ThroughputController,org.apache.hadoop.hbase.Stoppable {\n  public org.apache.hadoop.hbase.regionserver.throttle.PressureAwareThroughputController();\n  public abstract void setup(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public void start(java.lang.String);\n  public long control(java.lang.String, long) throws java.lang.InterruptedException;\n  public void finish(java.lang.String);\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public double getMaxThroughput();\n  public void setMaxThroughput(double);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.class;;;public class org.apache.hadoop.hbase.regionserver.throttle.StoreHotnessProtector {\n  public static final java.lang.String PARALLEL_PUT_STORE_THREADS_LIMIT;\n  public static final java.lang.String PARALLEL_PREPARE_PUT_STORE_MULTIPLIER;\n  public static final java.lang.String PARALLEL_PUT_STORE_THREADS_LIMIT_MIN_COLUMN_COUNT;\n  public static final long FIXED_SIZE;\n  public org.apache.hadoop.hbase.regionserver.throttle.StoreHotnessProtector(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.conf.Configuration);\n  public void init(org.apache.hadoop.conf.Configuration);\n  public void update(org.apache.hadoop.conf.Configuration);\n  public void start(java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.Cell>>) throws org.apache.hadoop.hbase.RegionTooBusyException;\n  public void finish(java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.Cell>>);\n  public java.lang.String toString();\n  public boolean isEnable();\n}\n;;;No, this class is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/throttle/ThroughputControlUtil.class;;;public final class org.apache.hadoop.hbase.regionserver.throttle.ThroughputControlUtil {\n  public static java.lang.String getNameForThrottling(org.apache.hadoop.hbase.regionserver.HStore, java.lang.String);\n}\n;;;No. This is a utility class that provides a method for getting a name for throttling, but it is not a message definition that would be placed on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a utility class with a static method.;;;N
org/apache/hadoop/hbase/regionserver/throttle/ThroughputController.class;;;public interface org.apache.hadoop.hbase.regionserver.throttle.ThroughputController extends org.apache.hadoop.hbase.Stoppable {\n  public abstract void setup(org.apache.hadoop.hbase.regionserver.RegionServerServices);\n  public abstract void start(java.lang.String);\n  public abstract long control(java.lang.String, long) throws java.lang.InterruptedException;\n  public abstract void finish(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL$1.class;;;class org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL$1 implements org.apache.hadoop.fs.PathFilter {\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL$2.class;;;class org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL$2 implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call() throws java.lang.Exception;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL$WalProps.class;;;final class org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL$WalProps {\n  public final java.util.Map<byte[], java.lang.Long> encodedName2HighestSequenceId;\n  public final long logSize;\n  public final long rollTimeNs;\n  public org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL$WalProps(java.util.Map<byte[], java.lang.Long>, long);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/AbstractFSWAL.class;;;public abstract class org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL<W extends org.apache.hadoop.hbase.wal.WALProvider$WriterBase> implements org.apache.hadoop.hbase.wal.WAL {\n  public static final java.lang.String WAL_ROLL_MULTIPLIER;\n  public static final java.lang.String MAX_LOGS;\n  public static final java.lang.String RING_BUFFER_SLOT_COUNT;\n  public static final java.lang.String WAL_SHUTDOWN_WAIT_TIMEOUT_MS;\n  public static final int DEFAULT_WAL_SHUTDOWN_WAIT_TIMEOUT_MS;\n  public long getFilenum();\n  public void init() throws java.io.IOException;\n  public void registerWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n  public boolean unregisterWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n  public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost getCoprocessorHost();\n  public java.lang.Long startCacheFlush(byte[], java.util.Set<byte[]>);\n  public java.lang.Long startCacheFlush(byte[], java.util.Map<byte[], java.lang.Long>);\n  public void completeCacheFlush(byte[], long);\n  public void abortCacheFlush(byte[]);\n  public long getEarliestMemStoreSeqNum(byte[]);\n  public long getEarliestMemStoreSeqNum(byte[], byte[]);\n  public java.util.Map<byte[], java.util.List<byte[]>> rollWriter() throws org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException, java.io.IOException;\n  public final void sync() throws java.io.IOException;\n  public final void sync(long) throws java.io.IOException;\n  public final void sync(boolean) throws java.io.IOException;\n  public final void sync(long, boolean) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getCurrentFileName();\n  public int getNumRolledLogFiles();\n  public int getNumLogFiles();\n  public static org.apache.hadoop.fs.Path getWALArchivePath(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public java.util.Map<byte[], java.util.List<byte[]>> rollWriter(boolean) throws java.io.IOException;\n  public long getLogFileSize();\n  public void requestLogRoll();\n  public void shutdown() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public int getInflightWALCloseCount();\n  public void updateStore(byte[], byte[], java.lang.Long, boolean);\n  public java.lang.String toString();\n  public java.util.OptionalLong getLogFileSizeIfBeingWritten(org.apache.hadoop.fs.Path);\n  public long appendData(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKeyImpl, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public long appendMarker(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKeyImpl, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void checkLogLowReplication(long);\n  public static void main(java.lang.String[]) throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/AbstractProtobufLogWriter.class;;;public abstract class org.apache.hadoop.hbase.regionserver.wal.AbstractProtobufLogWriter {\n  public org.apache.hadoop.hbase.regionserver.wal.AbstractProtobufLogWriter();\n  public void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, boolean, long, org.apache.hadoop.hbase.io.asyncfs.monitor.StreamSlowMonitor) throws java.io.IOException, org.apache.hadoop.hbase.util.CommonFSUtils$StreamLacksCapabilityException;\n  public long getLength();\n}\n;;;No. It is a class definition for an abstract class that defines methods for writing logs in HBase. It cannot be directly put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL$1.class;;;class org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL$1 {\n}\n;;;No. It is a class definition and not a message definition that can be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/wal/AsyncFSWAL.class;;;public class org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL extends org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL<org.apache.hadoop.hbase.wal.WALProvider$AsyncWriter> {\n  public static final java.lang.String WAL_BATCH_SIZE;\n  public static final long DEFAULT_WAL_BATCH_SIZE;\n  public static final java.lang.String ASYNC_WAL_USE_SHARED_EVENT_LOOP;\n  public static final boolean DEFAULT_ASYNC_WAL_USE_SHARED_EVENT_LOOP;\n  public static final java.lang.String ASYNC_WAL_WAIT_ON_SHUTDOWN_IN_SECONDS;\n  public static final int DEFAULT_ASYNC_WAL_WAIT_ON_SHUTDOWN_IN_SECONDS;\n  public org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, boolean, java.lang.String, java.lang.String, org.apache.hbase.thirdparty.io.netty.channel.EventLoopGroup, java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.Channel>) throws org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException, java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.Abortable, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, boolean, java.lang.String, java.lang.String, org.apache.hbase.thirdparty.io.netty.channel.EventLoopGroup, java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.Channel>, org.apache.hadoop.hbase.io.asyncfs.monitor.StreamSlowMonitor) throws org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException, java.io.IOException;\n}\n;;;No. This is a class definition for a Java class that is part of the Hadoop ecosystem. It is not a message definition that is meant to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/AsyncProtobufLogWriter$OutputStreamWrapper.class;;;final class org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter$OutputStreamWrapper extends java.io.OutputStream implements org.apache.hadoop.hbase.io.ByteBufferWriter {\n  public void write(int) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter$OutputStreamWrapper(org.apache.hadoop.hbase.io.asyncfs.AsyncFSOutput);\n  public void write(java.nio.ByteBuffer, int, int) throws java.io.IOException;\n  public void writeInt(int) throws java.io.IOException;\n  public void write(byte[], int, int) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n;;;No.;;;N;;;No. It is a class definition for an OutputStreamWrapper in the Hadoop HBase library. It is not a specific task that can be executed on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/wal/AsyncProtobufLogWriter.class;;;public class org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter extends org.apache.hadoop.hbase.regionserver.wal.AbstractProtobufLogWriter implements org.apache.hadoop.hbase.wal.AsyncFSWALProvider$AsyncWriter {\n  public org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter(org.apache.hbase.thirdparty.io.netty.channel.EventLoopGroup, java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.Channel>);\n  public void append(org.apache.hadoop.hbase.wal.WAL$Entry);\n  public java.util.concurrent.CompletableFuture<java.lang.Long> sync(boolean);\n  public synchronized void close() throws java.io.IOException;\n  public org.apache.hadoop.hbase.io.asyncfs.AsyncFSOutput getOutput();\n  public long getSyncedLength();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/CombinedAsyncWriter.class;;;public final class org.apache.hadoop.hbase.regionserver.wal.CombinedAsyncWriter implements org.apache.hadoop.hbase.wal.WALProvider$AsyncWriter {\n  public long getLength();\n  public long getSyncedLength();\n  public void close() throws java.io.IOException;\n  public void append(org.apache.hadoop.hbase.wal.WAL$Entry);\n  public java.util.concurrent.CompletableFuture<java.lang.Long> sync(boolean);\n  public static org.apache.hadoop.hbase.regionserver.wal.CombinedAsyncWriter create(org.apache.hadoop.hbase.wal.WALProvider$AsyncWriter, org.apache.hadoop.hbase.wal.WALProvider$AsyncWriter...);\n}\n;;;No, this class is not a message definition. It is a Java class that includes methods for writing data to a write-ahead log (WAL) in the HBase distributed database system. While messages may be placed on a message queue and passed between different components in HBase, this class itself is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/CompressionContext$DictionaryIndex.class;;;public final class org.apache.hadoop.hbase.regionserver.wal.CompressionContext$DictionaryIndex extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.wal.CompressionContext$DictionaryIndex> {\n  public static final org.apache.hadoop.hbase.regionserver.wal.CompressionContext$DictionaryIndex REGION;\n  public static final org.apache.hadoop.hbase.regionserver.wal.CompressionContext$DictionaryIndex TABLE;\n  public static final org.apache.hadoop.hbase.regionserver.wal.CompressionContext$DictionaryIndex FAMILY;\n  public static final org.apache.hadoop.hbase.regionserver.wal.CompressionContext$DictionaryIndex QUALIFIER;\n  public static final org.apache.hadoop.hbase.regionserver.wal.CompressionContext$DictionaryIndex ROW;\n  public static org.apache.hadoop.hbase.regionserver.wal.CompressionContext$DictionaryIndex[] values();\n  public static org.apache.hadoop.hbase.regionserver.wal.CompressionContext$DictionaryIndex valueOf(java.lang.String);\n}\n;;;No, this is not a message definition that might be put on a message queue. It appears to be an enum definition for a compression context dictionary index in the Apache HBase region server.;;;N;;;No, it is not a task definition. It represents an enumeration with various values representing different types of dictionary indices used for compression. It is not related to any specific task or operation.;;;N
org/apache/hadoop/hbase/regionserver/wal/CompressionContext$ValueCompressor.class;;;class org.apache.hadoop.hbase.regionserver.wal.CompressionContext$ValueCompressor {\n  public org.apache.hadoop.hbase.regionserver.wal.CompressionContext$ValueCompressor(org.apache.hadoop.hbase.io.compress.Compression$Algorithm);\n  public org.apache.hadoop.hbase.io.compress.Compression$Algorithm getAlgorithm();\n  public byte[] compress(byte[], int, int) throws java.io.IOException;\n  public int decompress(java.io.InputStream, int, byte[], int, int) throws java.io.IOException;\n  public void clear();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/CompressionContext.class;;;public class org.apache.hadoop.hbase.regionserver.wal.CompressionContext {\n  public static final java.lang.String ENABLE_WAL_TAGS_COMPRESSION;\n  public static final java.lang.String ENABLE_WAL_VALUE_COMPRESSION;\n  public static final java.lang.String WAL_VALUE_COMPRESSION_TYPE;\n  public org.apache.hadoop.hbase.regionserver.wal.CompressionContext(java.lang.Class<? extends org.apache.hadoop.hbase.io.util.Dictionary>, boolean, boolean, boolean, org.apache.hadoop.hbase.io.compress.Compression$Algorithm) throws java.lang.SecurityException, java.lang.NoSuchMethodException, java.lang.InstantiationException, java.lang.IllegalAccessException, java.lang.reflect.InvocationTargetException, java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.wal.CompressionContext(java.lang.Class<? extends org.apache.hadoop.hbase.io.util.Dictionary>, boolean, boolean) throws java.lang.SecurityException, java.lang.NoSuchMethodException, java.lang.InstantiationException, java.lang.IllegalAccessException, java.lang.reflect.InvocationTargetException, java.io.IOException;\n  public boolean hasTagCompression();\n  public boolean hasValueCompression();\n  public org.apache.hadoop.hbase.io.util.Dictionary getDictionary(java.lang.Enum);\n  public org.apache.hadoop.hbase.regionserver.wal.CompressionContext$ValueCompressor getValueCompressor();\n  public static org.apache.hadoop.hbase.io.compress.Compression$Algorithm getValueCompressionAlgorithm(org.apache.hadoop.conf.Configuration);\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a helper class for managing compression in HBase's write-ahead log.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/wal/Compressor.class;;;public class org.apache.hadoop.hbase.regionserver.wal.Compressor {\n  public org.apache.hadoop.hbase.regionserver.wal.Compressor();\n  public static void main(java.lang.String[]) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/DamagedWALException.class;;;public class org.apache.hadoop.hbase.regionserver.wal.DamagedWALException extends org.apache.hadoop.hbase.HBaseIOException {\n  public org.apache.hadoop.hbase.regionserver.wal.DamagedWALException();\n  public org.apache.hadoop.hbase.regionserver.wal.DamagedWALException(java.lang.String);\n  public org.apache.hadoop.hbase.regionserver.wal.DamagedWALException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.hbase.regionserver.wal.DamagedWALException(java.lang.Throwable);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/DualAsyncFSWAL.class;;;public class org.apache.hadoop.hbase.regionserver.wal.DualAsyncFSWAL extends org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL {\n  public org.apache.hadoop.hbase.regionserver.wal.DualAsyncFSWAL(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, boolean, java.lang.String, java.lang.String, org.apache.hbase.thirdparty.io.netty.channel.EventLoopGroup, java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.Channel>) throws org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException, java.io.IOException;\n  public void skipRemoteWAL(boolean);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/wal/FSHLog$RingBufferEventHandler.class;;;class org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler implements com.lmax.disruptor.EventHandler<org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck>, com.lmax.disruptor.LifecycleAware {\n  public void onEvent(org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck, long, boolean) throws java.lang.Exception;\n  public void onStart();\n  public void onShutdown();\n  public void onEvent(java.lang.Object, long, boolean) throws java.lang.Exception;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/FSHLog$RingBufferExceptionHandler.class;;;class org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferExceptionHandler implements com.lmax.disruptor.ExceptionHandler<org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck> {\n  public void handleEventException(java.lang.Throwable, long, org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck);\n  public void handleOnStartException(java.lang.Throwable);\n  public void handleOnShutdownException(java.lang.Throwable);\n  public void handleEventException(java.lang.Throwable, long, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/FSHLog$SafePointZigZagLatch.class;;;class org.apache.hadoop.hbase.regionserver.wal.FSHLog$SafePointZigZagLatch {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/FSHLog$SyncRunner.class;;;class org.apache.hadoop.hbase.regionserver.wal.FSHLog$SyncRunner extends java.lang.Thread {\n  public void run();\n}\n;;;No. This is a class definition for a Java thread, not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/FSHLog.class;;;public class org.apache.hadoop.hbase.regionserver.wal.FSHLog extends org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL<org.apache.hadoop.hbase.wal.WALProvider$Writer> {\n  public static final long FIXED_OVERHEAD;\n  public org.apache.hadoop.hbase.regionserver.wal.FSHLog(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.wal.FSHLog(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.Abortable, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.wal.FSHLog(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, boolean, java.lang.String, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.wal.FSHLog(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.Abortable, org.apache.hadoop.fs.Path, java.lang.String, java.lang.String, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>, boolean, java.lang.String, java.lang.String) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition for a Hadoop HBase region server write-ahead log implementation.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/FSWALEntry.class;;;class org.apache.hadoop.hbase.regionserver.wal.FSWALEntry extends org.apache.hadoop.hbase.wal.WAL$Entry {\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/MetricsWAL$1.class;;;class org.apache.hadoop.hbase.regionserver.wal.MetricsWAL$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/MetricsWAL.class;;;public class org.apache.hadoop.hbase.regionserver.wal.MetricsWAL implements org.apache.hadoop.hbase.regionserver.wal.WALActionsListener {\n  public org.apache.hadoop.hbase.regionserver.wal.MetricsWAL();\n  public void postSync(long, int);\n  public void postAppend(long, long, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void logRollRequested(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason);\n  public void postLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader$WALHdrContext.class;;;class org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrContext {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader$WALHdrResult.class;;;final class org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult> {\n  public static final org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult EOF;\n  public static final org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult SUCCESS;\n  public static final org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult UNKNOWN_WRITER_CLS;\n  public static org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult[] values();\n  public static org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader$WALHdrResult valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/ProtobufLogReader.class;;;public class org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader extends org.apache.hadoop.hbase.regionserver.wal.ReaderBase {\n  public static final byte[] PB_WAL_MAGIC;\n  public static final byte[] PB_WAL_COMPLETE_MAGIC;\n  public long trailerSize();\n  public org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader();\n  public void close() throws java.io.IOException;\n  public long getPosition() throws java.io.IOException;\n  public void reset() throws java.io.IOException;\n  public void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataInputStream) throws java.io.IOException;\n  public java.util.List<java.lang.String> getWriterClsNames();\n  public java.lang.String getCodecClsName();\n}\n;;;No, the class does not define any message structures or fields. It provides methods for reading and manipulating protobuf log files.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/ProtobufLogWriter.class;;;public class org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter extends org.apache.hadoop.hbase.regionserver.wal.AbstractProtobufLogWriter implements org.apache.hadoop.hbase.wal.FSHLogProvider$Writer {\n  public org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter();\n  public void append(org.apache.hadoop.hbase.wal.WAL$Entry) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void sync(boolean) throws java.io.IOException;\n  public long getSyncedLength();\n  public org.apache.hadoop.fs.FSDataOutputStream getStream();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/ReaderBase.class;;;public abstract class org.apache.hadoop.hbase.regionserver.wal.ReaderBase implements org.apache.hadoop.hbase.wal.AbstractFSWALProvider$Reader {\n  public org.apache.hadoop.hbase.regionserver.wal.ReaderBase();\n  public void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataInputStream) throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WAL$Entry next() throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WAL$Entry next(org.apache.hadoop.hbase.wal.WAL$Entry) throws java.io.IOException;\n  public void seek(long) throws java.io.IOException;\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is an abstract class that provides implementation details for reading data from a specific type of file (WAL, or Write Ahead Log) in the HBase database system.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/wal/RingBufferTruck$Type.class;;;public final class org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck$Type extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck$Type> {\n  public static final org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck$Type APPEND;\n  public static final org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck$Type SYNC;\n  public static final org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck$Type EMPTY;\n  public static org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck$Type[] values();\n  public static org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck$Type valueOf(java.lang.String);\n}\n;;;No. This is a class definition for an enumerated type in Apache HBase, but it is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition. It is a Java class representing an enumerated type with three possible values. It does not define any methods or logic that would correspond to a task.;;;N
org/apache/hadoop/hbase/regionserver/wal/RingBufferTruck.class;;;final class org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck {\n}\n;;;No, it is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SecureAsyncProtobufLogWriter.class;;;public class org.apache.hadoop.hbase.regionserver.wal.SecureAsyncProtobufLogWriter extends org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter {\n  public org.apache.hadoop.hbase.regionserver.wal.SecureAsyncProtobufLogWriter(org.apache.hbase.thirdparty.io.netty.channel.EventLoopGroup, java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.Channel>);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogReader.class;;;public class org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader extends org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader {\n  public org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader();\n  public java.util.List<java.lang.String> getWriterClsNames();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SecureProtobufLogWriter.class;;;public class org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter extends org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter {\n  public org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec$EncryptedKvDecoder.class;;;class org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvDecoder extends org.apache.hadoop.hbase.codec.KeyValueCodecWithTags$KeyValueDecoder {\n  public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvDecoder(java.io.InputStream);\n  public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvDecoder(java.io.InputStream, org.apache.hadoop.hbase.io.crypto.Decryptor);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec$EncryptedKvEncoder$1.class;;;class org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder$1 extends java.lang.ThreadLocal<byte[]> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec$EncryptedKvEncoder.class;;;class org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder extends org.apache.hadoop.hbase.codec.KeyValueCodecWithTags$KeyValueEncoder {\n  public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder(java.io.OutputStream);\n  public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec$EncryptedKvEncoder(java.io.OutputStream, org.apache.hadoop.hbase.io.crypto.Encryptor);\n  public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SecureWALCellCodec.class;;;public class org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec extends org.apache.hadoop.hbase.regionserver.wal.WALCellCodec {\n  public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.wal.CompressionContext);\n  public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Encryptor);\n  public org.apache.hadoop.hbase.regionserver.wal.SecureWALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Decryptor);\n  public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(java.io.InputStream);\n  public org.apache.hadoop.hbase.codec.Codec$Encoder getEncoder(java.io.OutputStream);\n  public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec getCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Encryptor);\n  public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec getCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.crypto.Decryptor);\n}\n;;;No. This class provides methods for encoding and decoding data in a secure manner, but it is not a definition for a message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SequenceIdAccounting.class;;;class org.apache.hadoop.hbase.regionserver.wal.SequenceIdAccounting {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SyncFuture.class;;;class org.apache.hadoop.hbase.regionserver.wal.SyncFuture {\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/SyncFutureCache.class;;;public final class org.apache.hadoop.hbase.regionserver.wal.SyncFutureCache {\n  public org.apache.hadoop.hbase.regionserver.wal.SyncFutureCache(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.regionserver.wal.SyncFuture getIfPresentOrNew();\n  public void offer(org.apache.hadoop.hbase.regionserver.wal.SyncFuture);\n  public void clear();\n}\n;;;No. It is a class definition for a cache in HBase. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALActionsListener$RollRequestReason.class;;;public final class org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason extends java.lang.Enum<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason> {\n  public static final org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason SIZE;\n  public static final org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason LOW_REPLICATION;\n  public static final org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason SLOW_SYNC;\n  public static final org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason ERROR;\n  public static org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason[] values();\n  public static org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason valueOf(java.lang.String);\n}\n;;;No;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALActionsListener.class;;;public interface org.apache.hadoop.hbase.regionserver.wal.WALActionsListener {\n  public default void preLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public default void postLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public default void preLogArchive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public default void postLogArchive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public default void logRollRequested(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason);\n  public default void logCloseRequested();\n  public default void visitLogEntryBeforeWrite(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit);\n  public default void postAppend(long, long, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public default void postSync(long, int);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec$BaosAndCompressor.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$BaosAndCompressor extends java.io.ByteArrayOutputStream implements org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor {\n  public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$BaosAndCompressor(org.apache.hadoop.hbase.regionserver.wal.CompressionContext);\n  public org.apache.hbase.thirdparty.com.google.protobuf.ByteString toByteString();\n  public org.apache.hbase.thirdparty.com.google.protobuf.ByteString compress(byte[], java.lang.Enum) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec$ByteStringCompressor.class;;;public interface org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor {\n  public abstract org.apache.hbase.thirdparty.com.google.protobuf.ByteString compress(byte[], java.lang.Enum) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec$ByteStringUncompressor.class;;;public interface org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor {\n  public abstract byte[] uncompress(org.apache.hbase.thirdparty.com.google.protobuf.ByteString, java.lang.Enum) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec$CompressedKvDecoder.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvDecoder extends org.apache.hadoop.hbase.codec.BaseDecoder {\n  public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvDecoder(java.io.InputStream, org.apache.hadoop.hbase.regionserver.wal.CompressionContext);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec$CompressedKvEncoder.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvEncoder extends org.apache.hadoop.hbase.codec.BaseEncoder {\n  public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$CompressedKvEncoder(java.io.OutputStream, org.apache.hadoop.hbase.regionserver.wal.CompressionContext);\n  public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec$EnsureKvEncoder.class;;;public class org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$EnsureKvEncoder extends org.apache.hadoop.hbase.codec.BaseEncoder {\n  public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$EnsureKvEncoder(java.io.OutputStream);\n  public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec$NoneCompressor.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$NoneCompressor implements org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor {\n  public org.apache.hbase.thirdparty.com.google.protobuf.ByteString compress(byte[], java.lang.Enum);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec$NoneUncompressor.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$NoneUncompressor implements org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor {\n  public byte[] uncompress(org.apache.hbase.thirdparty.com.google.protobuf.ByteString, java.lang.Enum);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec$StatelessUncompressor.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$StatelessUncompressor implements org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor {\n  public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$StatelessUncompressor(org.apache.hadoop.hbase.regionserver.wal.CompressionContext);\n  public byte[] uncompress(org.apache.hbase.thirdparty.com.google.protobuf.ByteString, java.lang.Enum) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCellCodec.class;;;public class org.apache.hadoop.hbase.regionserver.wal.WALCellCodec implements org.apache.hadoop.hbase.codec.Codec {\n  public static final java.lang.String WAL_CELL_CODEC_CLASS_KEY;\n  public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec();\n  public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.wal.CompressionContext);\n  public static java.lang.Class<?> getWALCellCodecClass(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec create(org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.regionserver.wal.CompressionContext) throws java.lang.UnsupportedOperationException;\n  public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.wal.CompressionContext) throws java.lang.UnsupportedOperationException;\n  public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(java.io.InputStream);\n  public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(org.apache.hadoop.hbase.nio.ByteBuff);\n  public org.apache.hadoop.hbase.codec.Codec$Encoder getEncoder(java.io.OutputStream);\n  public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor getByteStringCompressor();\n  public org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor getByteStringUncompressor();\n  public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor getNoneCompressor();\n  public static org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor getNoneUncompressor();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALClosedException.class;;;public class org.apache.hadoop.hbase.regionserver.wal.WALClosedException extends java.io.IOException {\n  public org.apache.hadoop.hbase.regionserver.wal.WALClosedException();\n  public org.apache.hadoop.hbase.regionserver.wal.WALClosedException(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost$1.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$1 extends org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALObserverOperation {\n  public void call(org.apache.hadoop.hbase.coprocessor.WALObserver) throws java.io.IOException;\n  public void call(java.lang.Object) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost$2.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$2 extends org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALObserverOperation {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost$3.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$3 extends org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALObserverOperation {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost$4.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$4 extends org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALObserverOperation {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue, as it is a class definition and not a specific task that needs to be executed.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost$WALEnvironment.class;;;class org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALEnvironment extends org.apache.hadoop.hbase.coprocessor.BaseEnvironment<org.apache.hadoop.hbase.coprocessor.WALCoprocessor> implements org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment {\n  public org.apache.hadoop.hbase.wal.WAL getWAL();\n  public org.apache.hadoop.hbase.metrics.MetricRegistry getMetricRegistryForRegionServer();\n  public void shutdown();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost$WALObserverOperation.class;;;abstract class org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALObserverOperation extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.WALCoprocessor, org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment>.ObserverOperationWithoutResult<org.apache.hadoop.hbase.coprocessor.WALObserver> {\n  public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALObserverOperation(org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/regionserver/wal/WALCoprocessorHost.class;;;public class org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<org.apache.hadoop.hbase.coprocessor.WALCoprocessor, org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment> {\n  public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost(org.apache.hadoop.hbase.wal.WAL, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost$WALEnvironment createEnvironment(org.apache.hadoop.hbase.coprocessor.WALCoprocessor, int, int, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.coprocessor.WALCoprocessor checkAndGetInstance(java.lang.Class<?>) throws java.lang.IllegalAccessException, java.lang.InstantiationException;\n  public void preWALWrite(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void postWALWrite(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void preWALRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void postWALRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.Coprocessor checkAndGetInstance(java.lang.Class) throws java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public org.apache.hadoop.hbase.CoprocessorEnvironment createEnvironment(org.apache.hadoop.hbase.Coprocessor, int, int, org.apache.hadoop.conf.Configuration);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/regionserver/wal/WALUtil.class;;;public class org.apache.hadoop.hbase.regionserver.wal.WALUtil {\n  public static final java.lang.String WAL_BLOCK_SIZE;\n  public static org.apache.hadoop.hbase.wal.WALKeyImpl writeCompactionMarker(org.apache.hadoop.hbase.wal.WAL, java.util.NavigableMap<byte[], java.lang.Integer>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$CompactionDescriptor, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationSink) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WALKeyImpl writeFlushMarker(org.apache.hadoop.hbase.wal.WAL, java.util.NavigableMap<byte[], java.lang.Integer>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$FlushDescriptor, boolean, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationSink) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WALKeyImpl writeRegionEventMarker(org.apache.hadoop.hbase.wal.WAL, java.util.NavigableMap<byte[], java.lang.Integer>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$RegionEventDescriptor, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationSink) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WALKeyImpl writeBulkLoadMarkerAndSync(org.apache.hadoop.hbase.wal.WAL, java.util.NavigableMap<byte[], java.lang.Integer>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$BulkLoadDescriptor, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, org.apache.hadoop.hbase.regionserver.regionreplication.RegionReplicationSink) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WALKeyImpl createWALKey(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, java.util.NavigableMap<byte[], java.lang.Integer>, java.util.Map<java.lang.String, byte[]>);\n  public static long getWALBlockSize(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static long getWALBlockSize(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public static void filterCells(org.apache.hadoop.hbase.wal.WALEdit, java.util.function.Function<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>);\n}\n;;;No, this class is not a message definition. It provides various utility methods for working with Apache HBase Write-Ahead Logs (WALs), but it does not define a message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/BaseReplicationEndpoint.class;;;public abstract class org.apache.hadoop.hbase.replication.BaseReplicationEndpoint extends org.apache.hbase.thirdparty.com.google.common.util.concurrent.AbstractService implements org.apache.hadoop.hbase.replication.ReplicationEndpoint {\n  public static final java.lang.String REPLICATION_WALENTRYFILTER_CONFIG_KEY;\n  public org.apache.hadoop.hbase.replication.BaseReplicationEndpoint();\n  public void init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context) throws java.io.IOException;\n  public void peerConfigUpdated(org.apache.hadoop.hbase.replication.ReplicationPeerConfig);\n  public org.apache.hadoop.hbase.replication.WALEntryFilter getWALEntryfilter();\n  public boolean canReplicateToSameCluster();\n  public boolean isStarting();\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/BulkLoadCellFilter.class;;;public class org.apache.hadoop.hbase.replication.BulkLoadCellFilter {\n  public org.apache.hadoop.hbase.replication.BulkLoadCellFilter();\n  public org.apache.hadoop.hbase.Cell filterCell(org.apache.hadoop.hbase.Cell, org.apache.hbase.thirdparty.com.google.common.base.Predicate<byte[]>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/ChainWALEmptyEntryFilter.class;;;public class org.apache.hadoop.hbase.replication.ChainWALEmptyEntryFilter extends org.apache.hadoop.hbase.replication.ChainWALEntryFilter {\n  public org.apache.hadoop.hbase.replication.ChainWALEmptyEntryFilter(org.apache.hadoop.hbase.replication.WALEntryFilter...);\n  public org.apache.hadoop.hbase.replication.ChainWALEmptyEntryFilter(java.util.List<org.apache.hadoop.hbase.replication.WALEntryFilter>);\n  public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry);\n  public void setFilterEmptyEntry(boolean);\n}\n;;;Yes, it is a message definition for a message queue in Hadoop.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/ChainWALEntryFilter.class;;;public class org.apache.hadoop.hbase.replication.ChainWALEntryFilter implements org.apache.hadoop.hbase.replication.WALEntryFilter {\n  public org.apache.hadoop.hbase.replication.ChainWALEntryFilter(org.apache.hadoop.hbase.replication.WALEntryFilter...);\n  public org.apache.hadoop.hbase.replication.ChainWALEntryFilter(java.util.List<org.apache.hadoop.hbase.replication.WALEntryFilter>);\n  public void initCellFilters();\n  public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/ClusterMarkingEntryFilter.class;;;public class org.apache.hadoop.hbase.replication.ClusterMarkingEntryFilter implements org.apache.hadoop.hbase.replication.WALEntryFilter {\n  public org.apache.hadoop.hbase.replication.ClusterMarkingEntryFilter(java.util.UUID, java.util.UUID, org.apache.hadoop.hbase.replication.ReplicationEndpoint);\n  public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint$PeerRegionServerListener.class;;;public class org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint$PeerRegionServerListener extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint$PeerRegionServerListener(org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint);\n  public synchronized void nodeChildrenChanged(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint$SinkPeer.class;;;public class org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint$SinkPeer {\n  public org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint$SinkPeer(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.AsyncRegionServerAdmin);\n  public org.apache.hadoop.hbase.client.AsyncRegionServerAdmin getRegionServer();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/replication/HBaseReplicationEndpoint.class;;;public abstract class org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint extends org.apache.hadoop.hbase.replication.BaseReplicationEndpoint implements org.apache.hadoop.hbase.Abortable {\n  public static final int DEFAULT_BAD_SINK_THRESHOLD;\n  public static final float DEFAULT_REPLICATION_SOURCE_RATIO;\n  public org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint();\n  public void init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context) throws java.io.IOException;\n  public void start();\n  public void stop();\n  public java.util.UUID getPeerUUID();\n  public void abort(java.lang.String, java.lang.Throwable);\n  public boolean isAborted();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/NamespaceTableCfWALEntryFilter.class;;;public class org.apache.hadoop.hbase.replication.NamespaceTableCfWALEntryFilter implements org.apache.hadoop.hbase.replication.WALEntryFilter,org.apache.hadoop.hbase.replication.WALCellFilter {\n  public org.apache.hadoop.hbase.replication.NamespaceTableCfWALEntryFilter(org.apache.hadoop.hbase.replication.ReplicationPeer);\n  public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry);\n  public org.apache.hadoop.hbase.Cell filterCell(org.apache.hadoop.hbase.wal.WAL$Entry, org.apache.hadoop.hbase.Cell);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/ReplicationBarrierFamilyFormat$ReplicationBarrierResult.class;;;public final class org.apache.hadoop.hbase.replication.ReplicationBarrierFamilyFormat$ReplicationBarrierResult {\n  public long[] getBarriers();\n  public org.apache.hadoop.hbase.master.RegionState$State getState();\n  public java.util.List<byte[]> getParentRegionNames();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/ReplicationBarrierFamilyFormat.class;;;public final class org.apache.hadoop.hbase.replication.ReplicationBarrierFamilyFormat {\n  public static final byte[] REPLICATION_PARENT_QUALIFIER;\n  public static void addReplicationBarrier(org.apache.hadoop.hbase.client.Put, long) throws java.io.IOException;\n  public static byte[] getParentsBytes(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>);\n  public static void addReplicationParent(org.apache.hadoop.hbase.client.Put, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put makePutForReplicationBarrier(org.apache.hadoop.hbase.client.RegionInfo, long, long) throws java.io.IOException;\n  public static long[] getReplicationBarriers(org.apache.hadoop.hbase.client.Result);\n  public static org.apache.hadoop.hbase.replication.ReplicationBarrierFamilyFormat$ReplicationBarrierResult getReplicationBarrierResult(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, byte[], byte[]) throws java.io.IOException;\n  public static long[] getReplicationBarriers(org.apache.hadoop.hbase.client.Connection, byte[]) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.util.Pair<java.lang.String, java.lang.Long>> getTableEncodedRegionNameAndLastBarrier(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static java.util.List<java.lang.String> getTableEncodedRegionNamesForSerialReplication(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/ReplicationEndpoint$Context.class;;;public class org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context {\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context(org.apache.hadoop.hbase.Server, org.apache.hadoop.conf.Configuration, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, java.lang.String, java.util.UUID, org.apache.hadoop.hbase.replication.ReplicationPeer, org.apache.hadoop.hbase.replication.regionserver.MetricsSource, org.apache.hadoop.hbase.TableDescriptors, org.apache.hadoop.hbase.Abortable);\n  public org.apache.hadoop.hbase.Server getServer();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.conf.Configuration getLocalConfiguration();\n  public org.apache.hadoop.fs.FileSystem getFilesystem();\n  public java.util.UUID getClusterId();\n  public java.lang.String getPeerId();\n  public org.apache.hadoop.hbase.replication.ReplicationPeerConfig getPeerConfig();\n  public org.apache.hadoop.hbase.replication.ReplicationPeer getReplicationPeer();\n  public org.apache.hadoop.hbase.replication.regionserver.MetricsSource getMetrics();\n  public org.apache.hadoop.hbase.TableDescriptors getTableDescriptors();\n  public org.apache.hadoop.hbase.Abortable getAbortable();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/ReplicationEndpoint$ReplicateContext.class;;;public class org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext {\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext();\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext setEntries(java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>);\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext setSize(int);\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext setWalGroupId(java.lang.String);\n  public java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry> getEntries();\n  public int getSize();\n  public java.lang.String getWalGroupId();\n  public void setTimeout(int);\n  public int getTimeout();\n}\n;;;No. This is a class definition for a context object used in the replication endpoint of HBase. It is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition. It is a class representing a context for replicating entries in HBase.;;;N
org/apache/hadoop/hbase/replication/ReplicationEndpoint.class;;;public interface org.apache.hadoop.hbase.replication.ReplicationEndpoint extends org.apache.hadoop.hbase.replication.ReplicationPeerConfigListener {\n  public abstract void init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context) throws java.io.IOException;\n  public abstract boolean canReplicateToSameCluster();\n  public abstract java.util.UUID getPeerUUID();\n  public abstract org.apache.hadoop.hbase.replication.WALEntryFilter getWALEntryfilter();\n  public abstract boolean replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext);\n  public abstract boolean isRunning();\n  public abstract boolean isStarting();\n  public abstract void start();\n  public abstract void awaitRunning();\n  public abstract void awaitRunning(long, java.util.concurrent.TimeUnit) throws java.util.concurrent.TimeoutException;\n  public abstract void stop();\n  public abstract void awaitTerminated();\n  public abstract void awaitTerminated(long, java.util.concurrent.TimeUnit) throws java.util.concurrent.TimeoutException;\n  public abstract java.lang.Throwable failureCause();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/ReplicationSinkServiceImpl$ReplicationStatisticsChore.class;;;final class org.apache.hadoop.hbase.replication.ReplicationSinkServiceImpl$ReplicationStatisticsChore extends org.apache.hadoop.hbase.ScheduledChore {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/ReplicationSinkServiceImpl.class;;;public class org.apache.hadoop.hbase.replication.ReplicationSinkServiceImpl implements org.apache.hadoop.hbase.regionserver.ReplicationSinkService {\n  public org.apache.hadoop.hbase.replication.ReplicationSinkServiceImpl();\n  public void replicateLogEntries(java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void initialize(org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.wal.WALFactory) throws java.io.IOException;\n  public void startReplicationService() throws java.io.IOException;\n  public void stopReplicationService();\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad refreshAndGetReplicationLoad();\n}\n;;;No. This is a class definition for a service that implements an interface. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/ScopeWALEntryFilter$1.class;;;class org.apache.hadoop.hbase.replication.ScopeWALEntryFilter$1 implements org.apache.hbase.thirdparty.com.google.common.base.Predicate<byte[]> {\n  public boolean apply(byte[]);\n  public boolean apply(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/ScopeWALEntryFilter.class;;;public class org.apache.hadoop.hbase.replication.ScopeWALEntryFilter implements org.apache.hadoop.hbase.replication.WALEntryFilter,org.apache.hadoop.hbase.replication.WALCellFilter {\n  public org.apache.hadoop.hbase.replication.ScopeWALEntryFilter();\n  public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry);\n  public org.apache.hadoop.hbase.Cell filterCell(org.apache.hadoop.hbase.wal.WAL$Entry, org.apache.hadoop.hbase.Cell);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/SystemTableWALEntryFilter.class;;;public class org.apache.hadoop.hbase.replication.SystemTableWALEntryFilter implements org.apache.hadoop.hbase.replication.WALEntryFilter {\n  public org.apache.hadoop.hbase.replication.SystemTableWALEntryFilter();\n  public org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/VerifyWALEntriesReplicationEndpoint.class;;;public class org.apache.hadoop.hbase.replication.VerifyWALEntriesReplicationEndpoint extends org.apache.hadoop.hbase.replication.BaseReplicationEndpoint {\n  public org.apache.hadoop.hbase.replication.VerifyWALEntriesReplicationEndpoint();\n  public boolean canReplicateToSameCluster();\n  public java.util.UUID getPeerUUID();\n  public org.apache.hadoop.hbase.replication.WALEntryFilter getWALEntryfilter();\n  public boolean replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext);\n  public void start();\n  public void stop();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/WALCellFilter.class;;;public interface org.apache.hadoop.hbase.replication.WALCellFilter {\n  public abstract org.apache.hadoop.hbase.Cell filterCell(org.apache.hadoop.hbase.wal.WAL$Entry, org.apache.hadoop.hbase.Cell);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/WALEntryFilter.class;;;public interface org.apache.hadoop.hbase.replication.WALEntryFilter {\n  public abstract org.apache.hadoop.hbase.wal.WAL$Entry filter(org.apache.hadoop.hbase.wal.WAL$Entry);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/master/ReplicationHFileCleaner$1.class;;;class org.apache.hadoop.hbase.replication.master.ReplicationHFileCleaner$1 implements org.apache.hbase.thirdparty.com.google.common.base.Predicate<org.apache.hadoop.fs.FileStatus> {\n  public boolean apply(org.apache.hadoop.fs.FileStatus);\n  public boolean apply(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/master/ReplicationHFileCleaner.class;;;public class org.apache.hadoop.hbase.replication.master.ReplicationHFileCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate {\n  public org.apache.hadoop.hbase.replication.master.ReplicationHFileCleaner();\n  public java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void setConf(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner$1.class;;;class org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner$1 implements org.apache.hbase.thirdparty.com.google.common.base.Predicate<org.apache.hadoop.fs.FileStatus> {\n  public boolean apply(org.apache.hadoop.fs.FileStatus);\n  public boolean apply(java.lang.Object);\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.class;;;public class org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseLogCleanerDelegate {\n  public org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner();\n  public void preClean();\n  public java.lang.Iterable<org.apache.hadoop.fs.FileStatus> getDeletableFiles(java.lang.Iterable<org.apache.hadoop.fs.FileStatus>);\n  public void init(java.util.Map<java.lang.String, java.lang.Object>);\n  public void setConf(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n  public void setConf(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.replication.ReplicationQueueStorage);\n  public void stop(java.lang.String);\n  public boolean isStopped();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for a log cleaner in Apache HBase.;;;N
org/apache/hadoop/hbase/replication/regionserver/ClaimReplicationQueueCallable.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ClaimReplicationQueueCallable extends org.apache.hadoop.hbase.procedure2.BaseRSProcedureCallable {\n  public org.apache.hadoop.hbase.replication.regionserver.ClaimReplicationQueueCallable();\n  public org.apache.hadoop.hbase.executor.EventType getEventType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/DefaultSourceFSConfigurationProvider.class;;;public class org.apache.hadoop.hbase.replication.regionserver.DefaultSourceFSConfigurationProvider implements org.apache.hadoop.hbase.replication.regionserver.SourceFSConfigurationProvider {\n  public org.apache.hadoop.hbase.replication.regionserver.DefaultSourceFSConfigurationProvider();\n  public org.apache.hadoop.conf.Configuration getConf(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues$1.class;;;class org.apache.hadoop.hbase.replication.regionserver.DumpReplicationQueues$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues$DumpOptions.class;;;class org.apache.hadoop.hbase.replication.regionserver.DumpReplicationQueues$DumpOptions {\n  public org.apache.hadoop.hbase.replication.regionserver.DumpReplicationQueues$DumpOptions();\n  public org.apache.hadoop.hbase.replication.regionserver.DumpReplicationQueues$DumpOptions(org.apache.hadoop.hbase.replication.regionserver.DumpReplicationQueues$DumpOptions);\n}\n;;;No.;;;N;;;No. This is not a task definition. It does not contain any information about a task to be performed or a process to be executed.;;;N
org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues$WarnOnlyAbortable.class;;;class org.apache.hadoop.hbase.replication.regionserver.DumpReplicationQueues$WarnOnlyAbortable implements org.apache.hadoop.hbase.Abortable {\n  public void abort(java.lang.String, java.lang.Throwable);\n  public boolean isAborted();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/DumpReplicationQueues.class;;;public class org.apache.hadoop.hbase.replication.regionserver.DumpReplicationQueues extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public org.apache.hadoop.hbase.replication.regionserver.DumpReplicationQueues();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public java.lang.String dumpReplicationSummary();\n  public java.lang.String dumpPeersState(java.util.List<org.apache.hadoop.hbase.replication.ReplicationPeerDescription>) throws java.lang.Exception;\n  public java.lang.String dumpQueues(org.apache.hadoop.hbase.zookeeper.ZKWatcher, java.util.Set<java.lang.String>, boolean) throws java.lang.Exception;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a Java class containing methods and functionality related to dumping replication queues and summary.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/HBaseInterClusterReplicationEndpoint.class;;;public class org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint extends org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint {\n  public static final java.lang.String REPLICATION_DROP_ON_DELETED_TABLE_KEY;\n  public static final java.lang.String REPLICATION_DROP_ON_DELETED_COLUMN_FAMILY_KEY;\n  public org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint();\n  public void init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context) throws java.io.IOException;\n  public static boolean isTableNotFoundException(java.lang.Throwable);\n  public static boolean isNoSuchColumnFamilyException(java.lang.Throwable);\n  public boolean replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/HFileReplicator$Copier.class;;;class org.apache.hadoop.hbase.replication.regionserver.HFileReplicator$Copier implements java.util.concurrent.Callable<java.lang.Void> {\n  public org.apache.hadoop.hbase.replication.regionserver.HFileReplicator$Copier(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.util.List<java.lang.String>) throws java.io.IOException;\n  public java.lang.Void call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/replication/regionserver/HFileReplicator.class;;;public class org.apache.hadoop.hbase.replication.regionserver.HFileReplicator implements java.io.Closeable {\n  public static final java.lang.String REPLICATION_BULKLOAD_COPY_MAXTHREADS_KEY;\n  public static final int REPLICATION_BULKLOAD_COPY_MAXTHREADS_DEFAULT;\n  public static final java.lang.String REPLICATION_BULKLOAD_COPY_HFILES_PERTHREAD_KEY;\n  public static final int REPLICATION_BULKLOAD_COPY_HFILES_PERTHREAD_DEFAULT;\n  public org.apache.hadoop.hbase.replication.regionserver.HFileReplicator(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, java.util.Map<java.lang.String, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.util.List<java.lang.String>>>>, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.AsyncClusterConnection, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public java.lang.Void replicate() throws java.io.IOException;\n}\n;;;No. Despite having methods and parameters, the class does not contain any message-specific information, nor is it designed to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/MetricsSink.class;;;public class org.apache.hadoop.hbase.replication.regionserver.MetricsSink {\n  public org.apache.hadoop.hbase.replication.regionserver.MetricsSink();\n  public long setAgeOfLastAppliedOp(long);\n  public long refreshAgeOfLastAppliedOp();\n  public void applyBatch(long);\n  public void applyBatch(long, long);\n  public void incrementFailedBatches();\n  public long getAgeOfLastAppliedOp();\n  public long getTimestampOfLastAppliedOp();\n  public long getStartTimestamp();\n  public long getAppliedOps();\n}\n;;;No, this is a class definition. It may be used to create instances of the class, but it is not a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/MetricsSource.class;;;public class org.apache.hadoop.hbase.replication.regionserver.MetricsSource implements org.apache.hadoop.hbase.metrics.BaseSource {\n  public org.apache.hadoop.hbase.replication.regionserver.MetricsSource(java.lang.String);\n  public org.apache.hadoop.hbase.replication.regionserver.MetricsSource(java.lang.String, org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSourceSource, org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationGlobalSourceSource, java.util.Map<java.lang.String, org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationTableSource>);\n  public void setAgeOfLastShippedOp(long, java.lang.String);\n  public void updateTableLevelMetrics(java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.wal.WAL$Entry, java.lang.Long>>);\n  public void setAgeOfLastShippedOpByTable(long, java.lang.String);\n  public long getAgeOfLastShippedOp(java.lang.String);\n  public void refreshAgeOfLastShippedOp(java.lang.String);\n  public void incrSizeOfLogQueue();\n  public void decrSizeOfLogQueue();\n  public void incrSourceInitializing();\n  public void decrSourceInitializing();\n  public void incrLogEditsRead();\n  public void incrLogEditsFiltered(long);\n  public void incrLogEditsFiltered();\n  public void shipBatch(long, int);\n  public void incrementFailedBatches();\n  public long getEditsFiltered();\n  public long getReplicableEdits();\n  public long getOpsShipped();\n  public void shipBatch(long, int, long);\n  public void incrLogReadInBytes(long);\n  public void clear();\n  public java.lang.Long getAgeOfLastShippedOp();\n  public int getSizeOfLogQueue();\n  public long getUncleanlyClosedWALs();\n  public long getTimestampOfLastShippedOp();\n  public long getTimeStampNextToReplicate();\n  public void setTimeStampNextToReplicate(long);\n  public long getReplicationDelay();\n  public int getSourceInitializing();\n  public java.lang.String getPeerID();\n  public void incrSizeOfHFileRefsQueue(long);\n  public void decrSizeOfHFileRefsQueue(int);\n  public void incrUnknownFileLengthForClosedWAL();\n  public void incrUncleanlyClosedWALs();\n  public void incrBytesSkippedInUncleanlyClosedWALs(long);\n  public void incrRestartedWALReading();\n  public void incrRepeatedFileBytes(long);\n  public void incrCompletedWAL();\n  public void incrCompletedRecoveryQueue();\n  public void incrFailedRecoveryQueue();\n  public void setOldestWalAge(long);\n  public long getOldestWalAge();\n  public void init();\n  public void setGauge(java.lang.String, long);\n  public void incGauge(java.lang.String, long);\n  public void decGauge(java.lang.String, long);\n  public void removeMetric(java.lang.String);\n  public void incCounters(java.lang.String, long);\n  public void updateHistogram(java.lang.String, long);\n  public java.lang.String getMetricsContext();\n  public java.lang.String getMetricsDescription();\n  public java.lang.String getMetricsJmxContext();\n  public java.lang.String getMetricsName();\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationTableSource> getSingleSourceSourceByTable();\n  public void setWALReaderEditsBufferUsage(long);\n  public long getWALReaderEditsBufferUsage();\n}\n;;;yes;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/PeerActionListener$1.class;;;final class org.apache.hadoop.hbase.replication.regionserver.PeerActionListener$1 implements org.apache.hadoop.hbase.replication.regionserver.PeerActionListener {\n}\n;;;No. This is a class definition for a PeerActionListener listener implementation in the HBase replication region server. It is not a message definition that might be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/replication/regionserver/PeerActionListener.class;;;public interface org.apache.hadoop.hbase.replication.regionserver.PeerActionListener {\n  public static final org.apache.hadoop.hbase.replication.regionserver.PeerActionListener DUMMY;\n  public default void peerSyncReplicationStateChange(java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState, int);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface definition for a listener in the HBase replication region server.;;;N
org/apache/hadoop/hbase/replication/regionserver/PeerProcedureHandler.class;;;public interface org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandler {\n  public abstract void addPeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract void removePeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract void disablePeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract void enablePeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract void updatePeerConfig(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract void transitSyncReplicationPeerState(java.lang.String, int, org.apache.hadoop.hbase.regionserver.HRegionServer) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public abstract void claimReplicationQueue(org.apache.hadoop.hbase.ServerName, java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n}\n;;;No, this is an interface that defines methods for handling peer procedures in a Hadoop HBase replication region server. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/PeerProcedureHandlerImpl.class;;;public class org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandlerImpl implements org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandler {\n  public org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandlerImpl(org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager, org.apache.hadoop.hbase.replication.regionserver.PeerActionListener);\n  public void addPeer(java.lang.String) throws java.io.IOException;\n  public void removePeer(java.lang.String) throws java.io.IOException;\n  public void enablePeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public void disablePeer(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public void updatePeerConfig(java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public void transitSyncReplicationPeerState(java.lang.String, int, org.apache.hadoop.hbase.regionserver.HRegionServer) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n  public void claimReplicationQueue(org.apache.hadoop.hbase.ServerName, java.lang.String) throws org.apache.hadoop.hbase.replication.ReplicationException, java.io.IOException;\n}\n;;;No, this class is an implementation of the PeerProcedureHandler interface and is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/RecoveredReplicationSource.class;;;public class org.apache.hadoop.hbase.replication.regionserver.RecoveredReplicationSource extends org.apache.hadoop.hbase.replication.regionserver.ReplicationSource {\n  public org.apache.hadoop.hbase.replication.regionserver.RecoveredReplicationSource();\n  public void init(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager, org.apache.hadoop.hbase.replication.ReplicationQueueStorage, org.apache.hadoop.hbase.replication.ReplicationPeer, org.apache.hadoop.hbase.Server, java.lang.String, java.util.UUID, org.apache.hadoop.hbase.replication.regionserver.WALFileLengthProvider, org.apache.hadoop.hbase.replication.regionserver.MetricsSource) throws java.io.IOException;\n  public void locateRecoveredPaths(java.lang.String) throws java.io.IOException;\n  public java.lang.String getPeerId();\n  public org.apache.hadoop.hbase.ServerName getServerWALsBelongTo();\n  public boolean isRecovered();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/RecoveredReplicationSourceShipper.class;;;public class org.apache.hadoop.hbase.replication.regionserver.RecoveredReplicationSourceShipper extends org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper {\n  public org.apache.hadoop.hbase.replication.regionserver.RecoveredReplicationSourceShipper(org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceLogQueue, org.apache.hadoop.hbase.replication.regionserver.RecoveredReplicationSource, org.apache.hadoop.hbase.replication.ReplicationQueueStorage);\n  public long getStartPosition();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/RefreshPeerCallable$1.class;;;class org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It appears to be an internal class definition within the HBase replication region server code.;;;N
org/apache/hadoop/hbase/replication/regionserver/RefreshPeerCallable.class;;;public class org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable extends org.apache.hadoop.hbase.procedure2.BaseRSProcedureCallable {\n  public org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable();\n  public org.apache.hadoop.hbase.executor.EventType getEventType();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/replication/regionserver/RejectReplicationRequestStateChecker.class;;;public class org.apache.hadoop.hbase.replication.regionserver.RejectReplicationRequestStateChecker implements java.util.function.BiPredicate<org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState> {\n  public org.apache.hadoop.hbase.replication.regionserver.RejectReplicationRequestStateChecker();\n  public boolean test(org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState);\n  public static org.apache.hadoop.hbase.replication.regionserver.RejectReplicationRequestStateChecker get();\n  public boolean test(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/RejectRequestsFromClientStateChecker.class;;;public class org.apache.hadoop.hbase.replication.regionserver.RejectRequestsFromClientStateChecker implements java.util.function.BiPredicate<org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState> {\n  public org.apache.hadoop.hbase.replication.regionserver.RejectRequestsFromClientStateChecker();\n  public boolean test(org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState);\n  public static org.apache.hadoop.hbase.replication.regionserver.RejectRequestsFromClientStateChecker get();\n  public boolean test(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplaySyncReplicationWALCallable.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplaySyncReplicationWALCallable extends org.apache.hadoop.hbase.procedure2.BaseRSProcedureCallable {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplaySyncReplicationWALCallable();\n  public org.apache.hadoop.hbase.executor.EventType getEventType();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/replication/regionserver/Replication$ReplicationStatisticsChore.class;;;final class org.apache.hadoop.hbase.replication.regionserver.Replication$ReplicationStatisticsChore extends org.apache.hadoop.hbase.ScheduledChore {\n}\n;;;No;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/replication/regionserver/Replication.class;;;public class org.apache.hadoop.hbase.replication.regionserver.Replication implements org.apache.hadoop.hbase.regionserver.ReplicationSourceService {\n  public org.apache.hadoop.hbase.replication.regionserver.Replication();\n  public void initialize(org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.wal.WALFactory) throws java.io.IOException;\n  public org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandler getPeerProcedureHandler();\n  public void stopReplicationService();\n  public void startReplicationService() throws java.io.IOException;\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager getReplicationManager();\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad refreshAndGetReplicationLoad();\n  public org.apache.hadoop.hbase.replication.regionserver.SyncReplicationPeerInfoProvider getSyncReplicationPeerInfoProvider();\n  public org.apache.hadoop.hbase.replication.ReplicationPeers getReplicationPeers();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationLoad.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad {\n  public static final org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad EMPTY_REPLICATIONLOAD;\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad();\n  public void buildReplicationLoad(java.util.List<org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface>, org.apache.hadoop.hbase.replication.regionserver.MetricsSink);\n  public java.lang.String sourceToString();\n  public java.lang.String sinkToString();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos$ReplicationLoadSink getReplicationLoadSink();\n  public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.ClusterStatusProtos$ReplicationLoadSource> getReplicationLoadSourceEntries();\n  public java.lang.String toString();\n}\n;;;No. It is not a message definition. It appears to be a Java class that contains methods for building and retrieving replication load information.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationObserver.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationObserver implements org.apache.hadoop.hbase.coprocessor.RegionCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionObserver {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationObserver();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionObserver> getRegionObserver();\n  public void preCommitStoreFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path>>) throws java.io.IOException;\n}\n;;;No. It is a class implementing two interfaces, RegionCoprocessor and RegionObserver. It does not define or describe a message that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationRuntimeException.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationRuntimeException extends java.lang.RuntimeException {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationRuntimeException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationRuntimeException(java.lang.String);\n}\n;;;No. This is a class definition for an exception in the HBase replication region server. It is not a message definition that would typically be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationSink {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSink(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void replicateEntries(java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WALEntry>, org.apache.hadoop.hbase.CellScanner, java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void stopReplicationSinkServices();\n  public java.lang.String getStats();\n  public org.apache.hadoop.hbase.replication.regionserver.MetricsSink getSinkMetrics();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationSource implements org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface {\n  public static final java.lang.String WAIT_ON_ENDPOINT_SECONDS;\n  public static final int DEFAULT_WAIT_ON_ENDPOINT_SECONDS;\n  public void init(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager, org.apache.hadoop.hbase.replication.ReplicationQueueStorage, org.apache.hadoop.hbase.replication.ReplicationPeer, org.apache.hadoop.hbase.Server, java.lang.String, java.util.UUID, org.apache.hadoop.hbase.replication.regionserver.WALFileLengthProvider, org.apache.hadoop.hbase.replication.regionserver.MetricsSource) throws java.io.IOException;\n  public void enqueueLog(org.apache.hadoop.fs.Path);\n  public java.util.Map<java.lang.String, java.util.concurrent.PriorityBlockingQueue<org.apache.hadoop.fs.Path>> getQueues();\n  public void addHFileRefs(org.apache.hadoop.hbase.TableName, byte[], java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path>>) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus> getWalGroupStatus();\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint getReplicationEndpoint();\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager getSourceManager();\n  public void tryThrottle(int) throws java.lang.InterruptedException;\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface startup();\n  public void terminate(java.lang.String);\n  public void terminate(java.lang.String, java.lang.Exception);\n  public void terminate(java.lang.String, java.lang.Exception, boolean);\n  public void terminate(java.lang.String, java.lang.Exception, boolean, boolean);\n  public java.lang.String getQueueId();\n  public org.apache.hadoop.fs.Path getCurrentPath();\n  public boolean isSourceActive();\n  public org.apache.hadoop.hbase.replication.ReplicationQueueInfo getReplicationQueueInfo();\n  public boolean isWorkerRunning();\n  public java.lang.String getStats();\n  public org.apache.hadoop.hbase.replication.regionserver.MetricsSource getSourceMetrics();\n  public void postShipEdits(java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>, int);\n  public org.apache.hadoop.hbase.replication.regionserver.WALFileLengthProvider getWALFileLengthProvider();\n  public org.apache.hadoop.hbase.ServerName getServerWALsBelongTo();\n  public org.apache.hadoop.hbase.replication.ReplicationPeer getPeer();\n  public org.apache.hadoop.hbase.replication.ReplicationQueueStorage getReplicationQueueStorage();\n  public java.lang.String logPeerId();\n}\n;;;No. This is a class definition for an implementation of an interface. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceFactory.class;;;public final class org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceFactory {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceInterface.class;;;public interface org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface {\n  public abstract void init(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager, org.apache.hadoop.hbase.replication.ReplicationQueueStorage, org.apache.hadoop.hbase.replication.ReplicationPeer, org.apache.hadoop.hbase.Server, java.lang.String, java.util.UUID, org.apache.hadoop.hbase.replication.regionserver.WALFileLengthProvider, org.apache.hadoop.hbase.replication.regionserver.MetricsSource) throws java.io.IOException;\n  public abstract void enqueueLog(org.apache.hadoop.fs.Path);\n  public abstract void addHFileRefs(org.apache.hadoop.hbase.TableName, byte[], java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path>>) throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public abstract org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface startup();\n  public abstract void terminate(java.lang.String);\n  public abstract void terminate(java.lang.String, java.lang.Exception);\n  public abstract void terminate(java.lang.String, java.lang.Exception, boolean);\n  public abstract org.apache.hadoop.fs.Path getCurrentPath();\n  public abstract java.lang.String getQueueId();\n  public default java.lang.String getPeerId();\n  public abstract org.apache.hadoop.hbase.replication.ReplicationPeer getPeer();\n  public abstract java.lang.String getStats();\n  public default boolean isPeerEnabled();\n  public default boolean isSyncReplication();\n  public abstract boolean isSourceActive();\n  public abstract org.apache.hadoop.hbase.replication.regionserver.MetricsSource getSourceMetrics();\n  public abstract org.apache.hadoop.hbase.replication.ReplicationEndpoint getReplicationEndpoint();\n  public abstract org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager getSourceManager();\n  public abstract org.apache.hadoop.hbase.replication.regionserver.WALFileLengthProvider getWALFileLengthProvider();\n  public abstract void tryThrottle(int) throws java.lang.InterruptedException;\n  public abstract void postShipEdits(java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>, int);\n  public abstract org.apache.hadoop.hbase.ServerName getServerWALsBelongTo();\n  public default java.util.Map<java.lang.String, org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus> getWalGroupStatus();\n  public default boolean isRecovered();\n  public abstract org.apache.hadoop.hbase.replication.ReplicationQueueStorage getReplicationQueueStorage();\n  public default void logPositionAndCleanOldLogs(org.apache.hadoop.hbase.replication.regionserver.WALEntryBatch);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceLogQueue.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceLogQueue {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceLogQueue(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.replication.regionserver.MetricsSource, org.apache.hadoop.hbase.replication.regionserver.ReplicationSource);\n  public boolean enqueueLog(org.apache.hadoop.fs.Path, java.lang.String);\n  public int getQueueSize(java.lang.String);\n  public int getNumQueues();\n  public java.util.Map<java.lang.String, java.util.concurrent.PriorityBlockingQueue<org.apache.hadoop.fs.Path>> getQueues();\n  public java.util.concurrent.PriorityBlockingQueue<org.apache.hadoop.fs.Path> getQueue(java.lang.String);\n  public void remove(java.lang.String);\n  public void clear(java.lang.String);\n  public org.apache.hadoop.hbase.replication.regionserver.MetricsSource getMetrics();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager$ReplicationQueueOperation.class;;;interface org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager$ReplicationQueueOperation {\n  public abstract void exec() throws org.apache.hadoop.hbase.replication.ReplicationException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager(org.apache.hadoop.hbase.replication.ReplicationQueueStorage, org.apache.hadoop.hbase.replication.ReplicationPeers, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.Server, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.util.UUID, org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.hbase.replication.regionserver.SyncReplicationPeerMappingManager, org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationGlobalSourceSource) throws java.io.IOException;\n  public void addPeer(java.lang.String) throws java.io.IOException;\n  public void removePeer(java.lang.String);\n  public void drainSources(java.lang.String) throws java.io.IOException, org.apache.hadoop.hbase.replication.ReplicationException;\n  public void refreshSources(java.lang.String) throws java.io.IOException;\n  public void logPositionAndCleanOldLogs(org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface, org.apache.hadoop.hbase.replication.regionserver.WALEntryBatch);\n  public void preLogRoll(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void postLogRoll(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void join();\n  public java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.util.NavigableSet<java.lang.String>>> getWALs();\n  public java.util.List<org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface> getSources();\n  public java.util.List<org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface> getOldSources();\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface getSource(java.lang.String);\n  public java.util.concurrent.atomic.AtomicLong getTotalBufferUsed();\n  public long getTotalBufferLimit();\n  public org.apache.hadoop.fs.Path getOldLogDir();\n  public org.apache.hadoop.fs.Path getLogDir();\n  public org.apache.hadoop.fs.FileSystem getFs();\n  public org.apache.hadoop.hbase.replication.ReplicationPeers getReplicationPeers();\n  public java.lang.String getStats();\n  public void addHFileRefs(org.apache.hadoop.hbase.TableName, byte[], java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path>>) throws java.io.IOException;\n  public void cleanUpHFileRefs(java.lang.String, java.util.List<java.lang.String>);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper$WorkerState.class;;;public final class org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper$WorkerState extends java.lang.Enum<org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper$WorkerState> {\n  public static final org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper$WorkerState RUNNING;\n  public static final org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper$WorkerState STOPPED;\n  public static final org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper$WorkerState FINISHED;\n  public static org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper$WorkerState[] values();\n  public static org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper$WorkerState valueOf(java.lang.String);\n}\n;;;No;;;N;;;No, it is not a task definition. It is a Java enum definition.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceShipper.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper extends java.lang.Thread {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper(org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceLogQueue, org.apache.hadoop.hbase.replication.regionserver.ReplicationSource);\n  public final void run();\n  public void startup(java.lang.Thread$UncaughtExceptionHandler);\n  public boolean isFinished();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALActionListener.class;;;class org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALActionListener implements org.apache.hadoop.hbase.regionserver.wal.WALActionsListener {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALActionListener(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager);\n  public void preLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void postLogRoll(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void visitLogEntryBeforeWrite(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit);\n}\n;;;No. This is a class implementation that defines methods for listening to events in a HBase region server's write-ahead log (WAL), but it is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceWALReader.class;;;class org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALReader extends java.lang.Thread {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceLogQueue, long, org.apache.hadoop.hbase.replication.WALEntryFilter, org.apache.hadoop.hbase.replication.regionserver.ReplicationSource, java.lang.String);\n  public void run();\n  public org.apache.hadoop.fs.Path getCurrentPath();\n  public org.apache.hadoop.hbase.replication.regionserver.WALEntryBatch take() throws java.lang.InterruptedException;\n  public org.apache.hadoop.hbase.replication.regionserver.WALEntryBatch poll(long) throws java.lang.InterruptedException;\n  public static long getEntrySizeExcludeBulkLoad(org.apache.hadoop.hbase.wal.WAL$Entry);\n  public boolean isReaderRunning();\n  public void setReaderRunning(boolean);\n}\n;;;No. It is a class definition for a thread in the Apache Hadoop HBase Replication module, but it is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class that extends the Thread class and contains methods and properties related to a specific functionality in the context of the HBase replication process. It is not a standalone task that can be executed independently.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationStatus$1.class;;;class org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationStatus$ReplicationStatusBuilder.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder();\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder withPeerId(java.lang.String);\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder withFileSize(long);\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder withWalGroup(java.lang.String);\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder withCurrentPath(org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder withQueueSize(int);\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder withAgeOfLastShippedOp(long);\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder withReplicationDelay(long);\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder withCurrentPosition(long);\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus build();\n}\n;;;No. This is a builder class for creating instances of the ReplicationStatus class. It is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a builder class for creating instances of `org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus` class in Java.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationStatus.class;;;public final class org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus {\n  public long getCurrentPosition();\n  public long getFileSize();\n  public java.lang.String getPeerId();\n  public java.lang.String getWalGroup();\n  public int getQueueSize();\n  public long getAgeOfLastShippedOp();\n  public long getReplicationDelay();\n  public org.apache.hadoop.fs.Path getCurrentPath();\n  public static org.apache.hadoop.hbase.replication.regionserver.ReplicationStatus$ReplicationStatusBuilder newBuilder();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp$1.class;;;class org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$1 implements org.apache.hadoop.hbase.Abortable {\n  public void abort(java.lang.String, java.lang.Throwable);\n  public boolean isAborted();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp$DummyServer.class;;;class org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp$DummyServer implements org.apache.hadoop.hbase.Server {\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.zookeeper.ZKWatcher getZooKeeper();\n  public org.apache.hadoop.hbase.CoordinatedStateManager getCoordinatedStateManager();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public void abort(java.lang.String, java.lang.Throwable);\n  public boolean isAborted();\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public org.apache.hadoop.hbase.client.Connection getConnection();\n  public org.apache.hadoop.hbase.ChoreService getChoreService();\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n  public boolean isStopping();\n  public org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.AsyncClusterConnection getAsyncClusterConnection();\n}\n;;;No. This is a class definition for a dummy server used in HBase replication, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationSyncUp.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public int run(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/ReplicationThrottler.class;;;public class org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler {\n  public org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler(double);\n  public boolean isEnabled();\n  public long getNextSleepInterval(int);\n  public void addPushSize(int);\n  public void resetStartTick();\n  public void setBandwidth(double);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker$1.class;;;class org.apache.hadoop.hbase.replication.regionserver.SerialReplicationChecker$1 extends org.apache.hbase.thirdparty.com.google.common.cache.CacheLoader<java.lang.String, org.apache.commons.lang3.mutable.MutableLong> {\n  public org.apache.commons.lang3.mutable.MutableLong load(java.lang.String) throws java.lang.Exception;\n  public java.lang.Object load(java.lang.Object) throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/SerialReplicationChecker.class;;;class org.apache.hadoop.hbase.replication.regionserver.SerialReplicationChecker {\n  public static final java.lang.String REPLICATION_SERIALLY_WAITING_KEY;\n  public static final long REPLICATION_SERIALLY_WAITING_DEFAULT;\n  public org.apache.hadoop.hbase.replication.regionserver.SerialReplicationChecker(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.replication.regionserver.ReplicationSource);\n  public boolean canPush(org.apache.hadoop.hbase.wal.WAL$Entry, org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public void waitUntilCanPush(org.apache.hadoop.hbase.wal.WAL$Entry, org.apache.hadoop.hbase.Cell) throws java.io.IOException, java.lang.InterruptedException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/SerialReplicationSourceWALReader.class;;;public class org.apache.hadoop.hbase.replication.regionserver.SerialReplicationSourceWALReader extends org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALReader {\n  public org.apache.hadoop.hbase.replication.regionserver.SerialReplicationSourceWALReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceLogQueue, long, org.apache.hadoop.hbase.replication.WALEntryFilter, org.apache.hadoop.hbase.replication.regionserver.ReplicationSource, java.lang.String);\n  public void setReaderRunning(boolean);\n  public boolean isReaderRunning();\n  public org.apache.hadoop.hbase.replication.regionserver.WALEntryBatch poll(long) throws java.lang.InterruptedException;\n  public org.apache.hadoop.hbase.replication.regionserver.WALEntryBatch take() throws java.lang.InterruptedException;\n  public org.apache.hadoop.fs.Path getCurrentPath();\n  public void run();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/SourceFSConfigurationProvider.class;;;public interface org.apache.hadoop.hbase.replication.regionserver.SourceFSConfigurationProvider {\n  public abstract org.apache.hadoop.conf.Configuration getConf(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n}\n;;;No. This class is an interface and does not contain any actual message definition.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/replication/regionserver/SwitchRpcThrottleRemoteCallable.class;;;public class org.apache.hadoop.hbase.replication.regionserver.SwitchRpcThrottleRemoteCallable extends org.apache.hadoop.hbase.procedure2.BaseRSProcedureCallable {\n  public org.apache.hadoop.hbase.replication.regionserver.SwitchRpcThrottleRemoteCallable();\n  public org.apache.hadoop.hbase.executor.EventType getEventType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/SyncReplicationPeerInfoProvider.class;;;public interface org.apache.hadoop.hbase.replication.regionserver.SyncReplicationPeerInfoProvider {\n  public abstract java.util.Optional<org.apache.hadoop.hbase.util.Pair<java.lang.String, java.lang.String>> getPeerIdAndRemoteWALDir(org.apache.hadoop.hbase.TableName);\n  public abstract boolean checkState(org.apache.hadoop.hbase.TableName, java.util.function.BiPredicate<org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState>);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/replication/regionserver/SyncReplicationPeerInfoProviderImpl.class;;;class org.apache.hadoop.hbase.replication.regionserver.SyncReplicationPeerInfoProviderImpl implements org.apache.hadoop.hbase.replication.regionserver.SyncReplicationPeerInfoProvider {\n  public java.util.Optional<org.apache.hadoop.hbase.util.Pair<java.lang.String, java.lang.String>> getPeerIdAndRemoteWALDir(org.apache.hadoop.hbase.TableName);\n  public boolean checkState(org.apache.hadoop.hbase.TableName, java.util.function.BiPredicate<org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/SyncReplicationPeerMappingManager.class;;;class org.apache.hadoop.hbase.replication.regionserver.SyncReplicationPeerMappingManager {\n}\n;;;No. It is a class definition for a manager in HBase's replication system, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/WALEntryBatch.class;;;class org.apache.hadoop.hbase.replication.regionserver.WALEntryBatch {\n  public static final org.apache.hadoop.hbase.replication.regionserver.WALEntryBatch NO_MORE_DATA;\n  public void addEntry(org.apache.hadoop.hbase.wal.WAL$Entry, long);\n  public java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry> getWalEntries();\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.wal.WAL$Entry, java.lang.Long>> getWalEntriesWithSize();\n  public org.apache.hadoop.fs.Path getLastWalPath();\n  public void setLastWalPath(org.apache.hadoop.fs.Path);\n  public long getLastWalPosition();\n  public void setLastWalPosition(long);\n  public int getNbEntries();\n  public int getNbRowKeys();\n  public int getNbHFiles();\n  public int getNbOperations();\n  public long getHeapSize();\n  public java.util.Map<java.lang.String, java.lang.Long> getLastSeqIds();\n  public boolean isEndOfFile();\n  public void setEndOfFile(boolean);\n  public void incrementNbRowKeys(int);\n  public void incrementNbHFiles(int);\n  public void incrementHeapSize(long);\n  public void setLastSeqId(java.lang.String, long);\n  public java.lang.String toString();\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/WALEntryFilterRetryableException.class;;;public class org.apache.hadoop.hbase.replication.regionserver.WALEntryFilterRetryableException extends java.lang.RuntimeException {\n  public org.apache.hadoop.hbase.replication.regionserver.WALEntryFilterRetryableException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.hbase.replication.regionserver.WALEntryFilterRetryableException(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/WALEntrySinkFilter.class;;;public interface org.apache.hadoop.hbase.replication.regionserver.WALEntrySinkFilter {\n  public static final java.lang.String WAL_ENTRY_FILTER_KEY;\n  public abstract void init(org.apache.hadoop.hbase.client.AsyncConnection);\n  public abstract boolean filter(org.apache.hadoop.hbase.TableName, long);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/replication/regionserver/WALEntryStream$1.class;;;class org.apache.hadoop.hbase.replication.regionserver.WALEntryStream$1 implements org.apache.hadoop.hbase.util.CancelableProgressable {\n  public boolean progress();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/WALEntryStream.class;;;class org.apache.hadoop.hbase.replication.regionserver.WALEntryStream implements java.io.Closeable {\n  public org.apache.hadoop.hbase.replication.regionserver.WALEntryStream(org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceLogQueue, org.apache.hadoop.conf.Configuration, long, org.apache.hadoop.hbase.replication.regionserver.WALFileLengthProvider, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.replication.regionserver.MetricsSource, java.lang.String) throws java.io.IOException;\n  public boolean hasNext() throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WAL$Entry peek() throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WAL$Entry next() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getPosition();\n  public org.apache.hadoop.fs.Path getCurrentPath();\n  public void reset() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/replication/regionserver/WALFileLengthProvider.class;;;public interface org.apache.hadoop.hbase.replication.regionserver.WALFileLengthProvider {\n  public abstract java.util.OptionalLong getLogFileSizeIfBeingWritten(org.apache.hadoop.fs.Path);\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/rsgroup/DisabledRSGroupInfoManager.class;;;class org.apache.hadoop.hbase.rsgroup.DisabledRSGroupInfoManager implements org.apache.hadoop.hbase.rsgroup.RSGroupInfoManager {\n  public org.apache.hadoop.hbase.rsgroup.DisabledRSGroupInfoManager(org.apache.hadoop.hbase.master.ServerManager);\n  public void start();\n  public void addRSGroup(org.apache.hadoop.hbase.rsgroup.RSGroupInfo) throws java.io.IOException;\n  public void removeRSGroup(java.lang.String) throws java.io.IOException;\n  public void moveServers(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroupOfServer(org.apache.hadoop.hbase.net.Address) throws java.io.IOException;\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroup(java.lang.String) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.rsgroup.RSGroupInfo> listRSGroups() throws java.io.IOException;\n  public boolean isOnline();\n  public void removeServers(java.util.Set<org.apache.hadoop.hbase.net.Address>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroupForTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.BalanceResponse balanceRSGroup(java.lang.String, org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public void setRSGroup(java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public java.lang.String determineRSGroupInfoForTable(org.apache.hadoop.hbase.TableName);\n  public void renameRSGroup(java.lang.String, java.lang.String) throws java.io.IOException;\n  public void updateRSGroupConfig(java.lang.String, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/rsgroup/MigrateRSGroupProcedure.class;;;public class org.apache.hadoop.hbase.rsgroup.MigrateRSGroupProcedure extends org.apache.hadoop.hbase.master.procedure.ModifyTableDescriptorProcedure {\n  public org.apache.hadoop.hbase.rsgroup.MigrateRSGroupProcedure();\n  public org.apache.hadoop.hbase.rsgroup.MigrateRSGroupProcedure(org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv, org.apache.hadoop.hbase.TableName);\n}\n;;;No;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/rsgroup/RSGroupAdminClient.class;;;public class org.apache.hadoop.hbase.rsgroup.RSGroupAdminClient {\n  public org.apache.hadoop.hbase.rsgroup.RSGroupAdminClient(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroupInfo(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroupInfoOfTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void moveServers(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String) throws java.io.IOException;\n  public void moveTables(java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public void addRSGroup(java.lang.String) throws java.io.IOException;\n  public void removeRSGroup(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.BalanceResponse balanceRSGroup(java.lang.String, org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.rsgroup.RSGroupInfo> listRSGroups() throws java.io.IOException;\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroupOfServer(org.apache.hadoop.hbase.net.Address) throws java.io.IOException;\n  public void moveServersAndTables(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public void removeServers(java.util.Set<org.apache.hadoop.hbase.net.Address>) throws java.io.IOException;\n}\n;;;No. This is a class that provides methods for interacting with the HBase RegionServer Group (RSGroup) administration client. It is not a message definition that would be placed on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupAdminEndpoint.class;;;public class org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint implements org.apache.hadoop.hbase.coprocessor.MasterCoprocessor {\n  public org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint();\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public java.lang.Iterable<org.apache.hbase.thirdparty.com.google.protobuf.Service> getServices();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupAdminServiceImpl.class;;;class org.apache.hadoop.hbase.rsgroup.RSGroupAdminServiceImpl extends org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RSGroupAdminService {\n  public void getRSGroupInfo(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoResponse>);\n  public void getRSGroupInfoOfTable(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoOfTableRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoOfTableResponse>);\n  public void moveServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$MoveServersRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$MoveServersResponse>);\n  public void moveTables(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$MoveTablesRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$MoveTablesResponse>);\n  public void addRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$AddRSGroupRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$AddRSGroupResponse>);\n  public void removeRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RemoveRSGroupRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RemoveRSGroupResponse>);\n  public void balanceRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$BalanceRSGroupRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$BalanceRSGroupResponse>);\n  public void listRSGroupInfos(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$ListRSGroupInfosRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$ListRSGroupInfosResponse>);\n  public void getRSGroupInfoOfServer(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoOfServerRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$GetRSGroupInfoOfServerResponse>);\n  public void moveServersAndTables(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$MoveServersAndTablesRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$MoveServersAndTablesResponse>);\n  public void removeServers(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RemoveServersRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RemoveServersResponse>);\n  public void renameRSGroup(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RenameRSGroupRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.RSGroupAdminProtos$RenameRSGroupResponse>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupBasedLoadBalancer.class;;;public class org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer implements org.apache.hadoop.hbase.master.LoadBalancer {\n  public static final java.lang.String FALLBACK_GROUP_ENABLE_KEY;\n  public org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer();\n  public synchronized void updateClusterMetrics(org.apache.hadoop.hbase.ClusterMetrics);\n  public synchronized void updateBalancerLoadInfo(java.util.Map<org.apache.hadoop.hbase.TableName, java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>>>);\n  public void setMasterServices(org.apache.hadoop.hbase.master.MasterServices);\n  public synchronized java.util.List<org.apache.hadoop.hbase.master.RegionPlan> balanceCluster(java.util.Map<org.apache.hadoop.hbase.TableName, java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>>>) throws java.io.IOException;\n  public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>> roundRobinAssignment(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException;\n  public java.util.Map<org.apache.hadoop.hbase.ServerName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>> retainAssignment(java.util.Map<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName>, java.util.List<org.apache.hadoop.hbase.ServerName>) throws org.apache.hadoop.hbase.HBaseIOException;\n  public org.apache.hadoop.hbase.ServerName randomAssignment(org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException;\n  public void initialize() throws java.io.IOException;\n  public boolean isOnline();\n  public boolean isFallbackEnabled();\n  public void regionOnline(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName);\n  public void regionOffline(org.apache.hadoop.hbase.client.RegionInfo);\n  public synchronized void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public void stop(java.lang.String);\n  public boolean isStopped();\n  public org.apache.hadoop.hbase.master.LoadBalancer getInternalBalancer();\n  public org.apache.hadoop.hbase.favored.FavoredNodesManager getFavoredNodesManager();\n  public synchronized void postMasterStartupInitialize();\n  public void updateBalancerStatus(boolean);\n  public void setClusterInfoProvider(org.apache.hadoop.hbase.master.balancer.ClusterInfoProvider);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupInfoManager.class;;;public interface org.apache.hadoop.hbase.rsgroup.RSGroupInfoManager {\n  public abstract void start();\n  public abstract void addRSGroup(org.apache.hadoop.hbase.rsgroup.RSGroupInfo) throws java.io.IOException;\n  public abstract void removeRSGroup(java.lang.String) throws java.io.IOException;\n  public abstract void moveServers(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroupOfServer(org.apache.hadoop.hbase.net.Address) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroup(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.rsgroup.RSGroupInfo> listRSGroups() throws java.io.IOException;\n  public abstract boolean isOnline();\n  public abstract void removeServers(java.util.Set<org.apache.hadoop.hbase.net.Address>) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroupForTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.rsgroup.RSGroupInfoManager create(org.apache.hadoop.hbase.master.MasterServices) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.BalanceResponse balanceRSGroup(java.lang.String, org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public abstract void setRSGroup(java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public abstract java.lang.String determineRSGroupInfoForTable(org.apache.hadoop.hbase.TableName);\n  public abstract void renameRSGroup(java.lang.String, java.lang.String) throws java.io.IOException;\n  public abstract void updateRSGroupConfig(java.lang.String, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an interface definition for a class that might be used in a program.;;;N;;;No.;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl$1.class;;;class org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl$1 implements org.apache.hadoop.hbase.master.ServerListener {\n  public void serverAdded(org.apache.hadoop.hbase.ServerName);\n  public void serverRemoved(org.apache.hadoop.hbase.ServerName);\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class that implements an interface and defines methods to be called when a server is added or removed from a group. It is not a message that can be sent between systems through a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl$2.class;;;class org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl$2 extends java.lang.Thread {\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl$RSGroupInfoHolder.class;;;final class org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl$RSGroupInfoHolder {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl$RSGroupMappingScript.class;;;class org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl$RSGroupMappingScript {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl$RSGroupStartupWorker.class;;;class org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl$RSGroupStartupWorker extends java.lang.Thread {\n  public void run();\n  public boolean isOnline();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupInfoManagerImpl.class;;;final class org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl implements org.apache.hadoop.hbase.rsgroup.RSGroupInfoManager {\n  public void start();\n  public synchronized void addRSGroup(org.apache.hadoop.hbase.rsgroup.RSGroupInfo) throws java.io.IOException;\n  public synchronized java.util.Set<org.apache.hadoop.hbase.net.Address> moveServers(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroupOfServer(org.apache.hadoop.hbase.net.Address);\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroup(java.lang.String);\n  public synchronized void removeRSGroup(java.lang.String) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.rsgroup.RSGroupInfo> listRSGroups();\n  public boolean isOnline();\n  public synchronized void removeServers(java.util.Set<org.apache.hadoop.hbase.net.Address>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.rsgroup.RSGroupInfo getRSGroupForTable(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.BalanceResponse balanceRSGroup(java.lang.String, org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public void setRSGroup(java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public void moveServers(java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String) throws java.io.IOException;\n  public java.lang.String determineRSGroupInfoForTable(org.apache.hadoop.hbase.TableName);\n  public synchronized void renameRSGroup(java.lang.String, java.lang.String) throws java.io.IOException;\n  public synchronized void updateRSGroupConfig(java.lang.String, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/rsgroup/RSGroupMajorCompactionTTL.class;;;public class org.apache.hadoop.hbase.rsgroup.RSGroupMajorCompactionTTL extends org.apache.hadoop.hbase.util.compaction.MajorCompactorTTL {\n  public int compactTTLRegionsOnGroup(org.apache.hadoop.conf.Configuration, java.lang.String, int, long, int, int, boolean, boolean) throws java.lang.Exception;\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/rsgroup/RSGroupUtil.class;;;public final class org.apache.hadoop.hbase.rsgroup.RSGroupUtil {\n  public static final java.lang.String RS_GROUP_ENABLED;\n  public static boolean isRSGroupEnabled(org.apache.hadoop.conf.Configuration);\n  public static void enableRSGroup(org.apache.hadoop.conf.Configuration);\n  public static java.util.List<org.apache.hadoop.hbase.TableName> listTablesInRSGroup(org.apache.hadoop.hbase.master.MasterServices, java.lang.String) throws java.io.IOException;\n  public static java.util.Optional<org.apache.hadoop.hbase.rsgroup.RSGroupInfo> getRSGroupInfo(org.apache.hadoop.hbase.master.MasterServices, org.apache.hadoop.hbase.rsgroup.RSGroupInfoManager, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.rsgroup.RSGroupInfo fillTables(org.apache.hadoop.hbase.rsgroup.RSGroupInfo, java.util.Collection<org.apache.hadoop.hbase.client.TableDescriptor>);\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/security/HBasePolicyProvider.class;;;public class org.apache.hadoop.hbase.security.HBasePolicyProvider extends org.apache.hadoop.security.authorize.PolicyProvider {\n  public org.apache.hadoop.hbase.security.HBasePolicyProvider();\n  public org.apache.hadoop.security.authorize.Service[] getServices();\n  public static void init(org.apache.hadoop.conf.Configuration, org.apache.hadoop.security.authorize.ServiceAuthorizationManager);\n}\n;;;No. This class is not a message definition that might be put on a message queue. It is a class definition in the Hadoop HBase security module that provides authorization policies for HBase services.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/HBaseSaslRpcServer.class;;;public class org.apache.hadoop.hbase.security.HBaseSaslRpcServer {\n  public org.apache.hadoop.hbase.security.HBaseSaslRpcServer(org.apache.hadoop.hbase.security.provider.SaslServerAuthenticationProvider, java.util.Map<java.lang.String, java.lang.String>, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n  public boolean isComplete();\n  public byte[] evaluateResponse(byte[]) throws javax.security.sasl.SaslException;\n  public void dispose();\n  public java.lang.String getAttemptingUser();\n  public byte[] wrap(byte[], int, int) throws javax.security.sasl.SaslException;\n  public byte[] unwrap(byte[], int, int) throws javax.security.sasl.SaslException;\n  public java.lang.String getNegotiatedQop();\n  public java.lang.String getAuthorizationID();\n  public static <T extends org.apache.hadoop.security.token.TokenIdentifier> T getIdentifier(java.lang.String, org.apache.hadoop.security.token.SecretManager<T>) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/SecurityUtil.class;;;public class org.apache.hadoop.hbase.security.SecurityUtil {\n  public org.apache.hadoop.hbase.security.SecurityUtil();\n  public static java.lang.String getUserFromPrincipal(java.lang.String);\n  public static java.lang.String getPrincipalWithoutRealm(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessChecker$InputUser.class;;;public class org.apache.hadoop.hbase.security.access.AccessChecker$InputUser extends org.apache.hadoop.hbase.security.User {\n  public org.apache.hadoop.hbase.security.access.AccessChecker$InputUser(java.lang.String, java.lang.String[]);\n  public java.lang.String getShortName();\n  public java.lang.String getName();\n  public java.lang.String[] getGroupNames();\n  public <T> T runAs(java.security.PrivilegedAction<T>);\n  public <T> T runAs(java.security.PrivilegedExceptionAction<T>) throws java.io.IOException, java.lang.InterruptedException;\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessChecker.class;;;public class org.apache.hadoop.hbase.security.access.AccessChecker {\n  public static boolean isAuthorizationSupported(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.security.access.AccessChecker(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.security.access.AuthManager getAuthManager();\n  public void requireAccess(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void requirePermission(org.apache.hadoop.hbase.security.User, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action) throws java.io.IOException;\n  public void requireGlobalPermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<byte[]>>, java.lang.String) throws java.io.IOException;\n  public void requireGlobalPermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String) throws java.io.IOException;\n  public void requireNamespacePermission(org.apache.hadoop.hbase.security.User, java.lang.String, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void requireNamespacePermission(org.apache.hadoop.hbase.security.User, java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<byte[]>>, org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void requirePermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void requireTablePermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void performOnSuperuser(java.lang.String, org.apache.hadoop.hbase.security.User, java.lang.String) throws java.io.IOException;\n  public void checkLockPermissions(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo[], java.lang.String) throws java.io.IOException;\n  public static void logResult(org.apache.hadoop.hbase.security.access.AuthResult);\n  public org.apache.hadoop.hbase.security.User validateCallerWithFilterUser(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.TablePermission, java.lang.String) throws java.io.IOException;\n  public static java.util.List<java.lang.String> getUserGroups(java.lang.String);\n  public boolean hasUserPermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.security.access.Permission);\n  public org.apache.hadoop.hbase.security.access.AuthResult permissionGranted(java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/access/AccessControlFilter$1.class;;;class org.apache.hadoop.hbase.security.access.AccessControlFilter$1 {\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/security/access/AccessControlFilter$Strategy.class;;;public final class org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy extends java.lang.Enum<org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy> {\n  public static final org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy CHECK_TABLE_AND_CF_ONLY;\n  public static final org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy CHECK_CELL_DEFAULT;\n  public static org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy[] values();\n  public static org.apache.hadoop.hbase.security.access.AccessControlFilter$Strategy valueOf(java.lang.String);\n}\n;;;No, this is not a message definition that might be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/security/access/AccessControlFilter.class;;;class org.apache.hadoop.hbase.security.access.AccessControlFilter extends org.apache.hadoop.hbase.filter.FilterBase {\n  public boolean filterRowKey(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.filter.Filter$ReturnCode filterCell(org.apache.hadoop.hbase.Cell);\n  public void reset() throws java.io.IOException;\n  public byte[] toByteArray();\n  public static org.apache.hadoop.hbase.security.access.AccessControlFilter parseFrom(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;No. This is a class definition for a filter used in the HBase security access control. It is not a message definition to be put on a message queue.;;;N;;;No;;;N
org/apache/hadoop/hbase/security/access/AccessController$1.class;;;class org.apache.hadoop.hbase.security.access.AccessController$1 implements java.security.PrivilegedExceptionAction<java.lang.Void> {\n  public java.lang.Void run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessController$2.class;;;class org.apache.hadoop.hbase.security.access.AccessController$2 implements java.security.PrivilegedExceptionAction<java.lang.Void> {\n  public java.lang.Void run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessController$3.class;;;class org.apache.hadoop.hbase.security.access.AccessController$3 implements java.security.PrivilegedExceptionAction<java.lang.Void> {\n  public java.lang.Void run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No. This is not a message definition. It is a class definition that implements the PrivilegedExceptionAction interface.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessController$4.class;;;class org.apache.hadoop.hbase.security.access.AccessController$4 implements java.security.PrivilegedExceptionAction<java.lang.Void> {\n  public java.lang.Void run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessController$5.class;;;class org.apache.hadoop.hbase.security.access.AccessController$5 implements java.security.PrivilegedExceptionAction<java.lang.Void> {\n  public java.lang.Void run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessController$6.class;;;class org.apache.hadoop.hbase.security.access.AccessController$6 implements java.security.PrivilegedExceptionAction<java.lang.Void> {\n  public java.lang.Void run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessController$7.class;;;class org.apache.hadoop.hbase.security.access.AccessController$7 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessController$OpType.class;;;final class org.apache.hadoop.hbase.security.access.AccessController$OpType extends java.lang.Enum<org.apache.hadoop.hbase.security.access.AccessController$OpType> {\n  public static final org.apache.hadoop.hbase.security.access.AccessController$OpType GET;\n  public static final org.apache.hadoop.hbase.security.access.AccessController$OpType EXISTS;\n  public static final org.apache.hadoop.hbase.security.access.AccessController$OpType SCAN;\n  public static final org.apache.hadoop.hbase.security.access.AccessController$OpType PUT;\n  public static final org.apache.hadoop.hbase.security.access.AccessController$OpType DELETE;\n  public static final org.apache.hadoop.hbase.security.access.AccessController$OpType CHECK_AND_PUT;\n  public static final org.apache.hadoop.hbase.security.access.AccessController$OpType CHECK_AND_DELETE;\n  public static final org.apache.hadoop.hbase.security.access.AccessController$OpType APPEND;\n  public static final org.apache.hadoop.hbase.security.access.AccessController$OpType INCREMENT;\n  public static org.apache.hadoop.hbase.security.access.AccessController$OpType[] values();\n  public static org.apache.hadoop.hbase.security.access.AccessController$OpType valueOf(java.lang.String);\n  public java.lang.String toString();\n}\n;;;No, the class is not a message definition that might be put on a message queue. It is an enum class that defines different types of operations that can be performed by the HBase access controller.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AccessController.class;;;public class org.apache.hadoop.hbase.security.access.AccessController implements org.apache.hadoop.hbase.coprocessor.MasterCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor,org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$AccessControlService$Interface,org.apache.hadoop.hbase.coprocessor.MasterObserver,org.apache.hadoop.hbase.coprocessor.RegionObserver,org.apache.hadoop.hbase.coprocessor.RegionServerObserver,org.apache.hadoop.hbase.coprocessor.EndpointObserver,org.apache.hadoop.hbase.coprocessor.BulkLoadObserver {\n  public org.apache.hadoop.hbase.security.access.AccessController();\n  public static boolean isCellAuthorizationSupported(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.regionserver.Region getRegion();\n  public org.apache.hadoop.hbase.security.access.AuthManager getAuthManager();\n  public void requireAccess(org.apache.hadoop.hbase.coprocessor.ObserverContext<?>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void requirePermission(org.apache.hadoop.hbase.coprocessor.ObserverContext<?>, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action) throws java.io.IOException;\n  public void requireGlobalPermission(org.apache.hadoop.hbase.coprocessor.ObserverContext<?>, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<byte[]>>) throws java.io.IOException;\n  public void requireGlobalPermission(org.apache.hadoop.hbase.coprocessor.ObserverContext<?>, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String) throws java.io.IOException;\n  public void requireNamespacePermission(org.apache.hadoop.hbase.coprocessor.ObserverContext<?>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void requireNamespacePermission(org.apache.hadoop.hbase.coprocessor.ObserverContext<?>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<byte[]>>, org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void requirePermission(org.apache.hadoop.hbase.coprocessor.ObserverContext<?>, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void requireTablePermission(org.apache.hadoop.hbase.coprocessor.ObserverContext<?>, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action...) throws java.io.IOException;\n  public void checkLockPermissions(org.apache.hadoop.hbase.coprocessor.ObserverContext<?>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo[], java.lang.String) throws java.io.IOException;\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment);\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionObserver> getRegionObserver();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.MasterObserver> getMasterObserver();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.EndpointObserver> getEndpointObserver();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.BulkLoadObserver> getBulkLoadObserver();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionServerObserver> getRegionServerObserver();\n  public java.lang.Iterable<org.apache.hbase.thirdparty.com.google.protobuf.Service> getServices();\n  public void preCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public void postCompletedCreateTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public void preDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postDeleteTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postTruncateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableDescriptor preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public java.lang.String preModifyTableStoreFileTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, java.lang.String) throws java.io.IOException;\n  public java.lang.String preModifyColumnFamilyStoreFileTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[], java.lang.String) throws java.io.IOException;\n  public void postModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preEnableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preAbortProcedure(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, long) throws java.io.IOException;\n  public void postAbortProcedure(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preGetProcedures(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preGetLocks(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preMove(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public void preAssign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void preUnassign(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void preRegionOffline(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void preSetSplitOrMergeEnabled(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, org.apache.hadoop.hbase.client.MasterSwitchType) throws java.io.IOException;\n  public void preBalance(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public void preBalanceSwitch(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException;\n  public void preShutdown(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preStopMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preListSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public void preCloneSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preRestoreSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preDeleteSnapshot(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public void preCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void preDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preModifyNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void preGetNamespaceDescriptor(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void postListNamespaces(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<java.lang.String>) throws java.io.IOException;\n  public void postListNamespaceDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.NamespaceDescriptor>) throws java.io.IOException;\n  public void preTableFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preSplitRegion(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException;\n  public void preClearDeadServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preDecommissionRegionServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.ServerName>, boolean) throws java.io.IOException;\n  public void preListDecommissionedRegionServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preRecommissionRegionServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.ServerName, java.util.List<byte[]>) throws java.io.IOException;\n  public void preOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException;\n  public void postOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>);\n  public void preFlush(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.FlushLifeCycleTracker) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.InternalScanner preCompact(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.Store, org.apache.hadoop.hbase.regionserver.InternalScanner, org.apache.hadoop.hbase.regionserver.ScanType, org.apache.hadoop.hbase.regionserver.compactions.CompactionLifeCycleTracker, org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest) throws java.io.IOException;\n  public void preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public boolean preExists(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, boolean) throws java.io.IOException;\n  public void prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n  public void postPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability);\n  public void preDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n  public void preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public void postDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Delete, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n  public boolean preCheckAndPut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException;\n  public boolean preCheckAndPutAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Put, boolean) throws java.io.IOException;\n  public boolean preCheckAndDelete(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException;\n  public boolean preCheckAndDeleteAfterRowLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, byte[], byte[], byte[], org.apache.hadoop.hbase.CompareOperator, org.apache.hadoop.hbase.filter.ByteArrayComparable, org.apache.hadoop.hbase.client.Delete, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result preAppend(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Append) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result preIncrement(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Increment) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> postIncrementBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> postAppendBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>) throws java.io.IOException;\n  public void preScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.RegionScanner postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException;\n  public boolean preScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException;\n  public void preScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public void postScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public void preBulkLoadHFile(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>) throws java.io.IOException;\n  public void prePrepareBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException;\n  public void preCleanupBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException;\n  public org.apache.hbase.thirdparty.com.google.protobuf.Message preEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hbase.thirdparty.com.google.protobuf.Service, java.lang.String, org.apache.hbase.thirdparty.com.google.protobuf.Message) throws java.io.IOException;\n  public void postEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hbase.thirdparty.com.google.protobuf.Service, java.lang.String, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hbase.thirdparty.com.google.protobuf.Message$Builder) throws java.io.IOException;\n  public void grant(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$GrantRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$GrantResponse>);\n  public void revoke(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$RevokeRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$RevokeResponse>);\n  public void getUserPermissions(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$GetUserPermissionsRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$GetUserPermissionsResponse>);\n  public void checkPermissions(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$CheckPermissionsRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$CheckPermissionsResponse>);\n  public void preClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, boolean) throws java.io.IOException;\n  public void preStopRegionServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public void preGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public void postGetTableDescriptors(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.TableName>, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public void postGetTableNames(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>, java.lang.String) throws java.io.IOException;\n  public void preMergeRegions(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public void preRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public void postRollWALWriterRequest(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetUserQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetTableQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetNamespaceQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public void preSetRegionServerQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.quotas.GlobalQuotaSettings) throws java.io.IOException;\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint postCreateReplicationEndPoint(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.replication.ReplicationEndpoint);\n  public void preReplicateLogEntries(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public void preClearCompactionQueues(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public void preAddReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public void preRemoveReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preEnableReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preDisableReplicationPeer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preGetReplicationPeerConfig(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preUpdateReplicationPeerConfig(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.replication.ReplicationPeerConfig) throws java.io.IOException;\n  public void preTransitReplicationPeerSyncReplicationState(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState) throws java.io.IOException;\n  public void preListReplicationPeers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preRequestLock(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo[], java.lang.String) throws java.io.IOException;\n  public void preLockHeartbeat(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, java.lang.String) throws java.io.IOException;\n  public void preExecuteProcedures(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>) throws java.io.IOException;\n  public void preSwitchRpcThrottle(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException;\n  public void preIsRpcThrottleEnabled(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preSwitchExceedThrottleQuota(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean) throws java.io.IOException;\n  public void hasPermission(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$HasPermissionRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.AccessControlProtos$HasPermissionResponse>);\n  public void preGrant(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.security.access.UserPermission, boolean) throws java.io.IOException;\n  public void preRevoke(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.security.access.UserPermission) throws java.io.IOException;\n  public void preGetUserPermissions(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[]) throws java.io.IOException;\n  public void preHasUserPermissions(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.util.List<org.apache.hadoop.hbase.security.access.Permission>) throws java.io.IOException;\n  public void preMoveServersAndTables(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.net.Address>, java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public void preMoveServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.net.Address>, java.lang.String) throws java.io.IOException;\n  public void preMoveTables(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.TableName>, java.lang.String) throws java.io.IOException;\n  public void preAddRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preRemoveRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preBalanceRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, org.apache.hadoop.hbase.client.BalanceRequest) throws java.io.IOException;\n  public void preRemoveServers(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.util.Set<org.apache.hadoop.hbase.net.Address>) throws java.io.IOException;\n  public void preGetRSGroupInfo(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preGetRSGroupInfoOfTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void preListRSGroups(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preListTablesInRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preGetConfiguredNamespacesAndTablesInRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void preGetRSGroupInfoOfServer(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.net.Address) throws java.io.IOException;\n  public void preRenameRSGroup(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.lang.String) throws java.io.IOException;\n  public void preUpdateRSGroupConfig(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n}\n;;;No, the class is not a message definition that might be put on a message queue. It is an enum class that defines different types of operations that can be performed by the HBase access controller.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AuthManager$1.class;;;class org.apache.hadoop.hbase.security.access.AuthManager$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AuthManager$PermissionCache.class;;;class org.apache.hadoop.hbase.security.access.AuthManager$PermissionCache<T extends org.apache.hadoop.hbase.security.access.Permission> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AuthManager.class;;;public final class org.apache.hadoop.hbase.security.access.AuthManager {\n  public void refreshTableCacheFromWritable(org.apache.hadoop.hbase.TableName, byte[]) throws java.io.IOException;\n  public void refreshNamespaceCacheFromWritable(java.lang.String, byte[]) throws java.io.IOException;\n  public boolean authorizeUserGlobal(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action);\n  public boolean authorizeUserNamespace(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action);\n  public boolean accessUserTable(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action);\n  public boolean authorizeUserTable(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action);\n  public boolean authorizeUserTable(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.security.access.Permission$Action);\n  public boolean authorizeUserTable(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action);\n  public boolean authorizeUserFamily(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, byte[], org.apache.hadoop.hbase.security.access.Permission$Action);\n  public boolean authorizeCell(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.security.access.Permission$Action);\n  public void removeNamespace(byte[]);\n  public void removeTable(org.apache.hadoop.hbase.TableName);\n  public long getMTime();\n}\n;;;No, this class is not a message definition. It appears to be a Java class defining a set of methods for managing authentication and access control in HBase. While the methods could potentially be invoked as part of a message processing pipeline, the class itself does not define a specific message format or protocol.;;;N;;;No, it is not a task definition. It is a class that defines methods for managing authentication and authorization in HBase.;;;N
org/apache/hadoop/hbase/security/access/AuthResult$Params.class;;;public class org.apache.hadoop.hbase.security.access.AuthResult$Params {\n  public org.apache.hadoop.hbase.security.access.AuthResult$Params();\n  public org.apache.hadoop.hbase.security.access.AuthResult$Params addExtraParam(java.lang.String, java.lang.String);\n  public org.apache.hadoop.hbase.security.access.AuthResult$Params setNamespace(java.lang.String);\n  public org.apache.hadoop.hbase.security.access.AuthResult$Params setTableName(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.security.access.AuthResult$Params setFamilies(java.util.Map<byte[], ? extends java.util.Collection<?>>);\n  public org.apache.hadoop.hbase.security.access.AuthResult$Params setFamily(byte[]);\n  public org.apache.hadoop.hbase.security.access.AuthResult$Params setQualifier(byte[]);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/AuthResult.class;;;public class org.apache.hadoop.hbase.security.access.AuthResult {\n  public org.apache.hadoop.hbase.security.access.AuthResult(boolean, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, byte[], byte[]);\n  public org.apache.hadoop.hbase.security.access.AuthResult(boolean, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>);\n  public org.apache.hadoop.hbase.security.access.AuthResult(boolean, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String);\n  public boolean isAllowed();\n  public org.apache.hadoop.hbase.security.User getUser();\n  public java.lang.String getReason();\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public byte[] getFamily();\n  public byte[] getQualifier();\n  public org.apache.hadoop.hbase.security.access.Permission$Action getAction();\n  public java.lang.String getRequest();\n  public org.apache.hadoop.hbase.security.access.AuthResult$Params getParams();\n  public void setAllowed(boolean);\n  public void setReason(java.lang.String);\n  public java.lang.String toContextString();\n  public java.lang.String toString();\n  public static org.apache.hadoop.hbase.security.access.AuthResult allow(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String);\n  public static org.apache.hadoop.hbase.security.access.AuthResult allow(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, byte[], byte[]);\n  public static org.apache.hadoop.hbase.security.access.AuthResult allow(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>);\n  public static org.apache.hadoop.hbase.security.access.AuthResult deny(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String);\n  public static org.apache.hadoop.hbase.security.access.AuthResult deny(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, byte[], byte[]);\n  public static org.apache.hadoop.hbase.security.access.AuthResult deny(java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>);\n  public java.lang.String toFamilyString();\n}\n;;;No. While this class is related to authorization and access control, it is not a clear message definition with fields that represent data to be transferred and processed by different systems in a uniform way. It has several methods that perform actions and return results, as well as methods that create instances of the class.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/CoprocessorWhitelistMasterObserver.class;;;public class org.apache.hadoop.hbase.security.access.CoprocessorWhitelistMasterObserver implements org.apache.hadoop.hbase.coprocessor.MasterCoprocessor,org.apache.hadoop.hbase.coprocessor.MasterObserver {\n  public static final java.lang.String CP_COPROCESSOR_WHITELIST_PATHS_KEY;\n  public org.apache.hadoop.hbase.security.access.CoprocessorWhitelistMasterObserver();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.MasterObserver> getMasterObserver();\n  public org.apache.hadoop.hbase.client.TableDescriptor preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preCreateTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/access/NoopAccessChecker.class;;;public final class org.apache.hadoop.hbase.security.access.NoopAccessChecker extends org.apache.hadoop.hbase.security.access.AccessChecker {\n  public org.apache.hadoop.hbase.security.access.NoopAccessChecker(org.apache.hadoop.conf.Configuration) throws java.lang.RuntimeException;\n  public void requireAccess(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.security.access.Permission$Action...);\n  public void requirePermission(org.apache.hadoop.hbase.security.User, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action);\n  public void requireGlobalPermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<byte[]>>, java.lang.String);\n  public void requireGlobalPermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action, java.lang.String);\n  public void requireNamespacePermission(org.apache.hadoop.hbase.security.User, java.lang.String, java.lang.String, java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action...);\n  public void requireNamespacePermission(org.apache.hadoop.hbase.security.User, java.lang.String, java.lang.String, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<byte[]>>, org.apache.hadoop.hbase.security.access.Permission$Action...);\n  public void requirePermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action...);\n  public void requireTablePermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.TableName, byte[], byte[], org.apache.hadoop.hbase.security.access.Permission$Action...);\n  public void performOnSuperuser(java.lang.String, org.apache.hadoop.hbase.security.User, java.lang.String);\n  public void checkLockPermissions(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo[], java.lang.String);\n  public boolean hasUserPermission(org.apache.hadoop.hbase.security.User, java.lang.String, org.apache.hadoop.hbase.security.access.Permission);\n  public org.apache.hadoop.hbase.security.access.AuthResult permissionGranted(java.lang.String, org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.access.Permission$Action, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], ? extends java.util.Collection<?>>);\n}\n;;;Yes, it is a message definition related to access control for HBase tables.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/access/PermissionStorage.class;;;public final class org.apache.hadoop.hbase.security.access.PermissionStorage {\n  public static final org.apache.hadoop.hbase.TableName ACL_TABLE_NAME;\n  public static final byte[] ACL_GLOBAL_NAME;\n  public static final java.lang.String ACL_LIST_FAMILY_STR;\n  public static final byte[] ACL_LIST_FAMILY;\n  public static final byte ACL_TAG_TYPE;\n  public static final char NAMESPACE_PREFIX;\n  public static final char ACL_KEY_DELIMITER;\n  public static void addUserPermission(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.security.access.UserPermission, org.apache.hadoop.hbase.client.Table, boolean) throws java.io.IOException;\n  public static void removeUserPermission(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.security.access.UserPermission, org.apache.hadoop.hbase.client.Table) throws java.io.IOException;\n  public static org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.UserPermission> getTablePermissions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.UserPermission> getNamespacePermissions(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public static org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.UserPermission> getGlobalPermissions(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.security.access.UserPermission> getUserTablePermissions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, byte[], byte[], java.lang.String, boolean) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.security.access.UserPermission> getUserNamespacePermissions(org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, boolean) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.security.access.UserPermission> getUserPermissions(org.apache.hadoop.conf.Configuration, byte[], byte[], byte[], java.lang.String, boolean) throws java.io.IOException;\n  public static byte[] writePermissionsAsBytes(org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.UserPermission>, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.UserPermission> readUserPermission(byte[], org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public static org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.Permission> readPermissions(byte[], org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public static boolean isGlobalEntry(byte[]);\n  public static boolean isNamespaceEntry(java.lang.String);\n  public static boolean isNamespaceEntry(byte[]);\n  public static boolean isTableEntry(byte[]);\n  public static java.lang.String toNamespaceEntry(java.lang.String);\n  public static java.lang.String fromNamespaceEntry(java.lang.String);\n  public static byte[] toNamespaceEntry(byte[]);\n  public static byte[] fromNamespaceEntry(byte[]);\n  public static java.util.List<org.apache.hadoop.hbase.security.access.Permission> getCellPermissionsForUser(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclCleaner.class;;;public class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate {\n  public org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclCleaner();\n  public void init(java.util.Map<java.lang.String, java.lang.Object>);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public boolean isEmptyDirDeletable(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController$1.class;;;class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclController$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController$SnapshotScannerHDFSAclStorage.class;;;final class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclController$SnapshotScannerHDFSAclStorage {\n}\n;;;No. This class is not a message definition and is not related to message queuing. It is a class definition for a specific purpose within the HBase security access module.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclController.class;;;public class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclController implements org.apache.hadoop.hbase.coprocessor.MasterCoprocessor,org.apache.hadoop.hbase.coprocessor.MasterObserver {\n  public org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclController();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.MasterObserver> getMasterObserver();\n  public void preMasterInitialization(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public void preStopMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>);\n  public void postCompletedCreateTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[]) throws java.io.IOException;\n  public void postCreateNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.NamespaceDescriptor) throws java.io.IOException;\n  public void postCompletedSnapshotAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void postCompletedTruncateTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postCompletedDeleteTableAction(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void postDeleteNamespace(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, java.lang.String) throws java.io.IOException;\n  public void postGrant(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.security.access.UserPermission, boolean) throws java.io.IOException;\n  public void postRevoke(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.security.access.UserPermission) throws java.io.IOException;\n}\n;;;No. This class is an implementation of the MasterCoprocessor and MasterObserver interfaces in HBase. It is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper$1.class;;;class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper$HDFSAclOperation$AclType.class;;;final class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$AclType extends java.lang.Enum<org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$AclType> {\n  public static final org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$AclType ACCESS;\n  public static final org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$AclType DEFAULT;\n  public static final org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$AclType DEFAULT_ADN_ACCESS;\n  public static org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$AclType[] values();\n  public static org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$AclType valueOf(java.lang.String);\n}\n;;;No. This is a Java class definition and does not define any message or message format that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper$HDFSAclOperation$Operation.class;;;interface org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$Operation {\n  public abstract void apply(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper$HDFSAclOperation$OperationType.class;;;final class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$OperationType extends java.lang.Enum<org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$OperationType> {\n  public static final org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$OperationType MODIFY;\n  public static final org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$OperationType REMOVE;\n  public static org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$OperationType[] values();\n  public static org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation$OperationType valueOf(java.lang.String);\n}\n;;;No. This class is an enumeration used for internal implementation in the HBase security access package and does not appear to be a message definition for a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper$HDFSAclOperation.class;;;class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$HDFSAclOperation {\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition in the HBase security access package.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper$PathHelper.class;;;final class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper$PathHelper {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/SnapshotScannerHDFSAclHelper.class;;;public class org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper implements java.io.Closeable {\n  public static final java.lang.String ACL_SYNC_TO_HDFS_ENABLE;\n  public static final java.lang.String ACL_SYNC_TO_HDFS_THREAD_NUMBER;\n  public static final java.lang.String SNAPSHOT_RESTORE_TMP_DIR;\n  public static final java.lang.String SNAPSHOT_RESTORE_TMP_DIR_DEFAULT;\n  public static final java.lang.String COMMON_DIRECTORY_PERMISSION;\n  public static final java.lang.String COMMON_DIRECTORY_PERMISSION_DEFAULT;\n  public static final java.lang.String SNAPSHOT_RESTORE_DIRECTORY_PERMISSION;\n  public static final java.lang.String SNAPSHOT_RESTORE_DIRECTORY_PERMISSION_DEFAULT;\n  public org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public void close();\n  public void setCommonDirectoryPermission() throws java.io.IOException;\n  public boolean grantAcl(org.apache.hadoop.hbase.security.access.UserPermission, java.util.Set<java.lang.String>, java.util.Set<org.apache.hadoop.hbase.TableName>);\n  public boolean revokeAcl(org.apache.hadoop.hbase.security.access.UserPermission, java.util.Set<java.lang.String>, java.util.Set<org.apache.hadoop.hbase.TableName>);\n  public boolean snapshotAcl(org.apache.hadoop.hbase.client.SnapshotDescription);\n  public boolean removeNamespaceAccessAcl(org.apache.hadoop.hbase.TableName, java.util.Set<java.lang.String>, java.lang.String);\n  public boolean removeNamespaceDefaultAcl(java.lang.String, java.util.Set<java.lang.String>);\n  public boolean removeTableDefaultAcl(org.apache.hadoop.hbase.TableName, java.util.Set<java.lang.String>);\n  public boolean addTableAcl(org.apache.hadoop.hbase.TableName, java.util.Set<java.lang.String>, java.lang.String);\n  public boolean removeTableAcl(org.apache.hadoop.hbase.TableName, java.util.Set<java.lang.String>);\n  public static boolean isAclSyncToHdfsEnabled(org.apache.hadoop.conf.Configuration);\n}\n;;;No. It is a class that provides a helper for working with HDFS ACLs in the Apache HBase security access feature, but it does not define a specific message that would be sent on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/access/ZKPermissionWatcher$1.class;;;class org.apache.hadoop.hbase.security.access.ZKPermissionWatcher$1 implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call() throws org.apache.zookeeper.KeeperException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No;;;N;;;Yes, it is a task definition that might be put on a task queue as it implements the Callable interface and has a call() method which represents the task that will be executed.;;;Y
org/apache/hadoop/hbase/security/access/ZKPermissionWatcher$2.class;;;class org.apache.hadoop.hbase.security.access.ZKPermissionWatcher$2 implements java.lang.Runnable {\n  public void run();\n}\n;;;no;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/security/access/ZKPermissionWatcher$3.class;;;class org.apache.hadoop.hbase.security.access.ZKPermissionWatcher$3 implements java.lang.Runnable {\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/security/access/ZKPermissionWatcher$4.class;;;class org.apache.hadoop.hbase.security.access.ZKPermissionWatcher$4 implements java.lang.Runnable {\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/security/access/ZKPermissionWatcher.class;;;public class org.apache.hadoop.hbase.security.access.ZKPermissionWatcher extends org.apache.hadoop.hbase.zookeeper.ZKListener implements java.io.Closeable {\n  public org.apache.hadoop.hbase.security.access.ZKPermissionWatcher(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.security.access.AuthManager, org.apache.hadoop.conf.Configuration);\n  public void start() throws org.apache.zookeeper.KeeperException;\n  public void close();\n  public void nodeCreated(java.lang.String);\n  public void nodeDeleted(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n  public void writeToZookeeper(byte[], byte[]);\n  public void deleteTableACLNode(org.apache.hadoop.hbase.TableName);\n  public void deleteNamespaceACLNode(java.lang.String);\n}\n;;;No, this is not a message definition. It is a class definition for a Java class that extends a base class and has several methods and constructors. It may be used as part of a messaging system, but it is not inherently a message definition itself.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/security/provider/AttemptingUserProvidingSaslServer.class;;;public class org.apache.hadoop.hbase.security.provider.AttemptingUserProvidingSaslServer {\n  public org.apache.hadoop.hbase.security.provider.AttemptingUserProvidingSaslServer(javax.security.sasl.SaslServer, java.util.function.Supplier<org.apache.hadoop.security.UserGroupInformation>);\n  public javax.security.sasl.SaslServer getServer();\n  public java.util.Optional<org.apache.hadoop.security.UserGroupInformation> getAttemptingUser();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/provider/DigestSaslServerAuthenticationProvider$SaslDigestCallbackHandler.class;;;class org.apache.hadoop.hbase.security.provider.DigestSaslServerAuthenticationProvider$SaslDigestCallbackHandler implements javax.security.auth.callback.CallbackHandler {\n  public org.apache.hadoop.hbase.security.provider.DigestSaslServerAuthenticationProvider$SaslDigestCallbackHandler(org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>, java.util.concurrent.atomic.AtomicReference<org.apache.hadoop.security.UserGroupInformation>);\n  public void handle(javax.security.auth.callback.Callback[]) throws org.apache.hadoop.security.token.SecretManager$InvalidToken, javax.security.auth.callback.UnsupportedCallbackException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/provider/DigestSaslServerAuthenticationProvider.class;;;public class org.apache.hadoop.hbase.security.provider.DigestSaslServerAuthenticationProvider extends org.apache.hadoop.hbase.security.provider.DigestSaslAuthenticationProvider implements org.apache.hadoop.hbase.security.provider.SaslServerAuthenticationProvider {\n  public org.apache.hadoop.hbase.security.provider.DigestSaslServerAuthenticationProvider();\n  public org.apache.hadoop.hbase.security.provider.AttemptingUserProvidingSaslServer createServer(org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public boolean supportsProtocolAuthentication();\n  public org.apache.hadoop.security.UserGroupInformation getAuthorizedUgi(java.lang.String, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n}\n;;;No, this class is not a message definition. It is a class definition for a Java class, which may be used in a messaging system but is not itself a message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/provider/GssSaslServerAuthenticationProvider$1.class;;;class org.apache.hadoop.hbase.security.provider.GssSaslServerAuthenticationProvider$1 implements java.security.PrivilegedExceptionAction<org.apache.hadoop.hbase.security.provider.AttemptingUserProvidingSaslServer> {\n  public org.apache.hadoop.hbase.security.provider.AttemptingUserProvidingSaslServer run() throws javax.security.sasl.SaslException;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/provider/GssSaslServerAuthenticationProvider$SaslGssCallbackHandler.class;;;class org.apache.hadoop.hbase.security.provider.GssSaslServerAuthenticationProvider$SaslGssCallbackHandler implements javax.security.auth.callback.CallbackHandler {\n  public void handle(javax.security.auth.callback.Callback[]) throws javax.security.auth.callback.UnsupportedCallbackException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/provider/GssSaslServerAuthenticationProvider.class;;;public class org.apache.hadoop.hbase.security.provider.GssSaslServerAuthenticationProvider extends org.apache.hadoop.hbase.security.provider.GssSaslAuthenticationProvider implements org.apache.hadoop.hbase.security.provider.SaslServerAuthenticationProvider {\n  public org.apache.hadoop.hbase.security.provider.GssSaslServerAuthenticationProvider();\n  public org.apache.hadoop.hbase.security.provider.AttemptingUserProvidingSaslServer createServer(org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public boolean supportsProtocolAuthentication();\n  public org.apache.hadoop.security.UserGroupInformation getAuthorizedUgi(java.lang.String, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n}\n;;;no;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/security/provider/SaslServerAuthenticationProvider.class;;;public interface org.apache.hadoop.hbase.security.provider.SaslServerAuthenticationProvider extends org.apache.hadoop.hbase.security.provider.SaslAuthenticationProvider {\n  public default void init(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.security.provider.AttemptingUserProvidingSaslServer createServer(org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public abstract boolean supportsProtocolAuthentication();\n  public abstract org.apache.hadoop.security.UserGroupInformation getAuthorizedUgi(java.lang.String, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n}\n;;;No, it is an interface definition for a Hadoop library's authentication provider. It cannot be put on a message queue as it is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/provider/SaslServerAuthenticationProviders.class;;;public final class org.apache.hadoop.hbase.security.provider.SaslServerAuthenticationProviders {\n  public static final java.lang.String EXTRA_PROVIDERS_KEY;\n  public int getNumRegisteredProviders();\n  public static org.apache.hadoop.hbase.security.provider.SaslServerAuthenticationProviders getInstance(org.apache.hadoop.conf.Configuration);\n  public static void reset();\n  public org.apache.hadoop.hbase.security.provider.SaslServerAuthenticationProvider selectProvider(byte);\n  public org.apache.hadoop.hbase.security.provider.SaslServerAuthenticationProvider getSimpleProvider();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/provider/SimpleSaslServerAuthenticationProvider.class;;;public class org.apache.hadoop.hbase.security.provider.SimpleSaslServerAuthenticationProvider extends org.apache.hadoop.hbase.security.provider.SimpleSaslAuthenticationProvider implements org.apache.hadoop.hbase.security.provider.SaslServerAuthenticationProvider {\n  public org.apache.hadoop.hbase.security.provider.SimpleSaslServerAuthenticationProvider();\n  public org.apache.hadoop.hbase.security.provider.AttemptingUserProvidingSaslServer createServer(org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>, java.util.Map<java.lang.String, java.lang.String>) throws java.io.IOException;\n  public boolean supportsProtocolAuthentication();\n  public org.apache.hadoop.security.UserGroupInformation getAuthorizedUgi(java.lang.String, org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.security.token.TokenIdentifier>) throws java.io.IOException;\n}\n;;;No, this is a class definition for a Java class used in Apache Hadoop, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/token/AuthenticationKey.class;;;public class org.apache.hadoop.hbase.security.token.AuthenticationKey implements org.apache.hadoop.io.Writable {\n  public org.apache.hadoop.hbase.security.token.AuthenticationKey();\n  public org.apache.hadoop.hbase.security.token.AuthenticationKey(int, long, javax.crypto.SecretKey);\n  public int getKeyId();\n  public long getExpiration();\n  public void setExpiration(long);\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n  public void write(java.io.DataOutput) throws java.io.IOException;\n  public void readFields(java.io.DataInput) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager$LeaderElector.class;;;class org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector extends java.lang.Thread implements org.apache.hadoop.hbase.Stoppable {\n  public org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector(org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager, org.apache.hadoop.hbase.zookeeper.ZKWatcher, java.lang.String);\n  public boolean isMaster();\n  public boolean isStopped();\n  public void stop(java.lang.String);\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/token/AuthenticationTokenSecretManager.class;;;public class org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager extends org.apache.hadoop.security.token.SecretManager<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier> {\n  public org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher, java.lang.String, long, long);\n  public void start();\n  public void stop();\n  public boolean isMaster();\n  public java.lang.String getName();\n  public byte[] retrievePassword(org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n  public org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier createIdentifier();\n  public org.apache.hadoop.security.token.Token<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier> generateToken(java.lang.String);\n  public synchronized void addKey(org.apache.hadoop.hbase.security.token.AuthenticationKey) throws java.io.IOException;\n  public static javax.crypto.SecretKey createSecretKey(byte[]);\n  public org.apache.hadoop.security.token.TokenIdentifier createIdentifier();\n  public byte[] retrievePassword(org.apache.hadoop.security.token.TokenIdentifier) throws org.apache.hadoop.security.token.SecretManager$InvalidToken;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/token/FsDelegationToken.class;;;public class org.apache.hadoop.hbase.security.token.FsDelegationToken {\n  public org.apache.hadoop.hbase.security.token.FsDelegationToken(org.apache.hadoop.hbase.security.UserProvider, java.lang.String);\n  public void acquireDelegationToken(org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public void acquireDelegationToken(java.lang.String, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public void releaseDelegationToken();\n  public org.apache.hadoop.hbase.security.UserProvider getUserProvider();\n  public java.lang.String getRenewer();\n  public org.apache.hadoop.security.token.Token<?> getUserToken();\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/security/token/TokenProvider.class;;;public class org.apache.hadoop.hbase.security.token.TokenProvider implements org.apache.hadoop.hbase.shaded.protobuf.generated.AuthenticationProtos$AuthenticationService$Interface,org.apache.hadoop.hbase.coprocessor.RegionCoprocessor {\n  public org.apache.hadoop.hbase.security.token.TokenProvider();\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment);\n  public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public java.lang.Iterable<org.apache.hbase.thirdparty.com.google.protobuf.Service> getServices();\n  public void getAuthenticationToken(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AuthenticationProtos$GetAuthenticationTokenRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.AuthenticationProtos$GetAuthenticationTokenResponse>);\n  public void whoAmI(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AuthenticationProtos$WhoAmIRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.AuthenticationProtos$WhoAmIResponse>);\n}\n;;;yes;;;Y;;;;;;N
org/apache/hadoop/hbase/security/token/TokenUtil.class;;;public class org.apache.hadoop.hbase.security.token.TokenUtil {\n  public org.apache.hadoop.hbase.security.token.TokenUtil();\n  public static java.util.concurrent.CompletableFuture<org.apache.hadoop.security.token.Token<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier>> obtainToken(org.apache.hadoop.hbase.client.AsyncConnection);\n  public static org.apache.hadoop.security.token.Token<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier> obtainToken(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.security.token.Token<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier> obtainToken(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.AuthenticationProtos$Token toToken(org.apache.hadoop.security.token.Token<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier>);\n  public static org.apache.hadoop.security.token.Token<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier> obtainToken(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException;\n  public static void obtainAndCacheToken(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.security.token.Token<org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier> toToken(org.apache.hadoop.hbase.shaded.protobuf.generated.AuthenticationProtos$Token);\n  public static void obtainTokenForJob(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.security.User, org.apache.hadoop.mapreduce.Job) throws java.io.IOException, java.lang.InterruptedException;\n  public static void obtainTokenForJob(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.mapred.JobConf, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException;\n  public static void addTokenForJob(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.mapred.JobConf, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException;\n  public static void addTokenForJob(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.security.User, org.apache.hadoop.mapreduce.Job) throws java.io.IOException, java.lang.InterruptedException;\n  public static boolean addTokenIfMissing(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException;\n}\n;;;No, this class is a utility class for obtaining and managing authentication tokens in HBase. It does not define a message that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/token/ZKSecretWatcher.class;;;public class org.apache.hadoop.hbase.security.token.ZKSecretWatcher extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public org.apache.hadoop.hbase.security.token.ZKSecretWatcher(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager);\n  public void start() throws org.apache.zookeeper.KeeperException;\n  public void nodeCreated(java.lang.String);\n  public void nodeDeleted(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n  public java.lang.String getRootKeyZNode();\n  public void removeKeyFromZK(org.apache.hadoop.hbase.security.token.AuthenticationKey);\n  public void addKeyToZK(org.apache.hadoop.hbase.security.token.AuthenticationKey);\n  public void updateKeyInZK(org.apache.hadoop.hbase.security.token.AuthenticationKey);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl$1.class;;;class org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl$1 implements org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator {\n  public boolean evaluate(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl$2.class;;;class org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl$2 implements org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator {\n  public boolean evaluate(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/DefaultVisibilityLabelServiceImpl.class;;;public class org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl implements org.apache.hadoop.hbase.security.visibility.VisibilityLabelService {\n  public org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void init(org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.OperationStatus[] addLabels(java.util.List<byte[]>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.OperationStatus[] setAuths(byte[], java.util.List<byte[]>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.OperationStatus[] clearAuths(byte[], java.util.List<byte[]>) throws java.io.IOException;\n  public java.util.List<java.lang.String> getUserAuths(byte[], boolean) throws java.io.IOException;\n  public java.util.List<java.lang.String> getGroupAuths(java.lang.String[], boolean) throws java.io.IOException;\n  public java.util.List<java.lang.String> listLabels(java.lang.String) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.Tag> createVisibilityExpTags(java.lang.String, boolean, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator getVisibilityExpEvaluator(org.apache.hadoop.hbase.security.visibility.Authorizations) throws java.io.IOException;\n  public boolean havingSystemAuth(org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public boolean matchVisibility(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte, java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte) throws java.io.IOException;\n  public byte[] encodeVisibilityForReplication(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte) throws java.io.IOException;\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/DefinedSetFilterScanLabelGenerator.class;;;public class org.apache.hadoop.hbase.security.visibility.DefinedSetFilterScanLabelGenerator implements org.apache.hadoop.hbase.security.visibility.ScanLabelGenerator {\n  public org.apache.hadoop.hbase.security.visibility.DefinedSetFilterScanLabelGenerator();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public java.util.List<java.lang.String> getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/EnforcingScanLabelGenerator.class;;;public class org.apache.hadoop.hbase.security.visibility.EnforcingScanLabelGenerator implements org.apache.hadoop.hbase.security.visibility.ScanLabelGenerator {\n  public org.apache.hadoop.hbase.security.visibility.EnforcingScanLabelGenerator();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public java.util.List<java.lang.String> getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/ExpressionExpander.class;;;public class org.apache.hadoop.hbase.security.visibility.ExpressionExpander {\n  public org.apache.hadoop.hbase.security.visibility.ExpressionExpander();\n  public org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode expand(org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/ExpressionParser.class;;;public class org.apache.hadoop.hbase.security.visibility.ExpressionParser {\n  public org.apache.hadoop.hbase.security.visibility.ExpressionParser();\n  public org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode parse(java.lang.String) throws org.apache.hadoop.hbase.security.visibility.ParseException;\n}\n;;;No. This class is a parser for parsing security expressions in Hadoop HBase. It does not define a message structure or content, nor is it related to message queues.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/FeedUserAuthScanLabelGenerator.class;;;public class org.apache.hadoop.hbase.security.visibility.FeedUserAuthScanLabelGenerator implements org.apache.hadoop.hbase.security.visibility.ScanLabelGenerator {\n  public org.apache.hadoop.hbase.security.visibility.FeedUserAuthScanLabelGenerator();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public java.util.List<java.lang.String> getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/ParseException.class;;;public class org.apache.hadoop.hbase.security.visibility.ParseException extends java.lang.Exception {\n  public org.apache.hadoop.hbase.security.visibility.ParseException();\n  public org.apache.hadoop.hbase.security.visibility.ParseException(java.lang.String);\n  public org.apache.hadoop.hbase.security.visibility.ParseException(java.lang.Throwable);\n  public org.apache.hadoop.hbase.security.visibility.ParseException(java.lang.String, java.lang.Throwable);\n}\n;;;No, this is not a message definition. It is a class definition for an exception in the Hadoop HBase security visibility framework.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/ScanLabelGenerator.class;;;public interface org.apache.hadoop.hbase.security.visibility.ScanLabelGenerator extends org.apache.hadoop.conf.Configurable {\n  public abstract java.util.List<java.lang.String> getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/SimpleScanLabelGenerator.class;;;public class org.apache.hadoop.hbase.security.visibility.SimpleScanLabelGenerator implements org.apache.hadoop.hbase.security.visibility.ScanLabelGenerator {\n  public org.apache.hadoop.hbase.security.visibility.SimpleScanLabelGenerator();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public java.util.List<java.lang.String> getLabels(org.apache.hadoop.hbase.security.User, org.apache.hadoop.hbase.security.visibility.Authorizations);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityController$DeleteVersionVisibilityExpressionFilter.class;;;class org.apache.hadoop.hbase.security.visibility.VisibilityController$DeleteVersionVisibilityExpressionFilter extends org.apache.hadoop.hbase.filter.FilterBase {\n  public org.apache.hadoop.hbase.security.visibility.VisibilityController$DeleteVersionVisibilityExpressionFilter(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte);\n  public boolean filterRowKey(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.filter.Filter$ReturnCode filterCell(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityController.class;;;public class org.apache.hadoop.hbase.security.visibility.VisibilityController implements org.apache.hadoop.hbase.coprocessor.MasterCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionCoprocessor,org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsService$Interface,org.apache.hadoop.hbase.coprocessor.MasterObserver,org.apache.hadoop.hbase.coprocessor.RegionObserver {\n  public org.apache.hadoop.hbase.security.visibility.VisibilityController();\n  public static boolean isCellAuthorizationSupported(org.apache.hadoop.conf.Configuration);\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionObserver> getRegionObserver();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.MasterObserver> getMasterObserver();\n  public java.lang.Iterable<org.apache.hbase.thirdparty.com.google.protobuf.Service> getServices();\n  public void postStartMaster(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableDescriptor preModifyTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void preDisableTable(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public void postOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>);\n  public void postSetSplitOrMergeEnabled(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment>, boolean, org.apache.hadoop.hbase.client.MasterSwitchType) throws java.io.IOException;\n  public void preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n  public void prePrepareTimeStampForDeleteVersion(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, org.apache.hadoop.hbase.Cell, byte[], org.apache.hadoop.hbase.client.Get) throws java.io.IOException;\n  public void preScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker postInstantiateDeleteTracker(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.RegionScanner postScannerOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.regionserver.RegionScanner) throws java.io.IOException;\n  public boolean preScannerNext(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner, java.util.List<org.apache.hadoop.hbase.client.Result>, int, boolean) throws java.io.IOException;\n  public void preScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public void postScannerClose(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.InternalScanner) throws java.io.IOException;\n  public void preGetOp(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Get, java.util.List<org.apache.hadoop.hbase.Cell>) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> postIncrementBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>> postAppendBeforeWAL(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Mutation, java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell>>) throws java.io.IOException;\n  public synchronized void addLabels(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsResponse>);\n  public synchronized void setAuths(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$SetAuthsRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsResponse>);\n  public synchronized void getAuths(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$GetAuthsRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$GetAuthsResponse>);\n  public synchronized void clearAuths(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$SetAuthsRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$VisibilityLabelsResponse>);\n  public synchronized void listLabels(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$ListLabelsRequest, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$ListLabelsResponse>);\n}\n;;;Yes, it is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityExpEvaluator.class;;;public interface org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator {\n  public abstract boolean evaluate(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityLabelFilter.class;;;class org.apache.hadoop.hbase.security.visibility.VisibilityLabelFilter extends org.apache.hadoop.hbase.filter.FilterBase {\n  public org.apache.hadoop.hbase.security.visibility.VisibilityLabelFilter(org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator, java.util.Map<org.apache.hadoop.hbase.util.ByteRange, java.lang.Integer>);\n  public boolean filterRowKey(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public org.apache.hadoop.hbase.filter.Filter$ReturnCode filterCell(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public void reset() throws java.io.IOException;\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;Yes, this class is a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityLabelOrdinalProvider.class;;;public interface org.apache.hadoop.hbase.security.visibility.VisibilityLabelOrdinalProvider {\n  public abstract int getLabelOrdinal(java.lang.String);\n  public abstract java.lang.String getLabel(int);\n}\n;;;No, it is an interface for a security feature in Hadoop HBase. It does not define a message that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityLabelService.class;;;public interface org.apache.hadoop.hbase.security.visibility.VisibilityLabelService extends org.apache.hadoop.conf.Configurable {\n  public abstract void init(org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.OperationStatus[] addLabels(java.util.List<byte[]>) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.OperationStatus[] setAuths(byte[], java.util.List<byte[]>) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.regionserver.OperationStatus[] clearAuths(byte[], java.util.List<byte[]>) throws java.io.IOException;\n  public abstract java.util.List<java.lang.String> getUserAuths(byte[], boolean) throws java.io.IOException;\n  public abstract java.util.List<java.lang.String> getGroupAuths(java.lang.String[], boolean) throws java.io.IOException;\n  public abstract java.util.List<java.lang.String> listLabels(java.lang.String) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.Tag> createVisibilityExpTags(java.lang.String, boolean, boolean) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.security.visibility.VisibilityExpEvaluator getVisibilityExpEvaluator(org.apache.hadoop.hbase.security.visibility.Authorizations) throws java.io.IOException;\n  public abstract boolean havingSystemAuth(org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public abstract boolean matchVisibility(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte, java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte) throws java.io.IOException;\n  public abstract byte[] encodeVisibilityForReplication(java.util.List<org.apache.hadoop.hbase.Tag>, java.lang.Byte) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityLabelServiceManager.class;;;public class org.apache.hadoop.hbase.security.visibility.VisibilityLabelServiceManager {\n  public static final java.lang.String VISIBILITY_LABEL_SERVICE_CLASS;\n  public static org.apache.hadoop.hbase.security.visibility.VisibilityLabelServiceManager getInstance();\n  public org.apache.hadoop.hbase.security.visibility.VisibilityLabelService getVisibilityLabelService(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.security.visibility.VisibilityLabelService getVisibilityLabelService();\n}\n;;;Yes, this class might be put on a message queue as it contains methods that can be invoked to get an instance of VisibilityLabelServiceManager and VisibilityLabelService.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityLabelsCache.class;;;public class org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache implements org.apache.hadoop.hbase.security.visibility.VisibilityLabelOrdinalProvider {\n  public static synchronized org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache createAndGet(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache get();\n  public void refreshLabelsCache(byte[]) throws java.io.IOException;\n  public void refreshUserAuthsCache(byte[]) throws java.io.IOException;\n  public int getLabelOrdinal(java.lang.String);\n  public java.lang.String getLabel(int);\n  public int getLabelsCount();\n  public java.util.List<java.lang.String> getUserAuths(java.lang.String);\n  public java.util.List<java.lang.String> getGroupAuths(java.lang.String[]);\n  public java.util.Set<java.lang.Integer> getUserAuthsAsOrdinals(java.lang.String);\n  public java.util.Set<java.lang.Integer> getGroupAuthsAsOrdinals(java.lang.String[]);\n  public void writeToZookeeper(byte[], boolean) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityNewVersionBehaivorTracker$1.class;;;class org.apache.hadoop.hbase.security.visibility.VisibilityNewVersionBehaivorTracker$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityNewVersionBehaivorTracker$TagInfo.class;;;class org.apache.hadoop.hbase.security.visibility.VisibilityNewVersionBehaivorTracker$TagInfo {\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityNewVersionBehaivorTracker$VisibilityDeleteVersionsNode.class;;;class org.apache.hadoop.hbase.security.visibility.VisibilityNewVersionBehaivorTracker$VisibilityDeleteVersionsNode extends org.apache.hadoop.hbase.regionserver.querymatcher.NewVersionBehaviorTracker$DeleteVersionsNode {\n  public void addVersionDelete(org.apache.hadoop.hbase.Cell);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityNewVersionBehaivorTracker.class;;;public class org.apache.hadoop.hbase.security.visibility.VisibilityNewVersionBehaivorTracker extends org.apache.hadoop.hbase.regionserver.querymatcher.NewVersionBehaviorTracker {\n  public org.apache.hadoop.hbase.security.visibility.VisibilityNewVersionBehaivorTracker(java.util.NavigableSet<byte[]>, org.apache.hadoop.hbase.CellComparator, int, int, int, long);\n  public void add(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult isDeleted(org.apache.hadoop.hbase.Cell);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityReplication.class;;;public class org.apache.hadoop.hbase.security.visibility.VisibilityReplication implements org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionServerObserver {\n  public org.apache.hadoop.hbase.security.visibility.VisibilityReplication();\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionServerObserver> getRegionServerObserver();\n  public org.apache.hadoop.hbase.replication.ReplicationEndpoint postCreateReplicationEndPoint(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessorEnvironment>, org.apache.hadoop.hbase.replication.ReplicationEndpoint);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityReplicationEndpoint.class;;;public class org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint implements org.apache.hadoop.hbase.replication.ReplicationEndpoint {\n  public org.apache.hadoop.hbase.security.visibility.VisibilityReplicationEndpoint(org.apache.hadoop.hbase.replication.ReplicationEndpoint, org.apache.hadoop.hbase.security.visibility.VisibilityLabelService);\n  public void init(org.apache.hadoop.hbase.replication.ReplicationEndpoint$Context) throws java.io.IOException;\n  public void peerConfigUpdated(org.apache.hadoop.hbase.replication.ReplicationPeerConfig);\n  public boolean replicate(org.apache.hadoop.hbase.replication.ReplicationEndpoint$ReplicateContext);\n  public synchronized java.util.UUID getPeerUUID();\n  public boolean canReplicateToSameCluster();\n  public org.apache.hadoop.hbase.replication.WALEntryFilter getWALEntryfilter();\n  public boolean isRunning();\n  public boolean isStarting();\n  public void start();\n  public void awaitRunning();\n  public void awaitRunning(long, java.util.concurrent.TimeUnit) throws java.util.concurrent.TimeoutException;\n  public void stop();\n  public void awaitTerminated();\n  public void awaitTerminated(long, java.util.concurrent.TimeUnit) throws java.util.concurrent.TimeoutException;\n  public java.lang.Throwable failureCause();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker$1.class;;;class org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityScanDeleteTracker.class;;;public class org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker extends org.apache.hadoop.hbase.regionserver.querymatcher.ScanDeleteTracker {\n  public org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker(org.apache.hadoop.hbase.CellComparator);\n  public void add(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.regionserver.querymatcher.DeleteTracker$DeleteResult isDeleted(org.apache.hadoop.hbase.Cell);\n  public void reset();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/VisibilityUtils.class;;;public class org.apache.hadoop.hbase.security.visibility.VisibilityUtils {\n  public static final java.lang.String VISIBILITY_LABEL_GENERATOR_CLASS;\n  public static final java.lang.String SYSTEM_LABEL;\n  public static final org.apache.hadoop.hbase.Tag SORTED_ORDINAL_SERIALIZATION_FORMAT_TAG;\n  public org.apache.hadoop.hbase.security.visibility.VisibilityUtils();\n  public static byte[] getDataToWriteToZooKeeper(java.util.Map<java.lang.String, java.lang.Integer>);\n  public static byte[] getUserAuthsDataToWriteToZooKeeper(java.util.Map<java.lang.String, java.util.List<java.lang.Integer>>);\n  public static java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$VisibilityLabel> readLabelsFromZKData(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.VisibilityLabelsProtos$MultiUserAuthorizations readUserAuthsFromZKData(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public static java.util.List<org.apache.hadoop.hbase.security.visibility.ScanLabelGenerator> getScanLabelGenerators(org.apache.hadoop.conf.Configuration);\n  public static java.lang.Byte extractVisibilityTags(org.apache.hadoop.hbase.Cell, java.util.List<org.apache.hadoop.hbase.Tag>);\n  public static java.lang.Byte extractAndPartitionTags(org.apache.hadoop.hbase.Cell, java.util.List<org.apache.hadoop.hbase.Tag>, java.util.List<org.apache.hadoop.hbase.Tag>);\n  public static boolean isVisibilityTagsPresent(org.apache.hadoop.hbase.Cell);\n  public static org.apache.hadoop.hbase.filter.Filter createVisibilityLabelFilter(org.apache.hadoop.hbase.regionserver.Region, org.apache.hadoop.hbase.security.visibility.Authorizations) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.security.User getActiveUser() throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.Tag> createVisibilityExpTags(java.lang.String, boolean, boolean, java.util.Set<java.lang.Integer>, org.apache.hadoop.hbase.security.visibility.VisibilityLabelOrdinalProvider) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/ZKVisibilityLabelWatcher.class;;;public class org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher(org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache, org.apache.hadoop.conf.Configuration);\n  public void start() throws org.apache.zookeeper.KeeperException;\n  public void nodeCreated(java.lang.String);\n  public void nodeDeleted(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n  public void writeToZookeeper(byte[], boolean);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/security/visibility/expression/ExpressionNode.class;;;public interface org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode {\n  public abstract boolean isSingleNode();\n  public abstract org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode deepClone();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/expression/LeafExpressionNode.class;;;public class org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode implements org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode {\n  public static final org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode OPEN_PARAN_NODE;\n  public static final org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode CLOSE_PARAN_NODE;\n  public org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode(java.lang.String);\n  public java.lang.String getIdentifier();\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n  public boolean isSingleNode();\n  public org.apache.hadoop.hbase.security.visibility.expression.LeafExpressionNode deepClone();\n  public org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode deepClone();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/expression/NonLeafExpressionNode.class;;;public class org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode implements org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode {\n  public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode();\n  public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode(org.apache.hadoop.hbase.security.visibility.expression.Operator);\n  public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode(org.apache.hadoop.hbase.security.visibility.expression.Operator, java.util.List<org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode>);\n  public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode(org.apache.hadoop.hbase.security.visibility.expression.Operator, org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode...);\n  public org.apache.hadoop.hbase.security.visibility.expression.Operator getOperator();\n  public java.util.List<org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode> getChildExps();\n  public void addChildExp(org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode);\n  public void addChildExps(java.util.List<org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode>);\n  public java.lang.String toString();\n  public boolean isSingleNode();\n  public org.apache.hadoop.hbase.security.visibility.expression.NonLeafExpressionNode deepClone();\n  public org.apache.hadoop.hbase.security.visibility.expression.ExpressionNode deepClone();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/security/visibility/expression/Operator.class;;;public final class org.apache.hadoop.hbase.security.visibility.expression.Operator extends java.lang.Enum<org.apache.hadoop.hbase.security.visibility.expression.Operator> {\n  public static final org.apache.hadoop.hbase.security.visibility.expression.Operator AND;\n  public static final org.apache.hadoop.hbase.security.visibility.expression.Operator OR;\n  public static final org.apache.hadoop.hbase.security.visibility.expression.Operator NOT;\n  public static org.apache.hadoop.hbase.security.visibility.expression.Operator[] values();\n  public static org.apache.hadoop.hbase.security.visibility.expression.Operator valueOf(java.lang.String);\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/server/trace/IpcServerSpanBuilder.class;;;public class org.apache.hadoop.hbase.server.trace.IpcServerSpanBuilder implements java.util.function.Supplier<io.opentelemetry.api.trace.Span> {\n  public org.apache.hadoop.hbase.server.trace.IpcServerSpanBuilder(org.apache.hadoop.hbase.ipc.RpcCall);\n  public io.opentelemetry.api.trace.Span get();\n  public org.apache.hadoop.hbase.server.trace.IpcServerSpanBuilder setName(java.lang.String);\n  public <T> org.apache.hadoop.hbase.server.trace.IpcServerSpanBuilder addAttribute(io.opentelemetry.api.common.AttributeKey<T>, T);\n  public io.opentelemetry.api.trace.Span build();\n  public java.lang.Object get();\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class that provides a builder for OpenTelemetry tracing spans in the Hadoop HBase server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/CreateSnapshot.class;;;public class org.apache.hadoop.hbase.snapshot.CreateSnapshot extends org.apache.hadoop.hbase.util.AbstractHBaseTool {\n  public org.apache.hadoop.hbase.snapshot.CreateSnapshot();\n  public static void main(java.lang.String[]);\n}\n;;;No. While this class may define a function that creates a snapshot, it is not a complete message definition with defined input/output fields and any necessary metadata. It also includes a main function, indicating that this is a standalone program rather than a message that might be put on a queue for processing.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper$1.class;;;class org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$1 implements org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask {\n  public void editRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper$2.class;;;class org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$2 implements org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask {\n  public void editRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper$3.class;;;class org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$3 implements org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask {\n  public void editRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper$4.class;;;class org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$4 implements org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask {\n  public void fillRegion(org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper$RestoreMetaChanges.class;;;public class org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges {\n  public org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges(org.apache.hadoop.hbase.client.TableDescriptor, java.util.Map<java.lang.String, org.apache.hadoop.hbase.util.Pair<java.lang.String, java.lang.String>>);\n  public org.apache.hadoop.hbase.client.TableDescriptor getTableDescriptor();\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.util.Pair<java.lang.String, java.lang.String>> getParentToChildrenPairMap();\n  public boolean hasRegionsToAdd();\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getRegionsToAdd();\n  public boolean hasRegionsToRestore();\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getRegionsToRestore();\n  public boolean hasRegionsToRemove();\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getRegionsToRemove();\n  public void updateMetaParentRegions(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/RestoreSnapshotHelper.class;;;public class org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper {\n  public org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, org.apache.hadoop.hbase.monitoring.MonitoredTask);\n  public org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher, org.apache.hadoop.hbase.monitoring.MonitoredTask, boolean);\n  public org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges restoreHdfsRegions() throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.RegionInfo cloneRegionInfo(org.apache.hadoop.hbase.client.RegionInfo);\n  public static org.apache.hadoop.hbase.client.RegionInfo cloneRegionInfo(org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.RegionInfo);\n  public static org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper$RestoreMetaChanges copySnapshotForScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n  public static void restoreSnapshotAcl(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.TableName, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n}\n;;;No. It is a helper class for restoring HBase snapshots and does not define any message structure that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils$1.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils$1 implements java.security.PrivilegedExceptionAction<org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.UserPermission>> {\n  public org.apache.hbase.thirdparty.com.google.common.collect.ListMultimap<java.lang.String, org.apache.hadoop.hbase.security.access.UserPermission> run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils$2.class;;;class org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils$2 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils$CompletedSnaphotDirectoriesFilter.class;;;public class org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils$CompletedSnaphotDirectoriesFilter extends org.apache.hadoop.hbase.util.FSUtils$BlackListDirFilter {\n  public org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils$CompletedSnaphotDirectoriesFilter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotDescriptionUtils.class;;;public final class org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils {\n  public static final int SNAPSHOT_LAYOUT_VERSION;\n  public static final java.lang.String SNAPSHOTINFO_FILE;\n  public static final java.lang.String SNAPSHOT_TMP_DIR_NAME;\n  public static final java.lang.String SNAPSHOT_WORKING_DIR;\n  public static final long NO_SNAPSHOT_START_TIME_SPECIFIED;\n  public static final java.lang.String MASTER_SNAPSHOT_TIMEOUT_MILLIS;\n  public static final long DEFAULT_MAX_WAIT_TIME;\n  public static final java.lang.String SNAPSHOT_CORRUPTED_FILE;\n  public static long getMaxMasterTimeout(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription$Type, long);\n  public static org.apache.hadoop.fs.Path getSnapshotRootDir(org.apache.hadoop.fs.Path);\n  public static org.apache.hadoop.fs.Path getCompletedSnapshotDir(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.fs.Path);\n  public static org.apache.hadoop.fs.Path getCompletedSnapshotDir(java.lang.String, org.apache.hadoop.fs.Path);\n  public static org.apache.hadoop.fs.Path getWorkingSnapshotDir(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.fs.Path getWorkingSnapshotDir(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.fs.Path getWorkingSnapshotDir(java.lang.String, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.fs.Path getCorruptedFlagFileForSnapshot(org.apache.hadoop.fs.Path);\n  public static final org.apache.hadoop.fs.Path getSnapshotsDir(org.apache.hadoop.fs.Path);\n  public static boolean isSubDirectoryOf(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public static boolean isWithinDefaultWorkingDir(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription validate(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.conf.Configuration) throws java.lang.IllegalArgumentException, java.io.IOException;\n  public static void writeSnapshotInfo(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription readSnapshotInfo(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException;\n  public static void completeSnapshot(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.snapshot.SnapshotCreationException, java.io.IOException;\n  public static boolean isSnapshotOwner(org.apache.hadoop.hbase.client.SnapshotDescription, org.apache.hadoop.hbase.security.User);\n  public static boolean isSecurityAvailable(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n}\n;;;No, this class contains utility methods and constants for working with HBase snapshots, but it is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition, but rather a utility class with static methods.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotInfo$1.class;;;class org.apache.hadoop.hbase.snapshot.SnapshotInfo$1 implements org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor {\n  public void storeFile(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotInfo$2.class;;;class org.apache.hadoop.hbase.snapshot.SnapshotInfo$2 extends org.apache.hbase.thirdparty.org.apache.commons.cli.DefaultParser {\n  public org.apache.hbase.thirdparty.org.apache.commons.cli.CommandLine parse(org.apache.hbase.thirdparty.org.apache.commons.cli.Options, java.lang.String[], java.util.Properties, boolean) throws org.apache.hbase.thirdparty.org.apache.commons.cli.ParseException;\n}\n;;;No, it is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotInfo$3.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotInfo$3 implements org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor {\n  public void storeFile(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotInfo$4.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotInfo$4 implements org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor {\n  public void storeFile(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotInfo$Options.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotInfo$Options {\n}\n;;;No, it is a class definition for the Options class in the Apache HBase SnapshotInfo package, but it is not a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotInfo$SnapshotStats$FileInfo.class;;;class org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats$FileInfo {\n  public boolean inArchive();\n  public boolean isCorrupted();\n  public boolean isMissing();\n  public long getSize();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/snapshot/SnapshotInfo$SnapshotStats.class;;;public class org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats {\n  public org.apache.hadoop.hbase.client.SnapshotDescription getSnapshotDescription();\n  public boolean isSnapshotCorrupted();\n  public int getStoreFilesCount();\n  public int getArchivedStoreFilesCount();\n  public int getMobStoreFilesCount();\n  public int getLogsCount();\n  public int getMissingStoreFilesCount();\n  public int getCorruptedStoreFilesCount();\n  public int getMissingLogsCount();\n  public long getStoreFilesSize();\n  public long getSharedStoreFilesSize();\n  public long getArchivedStoreFileSize();\n  public long getMobStoreFilesSize();\n  public long getNonSharedArchivedStoreFilesSize();\n  public float getSharedStoreFilePercentage();\n  public float getMobStoreFilePercentage();\n  public long getLogsSize();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/snapshot/SnapshotInfo.class;;;public final class org.apache.hadoop.hbase.snapshot.SnapshotInfo extends org.apache.hadoop.hbase.util.AbstractHBaseTool {\n  public org.apache.hadoop.hbase.snapshot.SnapshotInfo();\n  public int doWork() throws java.io.IOException, java.lang.InterruptedException;\n  public static org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats getSnapshotStats(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.SnapshotDescription) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.snapshot.SnapshotInfo$SnapshotStats getSnapshotStats(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, java.util.Map<org.apache.hadoop.fs.Path, java.lang.Integer>) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.SnapshotDescription> getSnapshotList(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.util.Map<org.apache.hadoop.fs.Path, java.lang.Integer> getSnapshotsFilesMap(org.apache.hadoop.conf.Configuration, java.util.concurrent.atomic.AtomicLong, java.util.concurrent.atomic.AtomicLong, java.util.concurrent.atomic.AtomicLong) throws java.io.IOException;\n  public static void main(java.lang.String[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotManifest$RegionVisitor.class;;;interface org.apache.hadoop.hbase.snapshot.SnapshotManifest$RegionVisitor<TRegion, TFamily> {\n  public abstract TRegion regionOpen(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public abstract void regionClose(TRegion) throws java.io.IOException;\n  public abstract TFamily familyOpen(TRegion, byte[]) throws java.io.IOException;\n  public abstract void familyClose(TRegion, TFamily) throws java.io.IOException;\n  public abstract void storeFile(TRegion, TFamily, org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotManifest.class;;;public final class org.apache.hadoop.hbase.snapshot.SnapshotManifest {\n  public static final java.lang.String SNAPSHOT_MANIFEST_SIZE_LIMIT_CONF_KEY;\n  public static final java.lang.String DATA_MANIFEST_NAME;\n  public static org.apache.hadoop.hbase.snapshot.SnapshotManifest create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.snapshot.SnapshotManifest create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.errorhandling.ForeignExceptionSnare, org.apache.hadoop.hbase.monitoring.MonitoredTask) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.snapshot.SnapshotManifest open(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription) throws java.io.IOException;\n  public void addTableDescriptor(org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public void addMobRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void addRegion(org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException;\n  public void addRegion(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getSnapshotDir();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription getSnapshotDescription();\n  public org.apache.hadoop.hbase.client.TableDescriptor getTableDescriptor();\n  public java.util.List<org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest> getRegionManifests();\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest> getRegionManifestsMap();\n  public void consolidate() throws java.io.IOException;\n  public static java.util.concurrent.ThreadPoolExecutor createExecutor(org.apache.hadoop.conf.Configuration, java.lang.String);\n}\n;;;No. This is a class that provides methods for working with a snapshot manifest in Hadoop HBase. It is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotManifestV1$1.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$1 implements java.util.concurrent.Callable<org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest> {\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/snapshot/SnapshotManifestV1$ManifestBuilder.class;;;class org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder implements org.apache.hadoop.hbase.snapshot.SnapshotManifest$RegionVisitor<org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.fs.Path> {\n  public org.apache.hadoop.hbase.snapshot.SnapshotManifestV1$ManifestBuilder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.regionserver.HRegionFileSystem regionOpen(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void regionClose(org.apache.hadoop.hbase.regionserver.HRegionFileSystem);\n  public org.apache.hadoop.fs.Path familyOpen(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, byte[]);\n  public void familyClose(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.fs.Path);\n  public void storeFile(org.apache.hadoop.hbase.regionserver.HRegionFileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException;\n  public void storeFile(java.lang.Object, java.lang.Object, org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException;\n  public void familyClose(java.lang.Object, java.lang.Object) throws java.io.IOException;\n  public java.lang.Object familyOpen(java.lang.Object, byte[]) throws java.io.IOException;\n  public void regionClose(java.lang.Object) throws java.io.IOException;\n  public java.lang.Object regionOpen(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No. This is a class definition for a builder class in the Apache Hadoop HBase library. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotManifestV1.class;;;public final class org.apache.hadoop.hbase.snapshot.SnapshotManifestV1 {\n  public static final int DESCRIPTOR_VERSION;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/snapshot/SnapshotManifestV2$1.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$1 implements org.apache.hadoop.fs.PathFilter {\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotManifestV2$2.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$2 implements java.util.concurrent.Callable<org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest> {\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/snapshot/SnapshotManifestV2$ManifestBuilder.class;;;class org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder implements org.apache.hadoop.hbase.snapshot.SnapshotManifest$RegionVisitor<org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder> {\n  public org.apache.hadoop.hbase.snapshot.SnapshotManifestV2$ManifestBuilder(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder regionOpen(org.apache.hadoop.hbase.client.RegionInfo);\n  public void regionClose(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder) throws java.io.IOException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder familyOpen(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, byte[]);\n  public void familyClose(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder);\n  public void storeFile(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$Builder, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$FamilyFiles$Builder, org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException;\n  public void storeFile(java.lang.Object, java.lang.Object, org.apache.hadoop.hbase.regionserver.StoreFileInfo) throws java.io.IOException;\n  public void familyClose(java.lang.Object, java.lang.Object) throws java.io.IOException;\n  public java.lang.Object familyOpen(java.lang.Object, byte[]) throws java.io.IOException;\n  public void regionClose(java.lang.Object) throws java.io.IOException;\n  public java.lang.Object regionOpen(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No. This is a class implementation and not a message definition.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotManifestV2.class;;;public final class org.apache.hadoop.hbase.snapshot.SnapshotManifestV2 {\n  public static final int DESCRIPTOR_VERSION;\n  public static final java.lang.String SNAPSHOT_MANIFEST_PREFIX;\n}\n;;;No. This class does not contain any message fields or methods and therefore it is not a message definition that can be put on a message queue.;;;N;;;No, it is not a task definition, but a class containing static fields. It does not define any specific task or code that needs to be executed.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil$1.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$1 implements org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor {\n  public void storeFile(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil$2.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$2 implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes, it is a task definition that can be put on a task queue.;;;Y
org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil$3.class;;;final class org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$3 implements org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor {\n  public void storeFile(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil$SnapshotVisitor.class;;;public interface org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor extends org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil$StoreFileVisitor.class;;;public interface org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor {\n  public abstract void storeFile(org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException;\n}\n;;;No, it is an interface for a visitor pattern in the HBase snapshot reference utility. It does not define a message that could be put on a message queue.;;;N;;;No, it is not a task definition. It is an interface definition for a visitor that processes store files in a Hadoop snapshot.;;;N
org/apache/hadoop/hbase/snapshot/SnapshotReferenceUtil.class;;;public final class org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil {\n  public static void visitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor) throws java.io.IOException;\n  public static void visitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$SnapshotVisitor) throws java.io.IOException;\n  public static void visitRegionStoreFiles(org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor) throws java.io.IOException;\n  public static void verifySnapshot(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription) throws java.io.IOException;\n  public static void verifySnapshot(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest) throws java.io.IOException;\n  public static void verifySnapshot(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor) throws java.io.IOException;\n  public static void concurrentVisitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest, java.lang.String, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor) throws java.io.IOException;\n  public static void concurrentVisitReferencedFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.snapshot.SnapshotManifest, java.util.concurrent.ExecutorService, org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil$StoreFileVisitor) throws java.io.IOException;\n  public static void verifyStoreFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotDescription, org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.hbase.shaded.protobuf.generated.SnapshotProtos$SnapshotRegionManifest$StoreFile) throws java.io.IOException;\n  public static java.util.Set<java.lang.String> getHFileNames(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n;;;No, it is a utility class in the HBase library that provides various methods for working with HBase snapshots. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData();\n  public void setFilter(java.lang.String);\n  public java.lang.String getFilter();\n  public boolean getFilter__IsNotDefault();\n  public void setFormat(java.lang.String);\n  public java.lang.String getFormat();\n  public boolean getFormat__IsNotDefault();\n  public void setParent(java.lang.String);\n  public java.lang.String getParent();\n  public boolean getParent__IsNotDefault();\n  public void setTaskMonitor(org.apache.hadoop.hbase.monitoring.TaskMonitor);\n  public org.apache.hadoop.hbase.monitoring.TaskMonitor getTaskMonitor();\n  public boolean getTaskMonitor__IsNotDefault();\n}\n;;;No. It is a class that defines data fields and methods for a template implementation, but it is not a message definition.;;;N;;;No, it is not a task definition. It is a data structure that holds information related to a task monitor template.;;;N
org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl();\n  public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData getImplData();\n  public final org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl setFilter(java.lang.String);\n  public final org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl setFormat(java.lang.String);\n  public final org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl setParent(java.lang.String);\n  public final org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl setTaskMonitor(org.apache.hadoop.hbase.monitoring.TaskMonitor);\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer();\n  public void render(java.io.Writer) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No, it is not a message definition that might be put on a message queue. It appears to be a Java class that extends another class and provides methods for setting certain properties and rendering a template.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/common/TaskMonitorTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No. This is a class definition for a template implementation in the Apache HBase project, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/AssignmentManagerStatusTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/AssignmentManagerStatusTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData();\n  public void setAssignmentManager(org.apache.hadoop.hbase.master.assignment.AssignmentManager);\n  public org.apache.hadoop.hbase.master.assignment.AssignmentManager getAssignmentManager();\n  public void setLimit(int);\n  public int getLimit();\n  public boolean getLimit__IsNotDefault();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/AssignmentManagerStatusTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/AssignmentManagerStatusTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl();\n  public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData getImplData();\n  public final org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl setLimit(int);\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.master.assignment.AssignmentManager);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.master.assignment.AssignmentManager) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.assignment.AssignmentManager) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No, it is a class definition for a template used to generate HTML code for displaying HBase assignment manager status.;;;N;;;No, it is not a task definition. It is a template class for rendering the status of the HBase Assignment Manager.;;;N
org/apache/hadoop/hbase/tmpl/master/AssignmentManagerStatusTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData();\n  public void setMaster(org.apache.hadoop.hbase.master.HMaster);\n  public org.apache.hadoop.hbase.master.HMaster getMaster();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl();\n  public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData getImplData();\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.master.HMaster);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No, it is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/BackupMasterStatusTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.BackupMasterStatusTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData();\n  public void setMaster(org.apache.hadoop.hbase.master.HMaster);\n  public org.apache.hadoop.hbase.master.HMaster getMaster();\n  public void setAssignmentManager(org.apache.hadoop.hbase.master.assignment.AssignmentManager);\n  public org.apache.hadoop.hbase.master.assignment.AssignmentManager getAssignmentManager();\n  public boolean getAssignmentManager__IsNotDefault();\n  public void setCatalogJanitorEnabled(boolean);\n  public boolean getCatalogJanitorEnabled();\n  public boolean getCatalogJanitorEnabled__IsNotDefault();\n  public void setDeadServers(java.util.Set<org.apache.hadoop.hbase.ServerName>);\n  public java.util.Set<org.apache.hadoop.hbase.ServerName> getDeadServers();\n  public boolean getDeadServers__IsNotDefault();\n  public void setFilter(java.lang.String);\n  public java.lang.String getFilter();\n  public boolean getFilter__IsNotDefault();\n  public void setFormat(java.lang.String);\n  public java.lang.String getFormat();\n  public boolean getFormat__IsNotDefault();\n  public void setFrags(java.util.Map<java.lang.String, java.lang.Integer>);\n  public java.util.Map<java.lang.String, java.lang.Integer> getFrags();\n  public boolean getFrags__IsNotDefault();\n  public void setMetaLocation(org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.ServerName getMetaLocation();\n  public boolean getMetaLocation__IsNotDefault();\n  public void setServerManager(org.apache.hadoop.hbase.master.ServerManager);\n  public org.apache.hadoop.hbase.master.ServerManager getServerManager();\n  public boolean getServerManager__IsNotDefault();\n  public void setServers(java.util.List<org.apache.hadoop.hbase.ServerName>);\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getServers();\n  public boolean getServers__IsNotDefault();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No. This is an interface definition for a template in the Hadoop HBase library, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/MasterStatusTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl();\n  public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData getImplData();\n  public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setAssignmentManager(org.apache.hadoop.hbase.master.assignment.AssignmentManager);\n  public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setCatalogJanitorEnabled(boolean);\n  public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setDeadServers(java.util.Set<org.apache.hadoop.hbase.ServerName>);\n  public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setFilter(java.lang.String);\n  public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setFormat(java.lang.String);\n  public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setFrags(java.util.Map<java.lang.String, java.lang.Integer>);\n  public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setMetaLocation(org.apache.hadoop.hbase.ServerName);\n  public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setServerManager(org.apache.hadoop.hbase.master.ServerManager);\n  public final org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl setServers(java.util.List<org.apache.hadoop.hbase.ServerName>);\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.master.HMaster);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No. This class is a template definition for generating HTML or other text-based output for the MasterStatus page of an Apache HBase instance. It is not a message definition that may be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/MasterStatusTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$Intf {\n  public java.lang.String formatZKString();\n  public static java.lang.String getUserTables(org.apache.hadoop.hbase.master.HMaster, java.util.List<org.apache.hadoop.hbase.client.TableDescriptor>);\n  public org.apache.hadoop.hbase.tmpl.master.MasterStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RSGroupListTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RSGroupListTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl$ImplData();\n  public void setMaster(org.apache.hadoop.hbase.master.HMaster);\n  public org.apache.hadoop.hbase.master.HMaster getMaster();\n  public void setServerManager(org.apache.hadoop.hbase.master.ServerManager);\n  public org.apache.hadoop.hbase.master.ServerManager getServerManager();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RSGroupListTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RSGroupListTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl();\n  public org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl$ImplData getImplData();\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.hbase.master.ServerManager);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.hbase.master.ServerManager) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster, org.apache.hadoop.hbase.master.ServerManager) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/tmpl/master/RSGroupListTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.RSGroupListTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.master.RSGroupListTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No. This class is a template implementation class and is not a message definition that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RegionServerListTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RegionServerListTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData();\n  public void setMaster(org.apache.hadoop.hbase.master.HMaster);\n  public org.apache.hadoop.hbase.master.HMaster getMaster();\n  public void setServers(java.util.List<org.apache.hadoop.hbase.ServerName>);\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getServers();\n  public boolean getServers__IsNotDefault();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RegionServerListTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/tmpl/master/RegionServerListTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl();\n  public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData getImplData();\n  public final org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl setServers(java.util.List<org.apache.hadoop.hbase.ServerName>);\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.master.HMaster);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RegionServerListTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.RegionServerListTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.master.RegionServerListTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RegionVisualizerTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RegionVisualizerTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl$ImplData();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RegionVisualizerTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RegionVisualizerTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl();\n  public org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl$ImplData getImplData();\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer();\n  public void render(java.io.Writer) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No, this is not a message definition, it is a class definition for a template in the Hadoop HBase system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/master/RegionVisualizerTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.master.RegionVisualizerTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData();\n  public void setCacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig);\n  public org.apache.hadoop.hbase.io.hfile.CacheConfig getCacheConfig();\n  public void setConfig(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConfig();\n  public void setBc(org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public org.apache.hadoop.hbase.io.hfile.BlockCache getBc();\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class definition for creating objects of type "org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData". It does not appear to have any specific functionality related to task processing.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface extending another interface and defining a method to render a template without flushing.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl();\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData getImplData();\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.BlockCache) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.BlockCache) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No, it is not a message definition. It appears to be a template class related to the HBase region server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheViewTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheViewTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData();\n  public void setCacheConfig(org.apache.hadoop.hbase.io.hfile.CacheConfig);\n  public org.apache.hadoop.hbase.io.hfile.CacheConfig getCacheConfig();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setBcn(java.lang.String);\n  public java.lang.String getBcn();\n  public void setBcv(java.lang.String);\n  public java.lang.String getBcv();\n  public void setBlockCache(org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public org.apache.hadoop.hbase.io.hfile.BlockCache getBlockCache();\n}\n;;;No, it is not a message definition that might be put on a message queue. It appears to be a Java class definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheViewTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheViewTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl();\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData getImplData();\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, org.apache.hadoop.hbase.io.hfile.BlockCache);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, org.apache.hadoop.hbase.io.hfile.BlockCache) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.conf.Configuration, java.lang.String, java.lang.String, org.apache.hadoop.hbase.io.hfile.BlockCache) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It appears to be a template for rendering a block cache view in the HBase region server.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/BlockCacheViewTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData();\n  public void setRegionServer(org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer();\n  public void setBcn(java.lang.String);\n  public java.lang.String getBcn();\n  public boolean getBcn__IsNotDefault();\n  public void setBcv(java.lang.String);\n  public java.lang.String getBcv();\n  public boolean getBcv__IsNotDefault();\n  public void setFilter(java.lang.String);\n  public java.lang.String getFilter();\n  public boolean getFilter__IsNotDefault();\n  public void setFormat(java.lang.String);\n  public java.lang.String getFormat();\n  public boolean getFormat__IsNotDefault();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl();\n  public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData getImplData();\n  public final org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl setBcn(java.lang.String);\n  public final org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl setBcv(java.lang.String);\n  public final org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl setFilter(java.lang.String);\n  public final org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl setFormat(java.lang.String);\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No. This class is a template class used for generating HTML code and is not related to message passing or message queues.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RSStatusTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$Intf {\n  public java.lang.String formatZKString();\n  public org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No. It does not contain any fields or properties, and the methods do not define any message or data being passed between systems. It appears to be a class that generates templates used for rendering status information in a Hadoop environment.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RegionListTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RegionListTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData();\n  public void setRegionServer(org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer();\n  public void setOnlineRegions(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>);\n  public java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getOnlineRegions();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RegionListTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;yes;;;Y;;;;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RegionListTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl();\n  public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData getImplData();\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.regionserver.HRegionServer, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No, this class is not a message definition and is not related to message queue. It appears to be related to HBase RegionServer template for rendering a list of regions.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/RegionListTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ReplicationStatusTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ReplicationStatusTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl$ImplData();\n  public void setRegionServer(org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ReplicationStatusTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ReplicationStatusTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl();\n  public org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl$ImplData getImplData();\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.regionserver.HRegionServer);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.HRegionServer) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ReplicationStatusTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.ReplicationStatusTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ServerMetricsTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ServerMetricsTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData();\n  public void setMWrap(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper);\n  public org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper getMWrap();\n  public void setMServerWrap(org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper);\n  public org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper getMServerWrap();\n  public void setBbAllocator(org.apache.hadoop.hbase.io.ByteBuffAllocator);\n  public org.apache.hadoop.hbase.io.ByteBuffAllocator getBbAllocator();\n}\n;;;No, it is not a message definition. It appears to be a Java class definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ServerMetricsTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ServerMetricsTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl();\n  public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData getImplData();\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper, org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper, org.apache.hadoop.hbase.io.ByteBuffAllocator);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper, org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper, org.apache.hadoop.hbase.io.ByteBuffAllocator) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper, org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper, org.apache.hadoop.hbase.io.ByteBuffAllocator) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No. This class is a template used for generating HTML pages for monitoring Hadoop HBase region servers. It is not a message definition that would typically be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/regionserver/ServerMetricsTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No. This is a class definition for a template used in the HBase region server. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/tool/CanaryStatusTmpl$1.class;;;class org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl$1 extends org.jamon.AbstractRenderer {\n  public void renderTo(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/tool/CanaryStatusTmpl$ImplData.class;;;public class org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl$ImplData extends org.jamon.AbstractTemplateProxy$ImplData {\n  public org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl$ImplData();\n  public void setSink(org.apache.hadoop.hbase.tool.CanaryTool$RegionStdOutSink);\n  public org.apache.hadoop.hbase.tool.CanaryTool$RegionStdOutSink getSink();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/tool/CanaryStatusTmpl$Intf.class;;;public interface org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl$Intf extends org.jamon.AbstractTemplateProxy$Intf {\n  public abstract void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/tool/CanaryStatusTmpl.class;;;public class org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl extends org.jamon.AbstractTemplateProxy {\n  public org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl(org.jamon.TemplateManager);\n  public org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl();\n  public org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl$ImplData getImplData();\n  public org.jamon.AbstractTemplateImpl constructImpl(java.lang.Class<? extends org.jamon.AbstractTemplateImpl>);\n  public org.jamon.Renderer makeRenderer(org.apache.hadoop.hbase.tool.CanaryTool$RegionStdOutSink);\n  public void render(java.io.Writer, org.apache.hadoop.hbase.tool.CanaryTool$RegionStdOutSink) throws java.io.IOException;\n  public void renderNoFlush(java.io.Writer, org.apache.hadoop.hbase.tool.CanaryTool$RegionStdOutSink) throws java.io.IOException;\n  public org.jamon.AbstractTemplateProxy$ImplData getImplData();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/tmpl/tool/CanaryStatusTmplImpl.class;;;public class org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmplImpl extends org.jamon.AbstractTemplateImpl implements org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl$Intf {\n  public org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmplImpl(org.jamon.TemplateManager, org.apache.hadoop.hbase.tmpl.tool.CanaryStatusTmpl$ImplData);\n  public void renderNoFlush(java.io.Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/BulkLoadHFiles$LoadQueueItem.class;;;public class org.apache.hadoop.hbase.tool.BulkLoadHFiles$LoadQueueItem {\n  public org.apache.hadoop.hbase.tool.BulkLoadHFiles$LoadQueueItem(byte[], org.apache.hadoop.fs.Path);\n  public java.lang.String toString();\n  public byte[] getFamily();\n  public org.apache.hadoop.fs.Path getFilePath();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/tool/BulkLoadHFiles.class;;;public interface org.apache.hadoop.hbase.tool.BulkLoadHFiles {\n  public static final java.lang.String RETRY_ON_IO_EXCEPTION;\n  public static final java.lang.String MAX_FILES_PER_REGION_PER_FAMILY;\n  public static final java.lang.String ASSIGN_SEQ_IDS;\n  public static final java.lang.String CREATE_TABLE_CONF_KEY;\n  public static final java.lang.String IGNORE_UNMATCHED_CF_CONF_KEY;\n  public static final java.lang.String ALWAYS_COPY_FILES;\n  public abstract java.util.Map<org.apache.hadoop.hbase.tool.BulkLoadHFiles$LoadQueueItem, java.nio.ByteBuffer> bulkLoad(org.apache.hadoop.hbase.TableName, java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>>) throws org.apache.hadoop.hbase.TableNotFoundException, java.io.IOException;\n  public abstract void disableReplication();\n  public abstract boolean isReplicationDisabled();\n  public abstract java.util.Map<org.apache.hadoop.hbase.tool.BulkLoadHFiles$LoadQueueItem, java.nio.ByteBuffer> bulkLoad(org.apache.hadoop.hbase.TableName, org.apache.hadoop.fs.Path) throws org.apache.hadoop.hbase.TableNotFoundException, java.io.IOException;\n  public static org.apache.hadoop.hbase.tool.BulkLoadHFiles create(org.apache.hadoop.conf.Configuration);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/BulkLoadHFilesTool$1.class;;;final class org.apache.hadoop.hbase.tool.BulkLoadHFilesTool$1 implements org.apache.hadoop.hbase.tool.BulkLoadHFilesTool$BulkHFileVisitor<byte[]> {\n  public byte[] bulkFamily(byte[]);\n  public void bulkHFile(byte[], org.apache.hadoop.fs.FileStatus);\n  public void bulkHFile(java.lang.Object, org.apache.hadoop.fs.FileStatus) throws java.io.IOException;\n  public java.lang.Object bulkFamily(byte[]) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/BulkLoadHFilesTool$2.class;;;class org.apache.hadoop.hbase.tool.BulkLoadHFilesTool$2 implements org.apache.hadoop.hbase.tool.BulkLoadHFilesTool$BulkHFileVisitor<org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder> {\n  public org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder bulkFamily(byte[]);\n  public void bulkHFile(org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder, org.apache.hadoop.fs.FileStatus) throws java.io.IOException;\n  public void bulkHFile(java.lang.Object, org.apache.hadoop.fs.FileStatus) throws java.io.IOException;\n  public java.lang.Object bulkFamily(byte[]) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/BulkLoadHFilesTool$BulkHFileVisitor.class;;;interface org.apache.hadoop.hbase.tool.BulkLoadHFilesTool$BulkHFileVisitor<TFamily> {\n  public abstract TFamily bulkFamily(byte[]) throws java.io.IOException;\n  public abstract void bulkHFile(TFamily, org.apache.hadoop.fs.FileStatus) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition for a task queue.;;;N
org/apache/hadoop/hbase/tool/BulkLoadHFilesTool.class;;;public class org.apache.hadoop.hbase.tool.BulkLoadHFilesTool extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.hbase.tool.BulkLoadHFiles,org.apache.hadoop.util.Tool {\n  public static final java.lang.String NAME;\n  public static final java.lang.String BULK_LOAD_HFILES_BY_FAMILY;\n  public org.apache.hadoop.hbase.tool.BulkLoadHFilesTool(org.apache.hadoop.conf.Configuration);\n  public void initialize();\n  public static void prepareHFileQueue(org.apache.hadoop.hbase.client.AsyncClusterConnection, org.apache.hadoop.hbase.TableName, java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>>, java.util.Deque<org.apache.hadoop.hbase.tool.BulkLoadHFiles$LoadQueueItem>, boolean) throws java.io.IOException;\n  public static void prepareHFileQueue(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.AsyncClusterConnection, org.apache.hadoop.hbase.TableName, org.apache.hadoop.fs.Path, java.util.Deque<org.apache.hadoop.hbase.tool.BulkLoadHFiles$LoadQueueItem>, boolean, boolean) throws java.io.IOException;\n  public void loadHFileQueue(org.apache.hadoop.hbase.client.AsyncClusterConnection, org.apache.hadoop.hbase.TableName, java.util.Deque<org.apache.hadoop.hbase.tool.BulkLoadHFiles$LoadQueueItem>, boolean) throws java.io.IOException;\n  public static byte[][] inferBoundaries(java.util.SortedMap<byte[], java.lang.Integer>);\n  public java.util.Map<org.apache.hadoop.hbase.tool.BulkLoadHFiles$LoadQueueItem, java.nio.ByteBuffer> bulkLoad(org.apache.hadoop.hbase.TableName, java.util.Map<byte[], java.util.List<org.apache.hadoop.fs.Path>>) throws java.io.IOException;\n  public java.util.Map<org.apache.hadoop.hbase.tool.BulkLoadHFiles$LoadQueueItem, java.nio.ByteBuffer> bulkLoad(org.apache.hadoop.hbase.TableName, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void setBulkToken(java.lang.String);\n  public void setClusterIds(java.util.List<java.lang.String>);\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public void disableReplication();\n  public boolean isReplicationDisabled();\n}\n;;;No;;;N;;;No;;;N
org/apache/hadoop/hbase/tool/Canary.class;;;public interface org.apache.hadoop.hbase.tool.Canary {\n  public static org.apache.hadoop.hbase.tool.Canary create(org.apache.hadoop.conf.Configuration, java.util.concurrent.ExecutorService);\n  public static org.apache.hadoop.hbase.tool.Canary create(org.apache.hadoop.conf.Configuration, java.util.concurrent.ExecutorService, org.apache.hadoop.hbase.tool.CanaryTool$Sink);\n  public abstract int checkRegions(java.lang.String[]) throws java.lang.Exception;\n  public abstract int checkRegionServers(java.lang.String[]) throws java.lang.Exception;\n  public abstract int checkZooKeeper() throws java.lang.Exception;\n  public abstract java.util.Map<java.lang.String, java.lang.String> getReadFailures();\n  public abstract java.util.Map<java.lang.String, java.lang.String> getWriteFailures();\n}\n;;;No. This is an interface for a tool in Hadoop, but it is not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/CanaryStatusServlet.class;;;public class org.apache.hadoop.hbase.tool.CanaryStatusServlet extends javax.servlet.http.HttpServlet {\n  public org.apache.hadoop.hbase.tool.CanaryStatusServlet();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/CanaryTool$1.class;;;class org.apache.hadoop.hbase.tool.CanaryTool$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/CanaryTool$Monitor.class;;;public abstract class org.apache.hadoop.hbase.tool.CanaryTool$Monitor implements java.lang.Runnable,java.io.Closeable {\n  public boolean isDone();\n  public boolean hasError();\n  public boolean finalCheckForErrors();\n  public void close() throws java.io.IOException;\n  public abstract void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/tool/CanaryTool$RegionMonitor.class;;;class org.apache.hadoop.hbase.tool.CanaryTool$RegionMonitor extends org.apache.hadoop.hbase.tool.CanaryTool$Monitor {\n  public org.apache.hadoop.hbase.tool.CanaryTool$RegionMonitor(org.apache.hadoop.hbase.client.Connection, java.lang.String[], boolean, org.apache.hadoop.hbase.tool.CanaryTool$Sink, java.util.concurrent.ExecutorService, boolean, org.apache.hadoop.hbase.TableName, boolean, java.util.HashMap<java.lang.String, java.lang.Long>, long, long);\n  public void run();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/tool/CanaryTool$RegionServerMonitor.class;;;class org.apache.hadoop.hbase.tool.CanaryTool$RegionServerMonitor extends org.apache.hadoop.hbase.tool.CanaryTool$Monitor {\n  public org.apache.hadoop.hbase.tool.CanaryTool$RegionServerMonitor(org.apache.hadoop.hbase.client.Connection, java.lang.String[], boolean, org.apache.hadoop.hbase.tool.CanaryTool$Sink, java.util.concurrent.ExecutorService, boolean, boolean, long);\n  public void run();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/tool/CanaryTool$RegionServerStdOutSink.class;;;public class org.apache.hadoop.hbase.tool.CanaryTool$RegionServerStdOutSink extends org.apache.hadoop.hbase.tool.CanaryTool$StdOutSink {\n  public org.apache.hadoop.hbase.tool.CanaryTool$RegionServerStdOutSink();\n  public void publishReadFailure(java.lang.String, java.lang.String);\n  public void publishReadTiming(java.lang.String, java.lang.String, long);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/tool/CanaryTool$RegionServerTask.class;;;class org.apache.hadoop.hbase.tool.CanaryTool$RegionServerTask implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call();\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/tool/CanaryTool$RegionStdOutSink.class;;;public class org.apache.hadoop.hbase.tool.CanaryTool$RegionStdOutSink extends org.apache.hadoop.hbase.tool.CanaryTool$StdOutSink {\n  public org.apache.hadoop.hbase.tool.CanaryTool$RegionStdOutSink();\n  public java.util.concurrent.ConcurrentMap<org.apache.hadoop.hbase.ServerName, java.util.concurrent.atomic.LongAdder> getPerServerFailuresCount();\n  public java.util.concurrent.ConcurrentMap<java.lang.String, java.util.concurrent.atomic.LongAdder> getPerTableFailuresCount();\n  public void resetFailuresCountDetails();\n  public void publishReadFailure(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo, java.lang.Exception);\n  public void publishReadFailure(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, java.lang.Exception);\n  public void publishReadTiming(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, long);\n  public void publishWriteFailure(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo, java.lang.Exception);\n  public void publishWriteFailure(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, java.lang.Exception);\n  public void publishWriteTiming(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor, long);\n  public java.util.Map<java.lang.String, java.util.concurrent.atomic.LongAdder> getReadLatencyMap();\n  public java.util.concurrent.atomic.LongAdder initializeAndGetReadLatencyForTable(java.lang.String);\n  public void initializeWriteLatency();\n  public java.util.concurrent.atomic.LongAdder getWriteLatency();\n  public java.util.concurrent.ConcurrentMap<java.lang.String, java.util.List<org.apache.hadoop.hbase.tool.CanaryTool$RegionTaskResult>> getRegionMap();\n  public int getTotalExpectedRegions();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/CanaryTool$RegionTask$TaskType.class;;;public final class org.apache.hadoop.hbase.tool.CanaryTool$RegionTask$TaskType extends java.lang.Enum<org.apache.hadoop.hbase.tool.CanaryTool$RegionTask$TaskType> {\n  public static final org.apache.hadoop.hbase.tool.CanaryTool$RegionTask$TaskType READ;\n  public static final org.apache.hadoop.hbase.tool.CanaryTool$RegionTask$TaskType WRITE;\n  public static org.apache.hadoop.hbase.tool.CanaryTool$RegionTask$TaskType[] values();\n  public static org.apache.hadoop.hbase.tool.CanaryTool$RegionTask$TaskType valueOf(java.lang.String);\n}\n;;;No. It is an enumeration class used internally by the CanaryTool class in the Hadoop ecosystem and does not define a message structure.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/tool/CanaryTool$RegionTask.class;;;class org.apache.hadoop.hbase.tool.CanaryTool$RegionTask implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call();\n  public java.lang.Void read();\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/tool/CanaryTool$RegionTaskResult.class;;;public class org.apache.hadoop.hbase.tool.CanaryTool$RegionTaskResult {\n  public org.apache.hadoop.hbase.tool.CanaryTool$RegionTaskResult(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.ColumnFamilyDescriptor);\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public java.lang.String getRegionNameAsString();\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public java.lang.String getTableNameAsString();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public java.lang.String getServerNameAsString();\n  public org.apache.hadoop.hbase.client.ColumnFamilyDescriptor getColumnFamily();\n  public java.lang.String getColumnFamilyNameAsString();\n  public long getReadLatency();\n  public void setReadLatency(long);\n  public long getWriteLatency();\n  public void setWriteLatency(long);\n  public boolean isReadSuccess();\n  public void setReadSuccess();\n  public boolean isWriteSuccess();\n  public void setWriteSuccess();\n}\n;;;No, it is a class that is meant to be instantiated and used within the context of the CanaryTool tool, rather than being put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/CanaryTool$Sink.class;;;public interface org.apache.hadoop.hbase.tool.CanaryTool$Sink {\n  public abstract long getReadFailureCount();\n  public abstract long incReadFailureCount();\n  public abstract java.util.Map<java.lang.String, java.lang.String> getReadFailures();\n  public abstract void updateReadFailures(java.lang.String, java.lang.String);\n  public abstract long getWriteFailureCount();\n  public abstract long incWriteFailureCount();\n  public abstract java.util.Map<java.lang.String, java.lang.String> getWriteFailures();\n  public abstract void updateWriteFailures(java.lang.String, java.lang.String);\n  public abstract long getReadSuccessCount();\n  public abstract long incReadSuccessCount();\n  public abstract long getWriteSuccessCount();\n  public abstract long incWriteSuccessCount();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/CanaryTool$StdOutSink.class;;;public class org.apache.hadoop.hbase.tool.CanaryTool$StdOutSink implements org.apache.hadoop.hbase.tool.CanaryTool$Sink {\n  public org.apache.hadoop.hbase.tool.CanaryTool$StdOutSink();\n  public long getReadFailureCount();\n  public long incReadFailureCount();\n  public java.util.Map<java.lang.String, java.lang.String> getReadFailures();\n  public void updateReadFailures(java.lang.String, java.lang.String);\n  public long getWriteFailureCount();\n  public long incWriteFailureCount();\n  public java.util.Map<java.lang.String, java.lang.String> getWriteFailures();\n  public void updateWriteFailures(java.lang.String, java.lang.String);\n  public long getReadSuccessCount();\n  public long incReadSuccessCount();\n  public long getWriteSuccessCount();\n  public long incWriteSuccessCount();\n}\n;;;No, it is not a message definition. It is a class definition that defines methods and properties for a specific tool in HBase. It is not meant to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/CanaryTool$ZookeeperMonitor.class;;;class org.apache.hadoop.hbase.tool.CanaryTool$ZookeeperMonitor extends org.apache.hadoop.hbase.tool.CanaryTool$Monitor {\n  public void run();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/tool/CanaryTool$ZookeeperStdOutSink.class;;;public class org.apache.hadoop.hbase.tool.CanaryTool$ZookeeperStdOutSink extends org.apache.hadoop.hbase.tool.CanaryTool$StdOutSink {\n  public org.apache.hadoop.hbase.tool.CanaryTool$ZookeeperStdOutSink();\n  public void publishReadFailure(java.lang.String, java.lang.String);\n  public void publishReadTiming(java.lang.String, java.lang.String, long);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/CanaryTool$ZookeeperTask.class;;;class org.apache.hadoop.hbase.tool.CanaryTool$ZookeeperTask implements java.util.concurrent.Callable<java.lang.Void> {\n  public org.apache.hadoop.hbase.tool.CanaryTool$ZookeeperTask(org.apache.hadoop.hbase.client.Connection, java.lang.String, java.lang.String, int, org.apache.hadoop.hbase.tool.CanaryTool$ZookeeperStdOutSink);\n  public java.lang.Void call() throws java.lang.Exception;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/tool/CanaryTool.class;;;public class org.apache.hadoop.hbase.tool.CanaryTool implements org.apache.hadoop.util.Tool,org.apache.hadoop.hbase.tool.Canary {\n  public static final java.lang.String HBASE_CANARY_INFO_PORT;\n  public static final java.lang.String HBASE_CANARY_INFO_BINDADDRESS;\n  public static final org.apache.hadoop.hbase.TableName DEFAULT_WRITE_TABLE_NAME;\n  public static final java.lang.String HBASE_CANARY_REGIONSERVER_ALL_REGIONS;\n  public static final java.lang.String HBASE_CANARY_REGION_WRITE_SNIFFING;\n  public static final java.lang.String HBASE_CANARY_REGION_WRITE_TABLE_TIMEOUT;\n  public static final java.lang.String HBASE_CANARY_REGION_WRITE_TABLE_NAME;\n  public static final java.lang.String HBASE_CANARY_REGION_READ_TABLE_TIMEOUT;\n  public static final java.lang.String HBASE_CANARY_ZOOKEEPER_PERMITTED_FAILURES;\n  public static final java.lang.String HBASE_CANARY_USE_REGEX;\n  public static final java.lang.String HBASE_CANARY_TIMEOUT;\n  public static final java.lang.String HBASE_CANARY_FAIL_ON_ERROR;\n  public int checkRegions(java.lang.String[]) throws java.lang.Exception;\n  public int checkRegionServers(java.lang.String[]) throws java.lang.Exception;\n  public int checkZooKeeper() throws java.lang.Exception;\n  public org.apache.hadoop.hbase.tool.CanaryTool();\n  public org.apache.hadoop.hbase.tool.CanaryTool(java.util.concurrent.ExecutorService);\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public java.util.Map<java.lang.String, java.lang.String> getReadFailures();\n  public java.util.Map<java.lang.String, java.lang.String> getWriteFailures();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/DataBlockEncodingValidator.class;;;public class org.apache.hadoop.hbase.tool.DataBlockEncodingValidator extends org.apache.hadoop.hbase.util.AbstractHBaseTool {\n  public org.apache.hadoop.hbase.tool.DataBlockEncodingValidator();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/HFileContentValidator.class;;;public class org.apache.hadoop.hbase.tool.HFileContentValidator extends org.apache.hadoop.hbase.util.AbstractHBaseTool {\n  public org.apache.hadoop.hbase.tool.HFileContentValidator();\n}\n;;;No. This class is a tool used for validating the content of HFiles in HBase, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/PreUpgradeValidator.class;;;public class org.apache.hadoop.hbase.tool.PreUpgradeValidator implements org.apache.hadoop.util.Tool {\n  public static final java.lang.String TOOL_NAME;\n  public static final java.lang.String VALIDATE_CP_NAME;\n  public static final java.lang.String VALIDATE_DBE_NAME;\n  public static final java.lang.String VALIDATE_HFILE;\n  public org.apache.hadoop.hbase.tool.PreUpgradeValidator();\n  public org.apache.hadoop.conf.Configuration getConf();\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/WriteSinkCoprocessor.class;;;public class org.apache.hadoop.hbase.tool.WriteSinkCoprocessor implements org.apache.hadoop.hbase.coprocessor.RegionCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionObserver {\n  public org.apache.hadoop.hbase.tool.WriteSinkCoprocessor();\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionObserver> getRegionObserver();\n  public void preOpen(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException;\n  public void preBatchMutate(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress<org.apache.hadoop.hbase.client.Mutation>) throws java.io.IOException;\n}\n;;;No. This class is an implementation of the RegionCoprocessor and RegionObserver interfaces in HBase, and is not specifically designed as a message definition for a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/coprocessor/Branch1CoprocessorMethods.class;;;public class org.apache.hadoop.hbase.tool.coprocessor.Branch1CoprocessorMethods extends org.apache.hadoop.hbase.tool.coprocessor.CoprocessorMethods {\n  public org.apache.hadoop.hbase.tool.coprocessor.Branch1CoprocessorMethods();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethod.class;;;public class org.apache.hadoop.hbase.tool.coprocessor.CoprocessorMethod {\n  public org.apache.hadoop.hbase.tool.coprocessor.CoprocessorMethod(java.lang.String);\n  public org.apache.hadoop.hbase.tool.coprocessor.CoprocessorMethod withParameters(java.lang.String...);\n  public org.apache.hadoop.hbase.tool.coprocessor.CoprocessorMethod withParameters(java.lang.Class<?>...);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/coprocessor/CoprocessorMethods.class;;;public class org.apache.hadoop.hbase.tool.coprocessor.CoprocessorMethods {\n  public org.apache.hadoop.hbase.tool.coprocessor.CoprocessorMethods();\n  public void addMethod(java.lang.String, java.lang.String...);\n  public void addMethod(java.lang.String, java.lang.Class<?>...);\n  public void addMethod(java.lang.reflect.Method);\n  public boolean hasMethod(java.lang.String, java.lang.String...);\n  public boolean hasMethod(java.lang.String, java.lang.Class<?>...);\n  public boolean hasMethod(java.lang.reflect.Method);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator$1.class;;;class org.apache.hadoop.hbase.tool.coprocessor.CoprocessorValidator$1 implements java.security.PrivilegedAction<org.apache.hadoop.hbase.tool.coprocessor.CoprocessorValidator$ResolverUrlClassLoader> {\n  public org.apache.hadoop.hbase.tool.coprocessor.CoprocessorValidator$ResolverUrlClassLoader run();\n  public java.lang.Object run();\n}\n;;;No.;;;N;;;No, this is not a task definition that might be put on a task queue. It is a class definition that implements the `PrivilegedAction` interface.;;;N
org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator$2.class;;;class org.apache.hadoop.hbase.tool.coprocessor.CoprocessorValidator$2 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator$ResolverUrlClassLoader.class;;;final class org.apache.hadoop.hbase.tool.coprocessor.CoprocessorValidator$ResolverUrlClassLoader extends java.net.URLClassLoader {\n  public java.lang.Class<?> loadClass(java.lang.String) throws java.lang.ClassNotFoundException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/coprocessor/CoprocessorValidator.class;;;public class org.apache.hadoop.hbase.tool.coprocessor.CoprocessorValidator extends org.apache.hadoop.hbase.util.AbstractHBaseTool {\n  public org.apache.hadoop.hbase.tool.coprocessor.CoprocessorValidator();\n  public void validateClasses(java.lang.ClassLoader, java.util.List<java.lang.String>, java.util.List<org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation>);\n  public void validateClasses(java.lang.ClassLoader, java.lang.String[], java.util.List<org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation>);\n}\n;;;No, this class is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/coprocessor/CoprocessorViolation$Severity.class;;;public final class org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation$Severity extends java.lang.Enum<org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation$Severity> {\n  public static final org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation$Severity WARNING;\n  public static final org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation$Severity ERROR;\n  public static org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation$Severity[] values();\n  public static org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation$Severity valueOf(java.lang.String);\n}\n;;;No. This is a Java class definition for an enum type and not a message definition for a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/coprocessor/CoprocessorViolation.class;;;public class org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation {\n  public org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation(java.lang.String, org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation$Severity, java.lang.String);\n  public org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation(java.lang.String, org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation$Severity, java.lang.String, java.lang.Throwable);\n  public java.lang.String getClassName();\n  public org.apache.hadoop.hbase.tool.coprocessor.CoprocessorViolation$Severity getSeverity();\n  public java.lang.String getMessage();\n  public java.lang.Throwable getThrowable();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/tool/coprocessor/CurrentCoprocessorMethods.class;;;public class org.apache.hadoop.hbase.tool.coprocessor.CurrentCoprocessorMethods extends org.apache.hadoop.hbase.tool.coprocessor.CoprocessorMethods {\n  public org.apache.hadoop.hbase.tool.coprocessor.CurrentCoprocessorMethods();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/AbstractFileStatusFilter.class;;;public abstract class org.apache.hadoop.hbase.util.AbstractFileStatusFilter implements org.apache.hadoop.fs.PathFilter,org.apache.hadoop.hbase.util.FileStatusFilter {\n  public org.apache.hadoop.hbase.util.AbstractFileStatusFilter();\n  public boolean accept(org.apache.hadoop.fs.FileStatus);\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/BloomContext.class;;;public abstract class org.apache.hadoop.hbase.util.BloomContext {\n  public org.apache.hadoop.hbase.util.BloomContext(org.apache.hadoop.hbase.util.BloomFilterWriter, org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.Cell getLastCell();\n  public void writeBloom(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public abstract void addLastBloomKey(org.apache.hadoop.hbase.io.hfile.HFile$Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/BloomFilter.class;;;public interface org.apache.hadoop.hbase.util.BloomFilter extends org.apache.hadoop.hbase.util.BloomFilterBase {\n  public abstract boolean contains(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.nio.ByteBuff, org.apache.hadoop.hbase.regionserver.BloomType);\n  public abstract boolean contains(byte[], int, int, org.apache.hadoop.hbase.nio.ByteBuff);\n  public abstract boolean supportsAutoLoading();\n}\n;;;No. This is not a message definition. It is an interface for a bloom filter with abstract methods.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/util/BloomFilterBase.class;;;public interface org.apache.hadoop.hbase.util.BloomFilterBase {\n  public abstract long getKeyCount();\n  public abstract long getMaxKeys();\n  public abstract long getByteSize();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/BloomFilterChunk.class;;;public class org.apache.hadoop.hbase.util.BloomFilterChunk implements org.apache.hadoop.hbase.util.BloomFilterBase {\n  public org.apache.hadoop.hbase.util.BloomFilterChunk(java.io.DataInput) throws java.io.IOException, java.lang.IllegalArgumentException;\n  public double actualErrorRate();\n  public org.apache.hadoop.hbase.util.BloomFilterChunk(int, org.apache.hadoop.hbase.regionserver.BloomType);\n  public org.apache.hadoop.hbase.util.BloomFilterChunk(int, double, int, int) throws java.lang.IllegalArgumentException;\n  public org.apache.hadoop.hbase.util.BloomFilterChunk createAnother();\n  public void allocBloom();\n  public void add(org.apache.hadoop.hbase.Cell);\n  public long getKeyCount();\n  public long getMaxKeys();\n  public long getByteSize();\n  public int getHashType();\n  public void compactBloom();\n  public void writeBloom(java.io.DataOutput) throws java.io.IOException;\n  public int getHashCount();\n  public java.lang.String toString();\n}\n;;;No. This is a class definition for the BloomFilterChunk class, but it is not a message definition that would be put on a message queue.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/util/BloomFilterFactory.class;;;public final class org.apache.hadoop.hbase.util.BloomFilterFactory {\n  public static final java.lang.String IO_STOREFILE_BLOOM_ERROR_RATE;\n  public static final java.lang.String IO_STOREFILE_BLOOM_MAX_FOLD;\n  public static final java.lang.String IO_STOREFILE_BLOOM_MAX_KEYS;\n  public static final java.lang.String IO_STOREFILE_BLOOM_ENABLED;\n  public static final java.lang.String IO_STOREFILE_DELETEFAMILY_BLOOM_ENABLED;\n  public static final java.lang.String IO_STOREFILE_BLOOM_BLOCK_SIZE;\n  public static org.apache.hadoop.hbase.util.BloomFilter createFromMeta(java.io.DataInput, org.apache.hadoop.hbase.io.hfile.HFile$Reader) throws java.lang.IllegalArgumentException, java.io.IOException;\n  public static boolean isGeneralBloomEnabled(org.apache.hadoop.conf.Configuration);\n  public static boolean isDeleteFamilyBloomEnabled(org.apache.hadoop.conf.Configuration);\n  public static float getErrorRate(org.apache.hadoop.conf.Configuration);\n  public static int getMaxFold(org.apache.hadoop.conf.Configuration);\n  public static int getBloomBlockSize(org.apache.hadoop.conf.Configuration);\n  public static int getMaxKeys(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.hbase.util.BloomFilterWriter createGeneralBloomAtWrite(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, org.apache.hadoop.hbase.regionserver.BloomType, int, org.apache.hadoop.hbase.io.hfile.HFile$Writer);\n  public static org.apache.hadoop.hbase.util.BloomFilterWriter createDeleteBloomAtWrite(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.io.hfile.CacheConfig, int, org.apache.hadoop.hbase.io.hfile.HFile$Writer);\n}\n;;;No, it is a utility class with static methods that provide functionality related to Bloom filters in HBase. It is not a message definition for a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/BloomFilterUtil.class;;;public final class org.apache.hadoop.hbase.util.BloomFilterUtil {\n  public static final java.lang.String STATS_RECORD_SEP;\n  public static final double LOG2_SQUARED;\n  public static final java.lang.String PREFIX_LENGTH_KEY;\n  public static final byte[] bitvals;\n  public static long computeBitSize(long, double);\n  public static void setRandomGeneratorForTest(java.util.Random);\n  public static long idealMaxKeys(long, double);\n  public static long computeMaxKeys(long, double, int);\n  public static double actualErrorRate(long, long, int);\n  public static int computeFoldableByteSize(long, int);\n  public static int optimalFunctionCount(int, long);\n  public static org.apache.hadoop.hbase.util.BloomFilterChunk createBySize(int, double, int, int, org.apache.hadoop.hbase.regionserver.BloomType);\n  public static boolean contains(byte[], int, int, org.apache.hadoop.hbase.nio.ByteBuff, int, int, org.apache.hadoop.hbase.util.Hash, int);\n  public static boolean contains(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.nio.ByteBuff, int, int, org.apache.hadoop.hbase.util.Hash, int, org.apache.hadoop.hbase.regionserver.BloomType);\n  public static java.lang.String formatStats(org.apache.hadoop.hbase.util.BloomFilterBase);\n  public static java.lang.String toString(org.apache.hadoop.hbase.util.BloomFilterChunk);\n  public static byte[] getBloomFilterParam(org.apache.hadoop.hbase.regionserver.BloomType, org.apache.hadoop.conf.Configuration) throws java.lang.IllegalArgumentException;\n}\n;;;No, the class contains utility methods for working with bloom filters, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/BloomFilterWriter.class;;;public interface org.apache.hadoop.hbase.util.BloomFilterWriter extends org.apache.hadoop.hbase.util.BloomFilterBase,org.apache.hadoop.hbase.regionserver.CellSink,org.apache.hadoop.hbase.regionserver.ShipperListener {\n  public abstract void compactBloom();\n  public abstract org.apache.hadoop.io.Writable getMetaWriter();\n  public abstract org.apache.hadoop.io.Writable getDataWriter();\n  public abstract org.apache.hadoop.hbase.Cell getPrevCell();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue$PriorityQueue.class;;;class org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue<E> {\n  public org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue$PriorityQueue(int, java.util.Comparator<? super E>);\n  public void add(E);\n  public E peek();\n  public E poll();\n  public int size();\n  public java.util.Comparator<? super E> comparator();\n  public boolean contains(java.lang.Object);\n  public int remainingCapacity();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/BoundedPriorityBlockingQueue.class;;;public class org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue<E> extends java.util.AbstractQueue<E> implements java.util.concurrent.BlockingQueue<E> {\n  public org.apache.hadoop.hbase.util.BoundedPriorityBlockingQueue(int, java.util.Comparator<? super E>);\n  public boolean offer(E);\n  public void put(E) throws java.lang.InterruptedException;\n  public boolean offer(E, long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public E take() throws java.lang.InterruptedException;\n  public E poll();\n  public E poll(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public E peek();\n  public int size();\n  public java.util.Iterator<E> iterator();\n  public java.util.Comparator<? super E> comparator();\n  public int remainingCapacity();\n  public boolean remove(java.lang.Object);\n  public boolean contains(java.lang.Object);\n  public int drainTo(java.util.Collection<? super E>);\n  public int drainTo(java.util.Collection<? super E>, int);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/CollectionBackedScanner.class;;;public class org.apache.hadoop.hbase.util.CollectionBackedScanner extends org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner {\n  public org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.SortedSet<org.apache.hadoop.hbase.Cell>);\n  public org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.SortedSet<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.List<org.apache.hadoop.hbase.Cell>);\n  public org.apache.hadoop.hbase.util.CollectionBackedScanner(java.util.List<org.apache.hadoop.hbase.Cell>, org.apache.hadoop.hbase.CellComparator);\n  public org.apache.hadoop.hbase.util.CollectionBackedScanner(org.apache.hadoop.hbase.CellComparator, org.apache.hadoop.hbase.Cell...);\n  public org.apache.hadoop.hbase.Cell peek();\n  public org.apache.hadoop.hbase.Cell next();\n  public boolean seek(org.apache.hadoop.hbase.Cell);\n  public boolean reseek(org.apache.hadoop.hbase.Cell);\n  public void close();\n}\n;;;No, this class is not a message definition. It is a class definition for a scanner in Hadoop for reading and iterating over data stored in HBase tables.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/CompressionTest.class;;;public class org.apache.hadoop.hbase.util.CompressionTest {\n  public org.apache.hadoop.hbase.util.CompressionTest();\n  public static boolean testCompression(java.lang.String);\n  public static void testCompression(org.apache.hadoop.hbase.io.compress.Compression$Algorithm) throws java.io.IOException;\n  public static void usage();\n  public static void doSmokeTest(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ConfigurationUtil.class;;;public final class org.apache.hadoop.hbase.util.ConfigurationUtil {\n  public static final char KVP_DELIMITER;\n  public static void setKeyValues(org.apache.hadoop.conf.Configuration, java.lang.String, java.util.Collection<java.util.Map$Entry<java.lang.String, java.lang.String>>);\n  public static void setKeyValues(org.apache.hadoop.conf.Configuration, java.lang.String, java.util.Collection<java.util.Map$Entry<java.lang.String, java.lang.String>>, char);\n  public static java.util.List<java.util.Map$Entry<java.lang.String, java.lang.String>> getKeyValues(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public static java.util.List<java.util.Map$Entry<java.lang.String, java.lang.String>> getKeyValues(org.apache.hadoop.conf.Configuration, java.lang.String, char);\n}\n;;;No. This class provides utility methods for manipulating configurations in Apache HBase, but it does not define a message or data format that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ConnectionCache$1.class;;;class org.apache.hadoop.hbase.util.ConnectionCache$1 extends java.lang.ThreadLocal<java.lang.String> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ConnectionCache$2.class;;;class org.apache.hadoop.hbase.util.ConnectionCache$2 implements org.apache.hadoop.hbase.Stoppable {\n  public void stop(java.lang.String);\n  public boolean isStopped();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/ConnectionCache$3.class;;;class org.apache.hadoop.hbase.util.ConnectionCache$3 extends org.apache.hadoop.hbase.ScheduledChore {\n}\n;;;no;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ConnectionCache$ConnectionInfo.class;;;class org.apache.hadoop.hbase.util.ConnectionCache$ConnectionInfo {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/ConnectionCache.class;;;public class org.apache.hadoop.hbase.util.ConnectionCache {\n  public org.apache.hadoop.hbase.util.ConnectionCache(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.security.UserProvider, int, int) throws java.io.IOException;\n  public void setEffectiveUser(java.lang.String);\n  public java.lang.String getEffectiveUser();\n  public void shutdown();\n  public org.apache.hadoop.hbase.client.Admin getAdmin() throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Table getTable(java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.RegionLocator getRegionLocator(byte[]) throws java.io.IOException;\n  public boolean updateConnectionAccessTime();\n  public java.lang.String getClusterId();\n}\n;;;No. This class provides utility methods and manages connections, but it does not define any message format or content that could be put on a message queue.;;;N;;;No. This is not a task definition but rather a utility class for managing connections to HBase.;;;N
org/apache/hadoop/hbase/util/CoprocessorConfigurationUtil.class;;;public final class org.apache.hadoop.hbase.util.CoprocessorConfigurationUtil {\n  public static boolean checkConfigurationChange(org.apache.hadoop.conf.Configuration, org.apache.hadoop.conf.Configuration, java.lang.String...);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/DirectMemoryUtils.class;;;public class org.apache.hadoop.hbase.util.DirectMemoryUtils {\n  public org.apache.hadoop.hbase.util.DirectMemoryUtils();\n  public static long getDirectMemorySize();\n  public static long getDirectMemoryUsage();\n  public static long getNettyDirectMemoryUsage();\n  public static void destroyDirectByteBuffer(java.nio.ByteBuffer) throws java.lang.IllegalArgumentException, java.lang.IllegalAccessException, java.lang.reflect.InvocationTargetException, java.lang.SecurityException, java.lang.NoSuchMethodException;\n}\n;;;No. This is a class that provides utility methods for managing direct memory usage in Apache HBase. It is not a message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/EncryptionTest.class;;;public class org.apache.hadoop.hbase.util.EncryptionTest {\n  public static void testKeyProvider(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void testCipherProvider(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void testEncryption(org.apache.hadoop.conf.Configuration, java.lang.String, byte[]) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class containing static methods for testing encryption-related functionalities in the Apache Hadoop HBase utility library.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/util/FSRegionScanner.class;;;class org.apache.hadoop.hbase.util.FSRegionScanner implements java.lang.Runnable {\n  public void run();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/FSTableDescriptors$1.class;;;class org.apache.hadoop.hbase.util.FSTableDescriptors$1 implements java.lang.Runnable {\n  public void run();\n}\n;;;No. This is a class definition for a runnable interface, but it is not a message definition.;;;N;;;No, it is not a task definition. It is a class implementation of the `Runnable` interface that defines a `run` method.;;;N
org/apache/hadoop/hbase/util/FSTableDescriptors$2.class;;;final class org.apache.hadoop.hbase.util.FSTableDescriptors$2 implements java.util.Comparator<org.apache.hadoop.fs.FileStatus> {\n  public int compare(org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileStatus);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No;;;N;;;No. It is a class that implements a Java interface and provides a method for comparing two FileStatus objects. It is not a task that can be put on a task queue.;;;N
org/apache/hadoop/hbase/util/FSTableDescriptors$3.class;;;final class org.apache.hadoop.hbase.util.FSTableDescriptors$3 implements org.apache.hadoop.fs.PathFilter {\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSTableDescriptors$SequenceIdAndFileLength.class;;;final class org.apache.hadoop.hbase.util.FSTableDescriptors$SequenceIdAndFileLength {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSTableDescriptors.class;;;public class org.apache.hadoop.hbase.util.FSTableDescriptors implements org.apache.hadoop.hbase.TableDescriptors {\n  public static final java.lang.String TABLEINFO_DIR;\n  public org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean);\n  public org.apache.hadoop.hbase.util.FSTableDescriptors(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, boolean, int);\n  public static void tryUpdateMetaTableDescriptor(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptor tryUpdateAndGetMetaTableDescriptor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.ColumnFamilyDescriptor getTableFamilyDescForMeta(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.hbase.client.ColumnFamilyDescriptor getReplBarrierFamilyDescForMeta();\n  public org.apache.hadoop.hbase.client.TableDescriptor get(org.apache.hadoop.hbase.TableName);\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.client.TableDescriptor> getAll() throws java.io.IOException;\n  public java.util.Map<java.lang.String, org.apache.hadoop.hbase.client.TableDescriptor> getByNamespace(java.lang.String) throws java.io.IOException;\n  public void update(org.apache.hadoop.hbase.client.TableDescriptor, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableDescriptor remove(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static boolean isTableDir(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptor getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptor getTableDescriptorFromFs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void deleteTableDescriptors(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public boolean createTableDescriptor(org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public boolean createTableDescriptor(org.apache.hadoop.hbase.client.TableDescriptor, boolean) throws java.io.IOException;\n  public boolean createTableDescriptorForTableDirectory(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.TableDescriptor, boolean) throws java.io.IOException;\n  public static boolean createTableDescriptorForTableDirectory(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.TableDescriptor, boolean) throws java.io.IOException;\n}\n;;;No. This class provides utility methods to interact with HBase table descriptors, but it is not a message definition to be put on a message queue.;;;N;;;No, it is not a task definition, rather it is a class definition for managing table descriptors in HBase.;;;N
org/apache/hadoop/hbase/util/FSUtils$1.class;;;final class org.apache.hadoop.hbase.util.FSUtils$1 implements org.apache.hadoop.hbase.util.FSUtils$ProgressReporter {\n  public void progress(org.apache.hadoop.fs.FileStatus);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/util/FSUtils$2.class;;;final class org.apache.hadoop.hbase.util.FSUtils$2 implements java.lang.Runnable {\n  public void run();\n}\n;;;No.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/util/FSUtils$3.class;;;final class org.apache.hadoop.hbase.util.FSUtils$3 implements org.apache.hadoop.hbase.util.FSUtils$ProgressReporter {\n  public void progress(org.apache.hadoop.fs.FileStatus);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/FSUtils$4.class;;;final class org.apache.hadoop.hbase.util.FSUtils$4 implements org.apache.hadoop.fs.PathFilter {\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils$BlackListDirFilter.class;;;public class org.apache.hadoop.hbase.util.FSUtils$BlackListDirFilter extends org.apache.hadoop.hbase.util.AbstractFileStatusFilter {\n  public org.apache.hadoop.hbase.util.FSUtils$BlackListDirFilter(org.apache.hadoop.fs.FileSystem, java.util.List<java.lang.String>);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils$DirFilter.class;;;public class org.apache.hadoop.hbase.util.FSUtils$DirFilter extends org.apache.hadoop.hbase.util.FSUtils$BlackListDirFilter {\n  public org.apache.hadoop.hbase.util.FSUtils$DirFilter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils$FamilyDirFilter.class;;;public class org.apache.hadoop.hbase.util.FSUtils$FamilyDirFilter extends org.apache.hadoop.hbase.util.AbstractFileStatusFilter {\n  public org.apache.hadoop.hbase.util.FSUtils$FamilyDirFilter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils$FileFilter.class;;;class org.apache.hadoop.hbase.util.FSUtils$FileFilter extends org.apache.hadoop.hbase.util.AbstractFileStatusFilter {\n  public org.apache.hadoop.hbase.util.FSUtils$FileFilter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No.;;;N;;;No. This is a class definition and not a task definition. It cannot be put on a task queue directly.;;;N
org/apache/hadoop/hbase/util/FSUtils$HFileFilter.class;;;public class org.apache.hadoop.hbase.util.FSUtils$HFileFilter extends org.apache.hadoop.hbase.util.AbstractFileStatusFilter {\n  public org.apache.hadoop.hbase.util.FSUtils$HFileFilter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils$HFileLinkFilter.class;;;public class org.apache.hadoop.hbase.util.FSUtils$HFileLinkFilter implements org.apache.hadoop.fs.PathFilter {\n  public org.apache.hadoop.hbase.util.FSUtils$HFileLinkFilter();\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils$ProgressReporter.class;;;interface org.apache.hadoop.hbase.util.FSUtils$ProgressReporter {\n  public abstract void progress(org.apache.hadoop.fs.FileStatus);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils$ReferenceAndLinkFileFilter.class;;;public class org.apache.hadoop.hbase.util.FSUtils$ReferenceAndLinkFileFilter implements org.apache.hadoop.fs.PathFilter {\n  public org.apache.hadoop.hbase.util.FSUtils$ReferenceAndLinkFileFilter(org.apache.hadoop.fs.FileSystem);\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/FSUtils$ReferenceFileFilter.class;;;public class org.apache.hadoop.hbase.util.FSUtils$ReferenceFileFilter extends org.apache.hadoop.hbase.util.AbstractFileStatusFilter {\n  public org.apache.hadoop.hbase.util.FSUtils$ReferenceFileFilter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils$RegionDirFilter.class;;;public class org.apache.hadoop.hbase.util.FSUtils$RegionDirFilter extends org.apache.hadoop.hbase.util.AbstractFileStatusFilter {\n  public static final java.util.regex.Pattern regionDirPattern;\n  public org.apache.hadoop.hbase.util.FSUtils$RegionDirFilter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils$UserTableDirFilter.class;;;public class org.apache.hadoop.hbase.util.FSUtils$UserTableDirFilter extends org.apache.hadoop.hbase.util.FSUtils$BlackListDirFilter {\n  public org.apache.hadoop.hbase.util.FSUtils$UserTableDirFilter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No. This is a class definition and not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSUtils.class;;;public final class org.apache.hadoop.hbase.util.FSUtils {\n  public static final boolean WINDOWS;\n  public static boolean isDistributedFileSystem(org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static boolean isMatchingTail(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public static boolean deleteRegionDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FSDataOutputStream create(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.permission.FsPermission, java.net.InetSocketAddress[]) throws java.io.IOException;\n  public static void checkFileSystemAvailable(org.apache.hadoop.fs.FileSystem) throws java.io.IOException;\n  public static void checkDfsSafeMode(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.lang.String getVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException, org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public static void checkVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException, org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public static void checkVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, int, int) throws java.io.IOException, org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public static void setVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void setVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, int, int) throws java.io.IOException;\n  public static void setVersion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String, int, int) throws java.io.IOException;\n  public static boolean checkClusterIdExists(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.ClusterId getClusterId(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void setClusterId(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.ClusterId, long) throws java.io.IOException;\n  public static void waitOnSafeMode(org.apache.hadoop.conf.Configuration, long) throws java.io.IOException;\n  public static boolean metaRegionExists(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.hdfs.client.HdfsDataInputStream) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.HDFSBlocksDistribution computeHDFSBlocksDistribution(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileStatus, long, long) throws java.io.IOException;\n  public static void addToHDFSBlocksDistribution(org.apache.hadoop.hbase.HDFSBlocksDistribution, org.apache.hadoop.fs.BlockLocation[]) throws java.io.IOException;\n  public static int getTotalTableFragmentation(org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException;\n  public static java.util.Map<java.lang.String, java.lang.Integer> getTableFragmentation(org.apache.hadoop.hbase.master.HMaster) throws java.io.IOException;\n  public static java.util.Map<java.lang.String, java.lang.Integer> getTableFragmentation(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void renameFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.fs.Path> getTableDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.fs.Path> getLocalTableDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.fs.Path> getRegionDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getRegionDirFromRootDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo);\n  public static org.apache.hadoop.fs.Path getRegionDirFromTableDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo);\n  public static org.apache.hadoop.fs.Path getRegionDirFromTableDir(org.apache.hadoop.fs.Path, java.lang.String);\n  public static java.util.List<org.apache.hadoop.fs.Path> getFamilyDirs(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.fs.Path> getReferenceFilePaths(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.fs.Path> getReferenceAndLinkFilePaths(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static int getRegionReferenceAndLinkFileCount(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName) throws java.io.IOException, java.lang.InterruptedException;\n  public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, org.apache.hadoop.fs.PathFilter, java.util.concurrent.ExecutorService, org.apache.hadoop.hbase.util.HbckErrorReporter) throws java.io.IOException, java.lang.InterruptedException;\n  public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(java.util.Map<java.lang.String, org.apache.hadoop.fs.Path>, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, org.apache.hadoop.fs.PathFilter, java.util.concurrent.ExecutorService, org.apache.hadoop.hbase.util.FSUtils$ProgressReporter) throws java.io.IOException, java.lang.InterruptedException;\n  public static int getRegionReferenceFileCount(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException, java.lang.InterruptedException;\n  public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter, java.util.concurrent.ExecutorService, org.apache.hadoop.hbase.util.HbckErrorReporter) throws java.io.IOException, java.lang.InterruptedException;\n  public static java.util.Map<java.lang.String, org.apache.hadoop.fs.Path> getTableStoreFilePathMap(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.PathFilter, java.util.concurrent.ExecutorService, org.apache.hadoop.hbase.util.FSUtils$ProgressReporter) throws java.io.IOException, java.lang.InterruptedException;\n  public static java.util.List<org.apache.hadoop.fs.FileStatus> filterFileStatuses(org.apache.hadoop.fs.FileStatus[], org.apache.hadoop.hbase.util.FileStatusFilter);\n  public static java.util.List<org.apache.hadoop.fs.FileStatus> filterFileStatuses(java.util.Iterator<org.apache.hadoop.fs.FileStatus>, org.apache.hadoop.hbase.util.FileStatusFilter);\n  public static java.util.List<org.apache.hadoop.fs.FileStatus> listStatusWithStatusFilter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FileStatusFilter) throws java.io.IOException;\n  public static java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>> getRegionDegreeLocalityMappingFromFS(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.util.Map<java.lang.String, java.util.Map<java.lang.String, java.lang.Float>> getRegionDegreeLocalityMappingFromFS(org.apache.hadoop.conf.Configuration, java.lang.String, int) throws java.io.IOException;\n  public static void setupShortCircuitRead(org.apache.hadoop.conf.Configuration);\n  public static void checkShortCircuitReadBufferSize(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.hdfs.DFSHedgedReadMetrics getDFSHedgedReadMetrics(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.fs.Path> copyFilesParallel(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, int) throws java.io.IOException;\n  public static boolean isSameHdfs(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.FileSystem);\n}\n;;;No. The class contains a set of utility methods for working with the Hadoop file system and does not define a specific message format to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FSVisitor$StoreFileVisitor.class;;;public interface org.apache.hadoop.hbase.util.FSVisitor$StoreFileVisitor {\n  public abstract void storeFile(java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is an interface with a single method `storeFile` that takes three `String` parameters and throws an `IOException`. It does not describe any specific task or job to be executed on a queue.;;;N
org/apache/hadoop/hbase/util/FSVisitor.class;;;public final class org.apache.hadoop.hbase.util.FSVisitor {\n  public static void visitTableStoreFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$StoreFileVisitor) throws java.io.IOException;\n  public static void visitRegionStoreFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.FSVisitor$StoreFileVisitor) throws java.io.IOException;\n}\n;;;No. The class contains only static methods and does not define any fields, constructors, or instance methods. It cannot be instantiated and used as a message definition on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/FileStatusFilter.class;;;public interface org.apache.hadoop.hbase.util.FileStatusFilter {\n  public abstract boolean accept(org.apache.hadoop.fs.FileStatus);\n}\n;;;No.;;;N;;;No. This is a class that defines the behavior of a file status filter and is not a task that needs to be executed on a task queue.;;;N
org/apache/hadoop/hbase/util/GetJavaProperty.class;;;public final class org.apache.hadoop.hbase.util.GetJavaProperty {\n  public static void main(java.lang.String[]);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/HBaseConfTool.class;;;public class org.apache.hadoop.hbase.util.HBaseConfTool {\n  public org.apache.hadoop.hbase.util.HBaseConfTool();\n  public static void main(java.lang.String[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HBaseFsck$1.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$1 extends java.lang.Thread {\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HBaseFsck$2.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$2 implements org.apache.hadoop.hbase.Abortable {\n  public void abort(java.lang.String, java.lang.Throwable);\n  public boolean isAborted();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HBaseFsck$3$1.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$3$1 implements java.util.Comparator<org.apache.hadoop.hbase.Cell> {\n  public int compare(org.apache.hadoop.hbase.Cell, org.apache.hadoop.hbase.Cell);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HBaseFsck$3.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$3 implements org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor {\n  public boolean visit(org.apache.hadoop.hbase.client.Result) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HBaseFsck$CheckRegionConsistencyWorkItem.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$CheckRegionConsistencyWorkItem implements java.util.concurrent.Callable<java.lang.Void> {\n  public synchronized java.lang.Void call() throws java.lang.Exception;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No. The class is an implementation of the Callable interface, used for multithreading purposes, and is not a message definition for a message queue.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/util/HBaseFsck$FileLockCallable.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$FileLockCallable implements java.util.concurrent.Callable<org.apache.hadoop.fs.FSDataOutputStream> {\n  public org.apache.hadoop.hbase.util.HBaseFsck$FileLockCallable(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.RetryCounter);\n  public org.apache.hadoop.fs.FSDataOutputStream call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/HBaseFsck$HBaseFsckTool.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$HBaseFsckTool extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public int run(java.lang.String[]) throws java.lang.Exception;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/HBaseFsck$PrintingErrorReporter.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$PrintingErrorReporter implements org.apache.hadoop.hbase.util.HbckErrorReporter {\n  public int errorCount;\n  public void clear();\n  public synchronized void reportError(org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE, java.lang.String);\n  public synchronized void reportError(org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HbckTableInfo);\n  public synchronized void reportError(org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HbckTableInfo, org.apache.hadoop.hbase.util.HbckRegionInfo);\n  public synchronized void reportError(org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HbckTableInfo, org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo);\n  public synchronized void reportError(java.lang.String);\n  public synchronized void report(java.lang.String);\n  public synchronized int summarize();\n  public java.util.ArrayList<org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE> getErrorList();\n  public synchronized void print(java.lang.String);\n  public boolean tableHasErrors(org.apache.hadoop.hbase.util.HbckTableInfo);\n  public void resetErrors();\n  public synchronized void detail(java.lang.String);\n  public synchronized void progress();\n}\n;;;No. This is a class definition that contains methods, but it is not a message definition on its own.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HBaseFsck$RegionBoundariesInformation.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$RegionBoundariesInformation {\n  public byte[] regionName;\n  public byte[] metaFirstKey;\n  public byte[] metaLastKey;\n  public byte[] storesFirstKey;\n  public byte[] storesLastKey;\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HBaseFsck$WorkItemHdfsDir$1.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$WorkItemHdfsDir$1 implements java.lang.Runnable {\n  public void run();\n}\n;;;No.;;;N;;;Yes. It is a task definition that implements the `Runnable` interface, which represents a task that can be executed asynchronously in a separate thread.;;;Y
org/apache/hadoop/hbase/util/HBaseFsck$WorkItemHdfsDir.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$WorkItemHdfsDir implements java.util.concurrent.Callable<java.lang.Void> {\n  public synchronized java.lang.Void call() throws java.lang.InterruptedException, java.util.concurrent.ExecutionException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;no;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/HBaseFsck$WorkItemHdfsRegionInfo.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$WorkItemHdfsRegionInfo implements java.util.concurrent.Callable<java.lang.Void> {\n  public synchronized java.lang.Void call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes, it is a task definition that might be put on a task queue as it implements the `Callable` interface which is commonly used for defining tasks that can be executed asynchronously.;;;Y
org/apache/hadoop/hbase/util/HBaseFsck$WorkItemOverlapMerge.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$WorkItemOverlapMerge implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call() throws java.lang.Exception;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/HBaseFsck$WorkItemRegion.class;;;class org.apache.hadoop.hbase.util.HBaseFsck$WorkItemRegion implements java.util.concurrent.Callable<java.lang.Void> {\n  public synchronized java.lang.Void call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No. This is a Java class definition and does not contain any message fields. It implements Callable interface which can be used to create a task for execution in a thread pool.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/HBaseFsck.class;;;public class org.apache.hadoop.hbase.util.HBaseFsck extends org.apache.hadoop.conf.Configured implements java.io.Closeable {\n  public static final long DEFAULT_TIME_LAG;\n  public static final long DEFAULT_SLEEP_BEFORE_RERUN;\n  public static final java.lang.String HBCK_LOCK_FILE;\n  public org.apache.hadoop.hbase.util.HBaseFsck(org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.ClassNotFoundException;\n  public org.apache.hadoop.hbase.util.HBaseFsck(org.apache.hadoop.conf.Configuration, java.util.concurrent.ExecutorService) throws org.apache.hadoop.hbase.MasterNotRunningException, org.apache.hadoop.hbase.ZooKeeperConnectionException, java.io.IOException, java.lang.ClassNotFoundException;\n  public static org.apache.hadoop.hbase.util.RetryCounterFactory createLockRetryCounterFactory(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.fs.Path getTmpDir(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FSDataOutputStream> checkAndMarkRunningHbck(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.RetryCounter) throws java.io.IOException;\n  public void connect() throws java.io.IOException;\n  public void offlineHdfsIntegrityRepair() throws java.io.IOException, java.lang.InterruptedException;\n  public int onlineConsistencyRepair() throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException;\n  public int onlineHbck() throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException, org.apache.hadoop.hbase.replication.ReplicationException;\n  public static byte[] keyOnly(byte[]);\n  public void close() throws java.io.IOException;\n  public void checkRegionBoundaries();\n  public org.apache.hadoop.hbase.util.HbckErrorReporter getErrors();\n  public void fixEmptyMetaCells() throws java.io.IOException;\n  public void fixOrphanTables() throws java.io.IOException;\n  public void loadHdfsRegionDirs() throws java.io.IOException, java.lang.InterruptedException;\n  public int mergeRegionDirs(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public static void setDisplayFullReport();\n  public static boolean shouldDisplayFullReport();\n  public static void setForceExclusive();\n  public boolean isExclusive();\n  public void setFixReplication(boolean);\n  public void setCleanReplicationBarrier(boolean);\n  public boolean shouldRerun();\n  public void setFixAssignments(boolean);\n  public void setFixMeta(boolean);\n  public void setFixEmptyMetaCells(boolean);\n  public void setCheckHdfs(boolean);\n  public void setFixHdfsHoles(boolean);\n  public void setFixTableOrphans(boolean);\n  public void setFixHdfsOverlaps(boolean);\n  public void setFixHdfsOrphans(boolean);\n  public void setFixVersionFile(boolean);\n  public boolean shouldFixVersionFile();\n  public void setSidelineBigOverlaps(boolean);\n  public boolean shouldSidelineBigOverlaps();\n  public void setFixSplitParents(boolean);\n  public void setRemoveParents(boolean);\n  public void setFixReferenceFiles(boolean);\n  public void setFixHFileLinks(boolean);\n  public boolean shouldIgnorePreCheckPermission();\n  public void setIgnorePreCheckPermission(boolean);\n  public void setMaxMerge(int);\n  public int getMaxMerge();\n  public void setMaxOverlapsToSideline(int);\n  public int getMaxOverlapsToSideline();\n  public void includeTable(org.apache.hadoop.hbase.TableName);\n  public void setTimeLag(long);\n  public void setSidelineDir(java.lang.String);\n  public org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker getHFilecorruptionChecker();\n  public void setHFileCorruptionChecker(org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker);\n  public void setRetCode(int);\n  public int getRetCode();\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n  public org.apache.hadoop.hbase.util.HBaseFsck exec(java.util.concurrent.ExecutorService, java.lang.String[]) throws org.apache.zookeeper.KeeperException, java.io.IOException, java.lang.InterruptedException, org.apache.hadoop.hbase.replication.ReplicationException;\n  public void setCleanReplicationBarrierTable(java.lang.String);\n  public void cleanReplicationBarrier() throws java.io.IOException;\n  public static void debugLsr(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void debugLsr(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.HbckErrorReporter) throws java.io.IOException;\n}\n;;;Yes, this class could be a message definition that might be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/HBaseFsckRepair.class;;;public class org.apache.hadoop.hbase.util.HBaseFsckRepair {\n  public org.apache.hadoop.hbase.util.HBaseFsckRepair();\n  public static void fixMultiAssignment(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.ServerName>) throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException;\n  public static void fixUnassigned(org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException, org.apache.zookeeper.KeeperException, java.lang.InterruptedException;\n  public static void waitUntilAssigned(org.apache.hadoop.hbase.client.Admin, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException, java.lang.InterruptedException;\n  public static void closeRegionSilentlyAndWait(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException, java.lang.InterruptedException;\n  public static void fixMetaHoleOnlineAndAddReplicas(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, java.util.Collection<org.apache.hadoop.hbase.ServerName>, int) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.regionserver.HRegion createHDFSRegionDir(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public static void removeParentInMeta(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/HFileArchiveUtil.class;;;public final class org.apache.hadoop.hbase.util.HFileArchiveUtil {\n  public static org.apache.hadoop.fs.Path getStoreArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, java.lang.String, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getStoreArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, byte[]) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getStoreArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, byte[]) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getStoreArchivePathForRootDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, byte[]);\n  public static org.apache.hadoop.fs.Path getStoreArchivePathForArchivePath(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.RegionInfo, byte[]);\n  public static org.apache.hadoop.fs.Path getRegionArchiveDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, org.apache.hadoop.fs.Path);\n  public static org.apache.hadoop.fs.Path getRegionArchiveDir(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName, java.lang.String);\n  public static org.apache.hadoop.fs.Path getTableArchivePath(org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.TableName);\n  public static org.apache.hadoop.fs.Path getTableArchivePath(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getArchivePath(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.TableName getTableName(org.apache.hadoop.fs.Path);\n}\n;;;No, this class is not a message definition, as it does not define a specific message structure to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HashedBytes.class;;;public class org.apache.hadoop.hbase.util.HashedBytes {\n  public org.apache.hadoop.hbase.util.HashedBytes(byte[]);\n  public byte[] getBytes();\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n  public java.lang.String toString();\n}\n;;;No. This class does not represent a message, but rather a utility class used in the Hadoop-HBase system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HbckErrorReporter$ERROR_CODE.class;;;public final class org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE extends java.lang.Enum<org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE> {\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE UNKNOWN;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NO_META_REGION;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NULL_META_REGION;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NO_VERSION_FILE;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NOT_IN_META_HDFS;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NOT_IN_META;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NOT_IN_META_OR_DEPLOYED;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NOT_IN_HDFS_OR_DEPLOYED;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NOT_IN_HDFS;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE SERVER_DOES_NOT_MATCH_META;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NOT_DEPLOYED;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE MULTI_DEPLOYED;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE SHOULD_NOT_BE_DEPLOYED;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE MULTI_META_REGION;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE RS_CONNECT_FAILURE;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE FIRST_REGION_STARTKEY_NOT_EMPTY;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE LAST_REGION_ENDKEY_NOT_EMPTY;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE DUPE_STARTKEYS;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE HOLE_IN_REGION_CHAIN;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE OVERLAP_IN_REGION_CHAIN;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE REGION_CYCLE;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE DEGENERATE_REGION;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE ORPHAN_HDFS_REGION;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE LINGERING_SPLIT_PARENT;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NO_TABLEINFO_FILE;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE LINGERING_REFERENCE_HFILE;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE LINGERING_HFILELINK;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE WRONG_USAGE;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE EMPTY_META_CELL;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE EXPIRED_TABLE_LOCK;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE BOUNDARIES_ERROR;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE ORPHAN_TABLE_STATE;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE NO_TABLE_STATE;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE UNDELETED_REPLICATION_QUEUE;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE DUPE_ENDKEYS;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE UNSUPPORTED_OPTION;\n  public static final org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE INVALID_TABLE;\n  public static org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE[] values();\n  public static org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE valueOf(java.lang.String);\n}\n;;;No. This class is an enumeration that lists possible error codes. It doesn't contain any fields or methods that would be used to define a message on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HbckErrorReporter.class;;;public interface org.apache.hadoop.hbase.util.HbckErrorReporter {\n  public abstract void clear();\n  public abstract void report(java.lang.String);\n  public abstract void reportError(java.lang.String);\n  public abstract void reportError(org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE, java.lang.String);\n  public abstract void reportError(org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HbckTableInfo);\n  public abstract void reportError(org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HbckTableInfo, org.apache.hadoop.hbase.util.HbckRegionInfo);\n  public abstract void reportError(org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE, java.lang.String, org.apache.hadoop.hbase.util.HbckTableInfo, org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo);\n  public abstract int summarize();\n  public abstract void detail(java.lang.String);\n  public abstract java.util.ArrayList<org.apache.hadoop.hbase.util.HbckErrorReporter$ERROR_CODE> getErrorList();\n  public abstract void progress();\n  public abstract void print(java.lang.String);\n  public abstract void resetErrors();\n  public abstract boolean tableHasErrors(org.apache.hadoop.hbase.util.HbckTableInfo);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HbckRegionInfo$1.class;;;final class org.apache.hadoop.hbase.util.HbckRegionInfo$1 implements java.util.Comparator<org.apache.hadoop.hbase.util.HbckRegionInfo> {\n  public int compare(org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HbckRegionInfo$HdfsEntry.class;;;public class org.apache.hadoop.hbase.util.HbckRegionInfo$HdfsEntry {\n  public org.apache.hadoop.hbase.util.HbckRegionInfo$HdfsEntry(org.apache.hadoop.fs.Path);\n}\n;;;No. This is a constructor definition for a class and cannot be put on a message queue directly.;;;N;;;No, it is not a task definition, but rather a class definition for an object that could potentially be used as a message payload.;;;N
org/apache/hadoop/hbase/util/HbckRegionInfo$MetaEntry.class;;;public class org.apache.hadoop.hbase.util.HbckRegionInfo$MetaEntry {\n  public org.apache.hadoop.hbase.util.HbckRegionInfo$MetaEntry(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, long);\n  public org.apache.hadoop.hbase.util.HbckRegionInfo$MetaEntry(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, long, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo);\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public org.apache.hadoop.hbase.ServerName getRegionServer();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n}\n;;;No. This class represents a data structure used internally by the HBase hbck (HBase Checking Tool) utility and is not meant to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HbckRegionInfo$OnlineEntry.class;;;class org.apache.hadoop.hbase.util.HbckRegionInfo$OnlineEntry {\n  public org.apache.hadoop.hbase.client.RegionInfo getRegionInfo();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HbckRegionInfo.class;;;public class org.apache.hadoop.hbase.util.HbckRegionInfo implements org.apache.hadoop.hbase.util.KeyRange {\n  public org.apache.hadoop.hbase.util.HbckRegionInfo(org.apache.hadoop.hbase.util.HbckRegionInfo$MetaEntry);\n  public synchronized int getReplicaId();\n  public synchronized void addServer(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName);\n  public synchronized java.lang.String toString();\n  public byte[] getStartKey();\n  public byte[] getEndKey();\n  public org.apache.hadoop.hbase.util.HbckRegionInfo$MetaEntry getMetaEntry();\n  public void setMetaEntry(org.apache.hadoop.hbase.util.HbckRegionInfo$MetaEntry);\n  public org.apache.hadoop.hbase.util.HbckRegionInfo$HdfsEntry getHdfsEntry();\n  public void setHdfsEntry(org.apache.hadoop.hbase.util.HbckRegionInfo$HdfsEntry);\n  public java.util.List<org.apache.hadoop.hbase.util.HbckRegionInfo$OnlineEntry> getOnlineEntries();\n  public java.util.List<org.apache.hadoop.hbase.ServerName> getDeployedOn();\n  public void loadHdfsRegioninfo(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public java.lang.String getRegionNameAsString();\n  public byte[] getRegionName();\n  public org.apache.hadoop.hbase.client.RegionInfo getPrimaryHRIForDeployedReplica();\n  public org.apache.hadoop.fs.Path getHdfsRegionDir();\n  public boolean containsOnlyHdfsEdits();\n  public boolean isHdfsRegioninfoPresent();\n  public long getModTime();\n  public org.apache.hadoop.hbase.client.RegionInfo getHdfsHRI();\n  public void setSkipChecks(boolean);\n  public boolean isSkipChecks();\n  public void setMerged(boolean);\n  public boolean isMerged();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HbckTableInfo$HDFSIntegrityFixer.class;;;class org.apache.hadoop.hbase.util.HbckTableInfo$HDFSIntegrityFixer extends org.apache.hadoop.hbase.util.HbckTableInfo$IntegrityFixSuggester {\n  public void handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleRegionEndKeyNotEmpty(byte[]) throws java.io.IOException;\n  public void handleHoleInRegionChain(byte[], byte[]) throws java.io.IOException;\n  public void handleOverlapGroup(java.util.Collection<org.apache.hadoop.hbase.util.HbckRegionInfo>) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/HbckTableInfo$IntegrityFixSuggester.class;;;class org.apache.hadoop.hbase.util.HbckTableInfo$IntegrityFixSuggester extends org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl {\n  public void handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleRegionEndKeyNotEmpty(byte[]) throws java.io.IOException;\n  public void handleDegenerateRegion(org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleDuplicateStartKeys(org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleSplit(org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleOverlapInRegionChain(org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleHoleInRegionChain(byte[], byte[]) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class definition for handling various types of errors in a HBase table.;;;N
org/apache/hadoop/hbase/util/HbckTableInfo.class;;;public class org.apache.hadoop.hbase.util.HbckTableInfo {\n  public void addRegionInfo(org.apache.hadoop.hbase.util.HbckRegionInfo);\n  public void addServer(org.apache.hadoop.hbase.ServerName);\n  public org.apache.hadoop.hbase.TableName getName();\n  public int getNumRegions();\n  public synchronized org.apache.hbase.thirdparty.com.google.common.collect.ImmutableList<org.apache.hadoop.hbase.client.RegionInfo> getRegionsFromMeta(java.util.TreeMap<java.lang.String, org.apache.hadoop.hbase.util.HbckRegionInfo>);\n  public boolean checkRegionChain(org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandler) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/IdReadWriteLock.class;;;public abstract class org.apache.hadoop.hbase.util.IdReadWriteLock<T> {\n  public org.apache.hadoop.hbase.util.IdReadWriteLock();\n  public abstract java.util.concurrent.locks.ReentrantReadWriteLock getLock(T);\n  public void waitForWaiters(T, int) throws java.lang.InterruptedException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/IdReadWriteLockStrongRef.class;;;public class org.apache.hadoop.hbase.util.IdReadWriteLockStrongRef<T> extends org.apache.hadoop.hbase.util.IdReadWriteLock<T> {\n  public org.apache.hadoop.hbase.util.IdReadWriteLockStrongRef();\n  public java.util.concurrent.locks.ReentrantReadWriteLock getLock(T);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/IdReadWriteLockWithObjectPool$1.class;;;class org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$1 implements org.apache.hadoop.hbase.util.ObjectPool$ObjectFactory<T, java.util.concurrent.locks.ReentrantReadWriteLock> {\n  public java.util.concurrent.locks.ReentrantReadWriteLock createObject(T);\n  public java.lang.Object createObject(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/IdReadWriteLockWithObjectPool$2.class;;;class org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$2 implements org.apache.hadoop.hbase.util.ObjectPool$ObjectFactory<T, java.util.concurrent.locks.ReentrantReadWriteLock> {\n  public java.util.concurrent.locks.ReentrantReadWriteLock createObject(T);\n  public java.lang.Object createObject(java.lang.Object);\n}\n;;;yes;;;Y;;;;;;N
org/apache/hadoop/hbase/util/IdReadWriteLockWithObjectPool$3.class;;;class org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$3 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/IdReadWriteLockWithObjectPool$ReferenceType.class;;;public final class org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$ReferenceType extends java.lang.Enum<org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$ReferenceType> {\n  public static final org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$ReferenceType WEAK;\n  public static final org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$ReferenceType SOFT;\n  public static org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$ReferenceType[] values();\n  public static org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$ReferenceType valueOf(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/IdReadWriteLockWithObjectPool.class;;;public class org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool<T> extends org.apache.hadoop.hbase.util.IdReadWriteLock<T> {\n  public org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool();\n  public org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool(org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$ReferenceType);\n  public java.util.concurrent.locks.ReentrantReadWriteLock getLock(T);\n  public org.apache.hadoop.hbase.util.IdReadWriteLockWithObjectPool$ReferenceType getReferenceType();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/JVMClusterUtil$MasterThread.class;;;public class org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread extends java.lang.Thread {\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread(org.apache.hadoop.hbase.master.HMaster, int);\n  public org.apache.hadoop.hbase.master.HMaster getMaster();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/JVMClusterUtil$RegionServerThread.class;;;public class org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread extends java.lang.Thread {\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread(org.apache.hadoop.hbase.regionserver.HRegionServer, int);\n  public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer();\n  public void waitForServerOnline();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/JVMClusterUtil.class;;;public class org.apache.hadoop.hbase.util.JVMClusterUtil {\n  public org.apache.hadoop.hbase.util.JVMClusterUtil();\n  public static org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread createRegionServerThread(org.apache.hadoop.conf.Configuration, java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>, int) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread createMasterThread(org.apache.hadoop.conf.Configuration, java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, int) throws java.io.IOException;\n  public static java.lang.String startup(java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread>, java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread>) throws java.io.IOException;\n  public static void shutdown(java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread>, java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread>);\n}\n;;;No. This class contains methods for starting and stopping HBase clusters, but it is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/JvmPauseMonitor$1.class;;;class org.apache.hadoop.hbase.util.JvmPauseMonitor$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition that is specific to the Hadoop ecosystem.;;;N
org/apache/hadoop/hbase/util/JvmPauseMonitor$GcTimes.class;;;class org.apache.hadoop.hbase.util.JvmPauseMonitor$GcTimes {\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/JvmPauseMonitor$Monitor.class;;;class org.apache.hadoop.hbase.util.JvmPauseMonitor$Monitor implements java.lang.Runnable {\n  public void run();\n}\n;;;No.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/util/JvmPauseMonitor.class;;;public class org.apache.hadoop.hbase.util.JvmPauseMonitor {\n  public static final java.lang.String WARN_THRESHOLD_KEY;\n  public static final java.lang.String INFO_THRESHOLD_KEY;\n  public org.apache.hadoop.hbase.util.JvmPauseMonitor(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.util.JvmPauseMonitor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.metrics.JvmPauseMonitorSource);\n  public void start();\n  public void stop();\n  public void updateMetrics(long, boolean);\n  public org.apache.hadoop.hbase.metrics.JvmPauseMonitorSource getMetricsSource();\n  public void setMetricsSource(org.apache.hadoop.hbase.metrics.JvmPauseMonitorSource);\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No, this class is not a message definition that might be put on a message queue. It appears to be a utility class for monitoring JVM pauses in HBase applications.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/JvmVersion.class;;;public abstract class org.apache.hadoop.hbase.util.JvmVersion {\n  public org.apache.hadoop.hbase.util.JvmVersion();\n  public static boolean isBadJvmVersion();\n  public static java.lang.String getVersion();\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class representing a utility for getting the version and checking the compatibility of the Java Virtual Machine (JVM) for Hadoop HBase framework.;;;N
org/apache/hadoop/hbase/util/KeyRange.class;;;public interface org.apache.hadoop.hbase.util.KeyRange {\n  public abstract byte[] getStartKey();\n  public abstract byte[] getEndKey();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/LeaseNotRecoveredException.class;;;public class org.apache.hadoop.hbase.util.LeaseNotRecoveredException extends org.apache.hadoop.hbase.HBaseIOException {\n  public org.apache.hadoop.hbase.util.LeaseNotRecoveredException();\n  public org.apache.hadoop.hbase.util.LeaseNotRecoveredException(java.lang.String);\n  public org.apache.hadoop.hbase.util.LeaseNotRecoveredException(java.lang.String, java.lang.Throwable);\n  public org.apache.hadoop.hbase.util.LeaseNotRecoveredException(java.lang.Throwable);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a Java class definition for an exception that can be thrown in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/LossyCounting$LossyCountingListener.class;;;public interface org.apache.hadoop.hbase.util.LossyCounting$LossyCountingListener<T> {\n  public abstract void sweep(T);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/LossyCounting$SweepRunnable.class;;;class org.apache.hadoop.hbase.util.LossyCounting$SweepRunnable implements java.lang.Runnable {\n  public void run();\n}\n;;;No. This is a class that implements the Runnable interface in Java, but it is not a message definition that would typically be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/LossyCounting.class;;;public class org.apache.hadoop.hbase.util.LossyCounting<T> {\n  public org.apache.hadoop.hbase.util.LossyCounting(java.lang.String, double, org.apache.hadoop.hbase.util.LossyCounting$LossyCountingListener<T>);\n  public org.apache.hadoop.hbase.util.LossyCounting(java.lang.String, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.LossyCounting$LossyCountingListener<T>);\n  public void add(T);\n  public void sweep();\n  public long getBucketSize();\n  public long getDataSize();\n  public boolean contains(T);\n  public java.util.Set<T> getElements();\n  public long getCurrentTerm();\n  public java.util.concurrent.Future<?> getSweepFuture();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ManualEnvironmentEdge.class;;;public class org.apache.hadoop.hbase.util.ManualEnvironmentEdge implements org.apache.hadoop.hbase.util.EnvironmentEdge {\n  public org.apache.hadoop.hbase.util.ManualEnvironmentEdge();\n  public void setValue(long);\n  public void incValue(long);\n  public long currentTime();\n}\n;;;No, this class is not a message definition. It is a class that implements an interface, and provides methods for interacting with a simulated clock.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ModifyRegionUtils$1.class;;;final class org.apache.hadoop.hbase.util.ModifyRegionUtils$1 implements java.util.concurrent.Callable<org.apache.hadoop.hbase.client.RegionInfo> {\n  public org.apache.hadoop.hbase.client.RegionInfo call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition that implements the Callable interface and contains two method definitions.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/ModifyRegionUtils$2.class;;;final class org.apache.hadoop.hbase.util.ModifyRegionUtils$2 implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/ModifyRegionUtils$RegionEditTask.class;;;public interface org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask {\n  public abstract void editRegion(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;Yes, it is a message definition interface that can be put on a message queue.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/ModifyRegionUtils$RegionFillTask.class;;;public interface org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask {\n  public abstract void fillRegion(org.apache.hadoop.hbase.regionserver.HRegion) throws java.io.IOException;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/ModifyRegionUtils.class;;;public abstract class org.apache.hadoop.hbase.util.ModifyRegionUtils {\n  public static org.apache.hadoop.hbase.client.RegionInfo[] createRegionInfos(org.apache.hadoop.hbase.client.TableDescriptor, byte[][]);\n  public static java.util.List<org.apache.hadoop.hbase.client.RegionInfo> createRegions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.RegionInfo> createRegions(java.util.concurrent.ThreadPoolExecutor, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo[], org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.RegionInfo createRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionFillTask) throws java.io.IOException;\n  public static void editRegions(java.util.concurrent.ThreadPoolExecutor, java.util.Collection<org.apache.hadoop.hbase.client.RegionInfo>, org.apache.hadoop.hbase.util.ModifyRegionUtils$RegionEditTask) throws java.io.IOException;\n}\n;;;No, this class contains static utility methods for modifying regions in HBase, but it's not a message definition that would be put on a message queue.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/MoveWithAck.class;;;class org.apache.hadoop.hbase.util.MoveWithAck implements java.util.concurrent.Callable<java.lang.Boolean> {\n  public java.lang.Boolean call() throws java.io.IOException, java.lang.InterruptedException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No. This class is a Java Callable implementation and is not a message definition that might be put on a message queue.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/util/MoveWithoutAck.class;;;class org.apache.hadoop.hbase.util.MoveWithoutAck implements java.util.concurrent.Callable<java.lang.Boolean> {\n  public java.lang.Boolean call();\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/MunkresAssignment.class;;;public class org.apache.hadoop.hbase.util.MunkresAssignment {\n  public org.apache.hadoop.hbase.util.MunkresAssignment(float[][]);\n  public int[] solve();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/NettyEventLoopGroupConfig.class;;;public class org.apache.hadoop.hbase.util.NettyEventLoopGroupConfig {\n  public org.apache.hadoop.hbase.util.NettyEventLoopGroupConfig(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public org.apache.hbase.thirdparty.io.netty.channel.EventLoopGroup group();\n  public java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.ServerChannel> serverChannelClass();\n  public java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.Channel> clientChannelClass();\n  public static org.apache.hadoop.hbase.util.NettyEventLoopGroupConfig setup(org.apache.hadoop.conf.Configuration, java.lang.String);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue, but a configuration class for setting up Netty event loop group.;;;N
org/apache/hadoop/hbase/util/OOMEChecker.class;;;public final class org.apache.hadoop.hbase.util.OOMEChecker {\n  public static boolean exitIfOOME(java.lang.Throwable, java.lang.String);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/RegionMover$1.class;;;class org.apache.hadoop.hbase.util.RegionMover$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/RegionMover$RegionMoverBuilder.class;;;public class org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder {\n  public org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder(java.lang.String);\n  public org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder(java.lang.String, org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder filename(java.lang.String);\n  public org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder maxthreads(int);\n  public org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder excludeFile(java.lang.String);\n  public org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder designatedFile(java.lang.String);\n  public org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder ack(boolean);\n  public org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder timeout(int);\n  public org.apache.hadoop.hbase.util.RegionMover$RegionMoverBuilder rackManager(org.apache.hadoop.hbase.master.RackManager);\n  public org.apache.hadoop.hbase.util.RegionMover build() throws java.io.IOException;\n}\n;;;No, it is a builder class for creating an object of RegionMover, which may then be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a builder class definition for creating an object instance of `org.apache.hadoop.hbase.util.RegionMover` class.;;;N
org/apache/hadoop/hbase/util/RegionMover.class;;;public class org.apache.hadoop.hbase.util.RegionMover extends org.apache.hadoop.hbase.util.AbstractHBaseTool implements java.io.Closeable {\n  public static final java.lang.String MOVE_RETRIES_MAX_KEY;\n  public static final java.lang.String MOVE_WAIT_MAX_KEY;\n  public static final java.lang.String SERVERSTART_WAIT_MAX_KEY;\n  public static final int DEFAULT_MOVE_RETRIES_MAX;\n  public static final int DEFAULT_MOVE_WAIT_MAX;\n  public static final int DEFAULT_SERVERSTART_WAIT_MAX;\n  public void close();\n  public boolean load() throws java.util.concurrent.ExecutionException, java.lang.InterruptedException, java.util.concurrent.TimeoutException;\n  public boolean unload() throws java.lang.InterruptedException, java.util.concurrent.ExecutionException, java.util.concurrent.TimeoutException;\n  public boolean unloadFromRack() throws java.lang.InterruptedException, java.util.concurrent.ExecutionException, java.util.concurrent.TimeoutException;\n  public static void main(java.lang.String[]);\n}\n;;;No, this class is not a message definition.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/util/RegionSplitCalculator$1.class;;;final class org.apache.hadoop.hbase.util.RegionSplitCalculator$1 extends org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator {\n  public int compare(byte[], byte[]);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No. This is a Java class definition and cannot be put on a message queue directly.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/RegionSplitCalculator.class;;;public class org.apache.hadoop.hbase.util.RegionSplitCalculator<R extends org.apache.hadoop.hbase.util.KeyRange> {\n  public static final java.util.Comparator<byte[]> BYTES_COMPARATOR;\n  public org.apache.hadoop.hbase.util.RegionSplitCalculator(java.util.Comparator<R>);\n  public boolean add(R);\n  public org.apache.hbase.thirdparty.com.google.common.collect.Multimap<byte[], R> calcCoverage();\n  public java.util.TreeSet<byte[]> getSplits();\n  public org.apache.hbase.thirdparty.com.google.common.collect.Multimap<byte[], R> getStarts();\n  public static <R extends org.apache.hadoop.hbase.util.KeyRange> java.util.List<R> findBigRanges(java.util.Collection<R>, int);\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/util/RegionSplitter$DecimalStringSplit.class;;;public class org.apache.hadoop.hbase.util.RegionSplitter$DecimalStringSplit extends org.apache.hadoop.hbase.util.RegionSplitter$NumberStringSplit {\n  public org.apache.hadoop.hbase.util.RegionSplitter$DecimalStringSplit();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/RegionSplitter$HexStringSplit.class;;;public class org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit extends org.apache.hadoop.hbase.util.RegionSplitter$NumberStringSplit {\n  public org.apache.hadoop.hbase.util.RegionSplitter$HexStringSplit();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/RegionSplitter$NumberStringSplit.class;;;public abstract class org.apache.hadoop.hbase.util.RegionSplitter$NumberStringSplit implements org.apache.hadoop.hbase.util.RegionSplitter$SplitAlgorithm {\n  public byte[] split(byte[], byte[]);\n  public byte[][] split(int);\n  public byte[][] split(byte[], byte[], int, boolean);\n  public byte[] firstRow();\n  public byte[] lastRow();\n  public void setFirstRow(java.lang.String);\n  public void setLastRow(java.lang.String);\n  public byte[] strToRow(java.lang.String);\n  public java.lang.String rowToStr(byte[]);\n  public java.lang.String separator();\n  public void setFirstRow(byte[]);\n  public void setLastRow(byte[]);\n  public java.math.BigInteger split2(java.math.BigInteger, java.math.BigInteger);\n  public byte[][] convertToBytes(java.math.BigInteger[]);\n  public byte[] convertToByte(java.math.BigInteger, int);\n  public byte[] convertToByte(java.math.BigInteger);\n  public java.math.BigInteger convertToBigInteger(byte[]);\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/RegionSplitter$SplitAlgorithm.class;;;public interface org.apache.hadoop.hbase.util.RegionSplitter$SplitAlgorithm {\n  public abstract byte[] split(byte[], byte[]);\n  public abstract byte[][] split(int);\n  public abstract byte[][] split(byte[], byte[], int, boolean);\n  public abstract byte[] firstRow();\n  public abstract byte[] lastRow();\n  public abstract void setFirstRow(java.lang.String);\n  public abstract void setLastRow(java.lang.String);\n  public abstract byte[] strToRow(java.lang.String);\n  public abstract java.lang.String rowToStr(byte[]);\n  public abstract java.lang.String separator();\n  public abstract void setFirstRow(byte[]);\n  public abstract void setLastRow(byte[]);\n}\n;;;No.;;;N;;;No, it is not a task definition. It is an interface defining methods for splitting regions in HBase.;;;N
org/apache/hadoop/hbase/util/RegionSplitter$UniformSplit.class;;;public class org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit implements org.apache.hadoop.hbase.util.RegionSplitter$SplitAlgorithm {\n  public org.apache.hadoop.hbase.util.RegionSplitter$UniformSplit();\n  public byte[] split(byte[], byte[]);\n  public byte[][] split(int);\n  public byte[][] split(byte[], byte[], int, boolean);\n  public byte[] firstRow();\n  public byte[] lastRow();\n  public void setFirstRow(java.lang.String);\n  public void setLastRow(java.lang.String);\n  public void setFirstRow(byte[]);\n  public void setLastRow(byte[]);\n  public byte[] strToRow(java.lang.String);\n  public java.lang.String rowToStr(byte[]);\n  public java.lang.String separator();\n  public java.lang.String toString();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/util/RegionSplitter.class;;;public class org.apache.hadoop.hbase.util.RegionSplitter {\n  public org.apache.hadoop.hbase.util.RegionSplitter();\n  public static void main(java.lang.String[]) throws java.io.IOException, java.lang.InterruptedException, org.apache.hbase.thirdparty.org.apache.commons.cli.ParseException;\n  public static org.apache.hadoop.hbase.util.RegionSplitter$SplitAlgorithm newSplitAlgoInstance(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n}\n;;;No, this is not a message definition. It's a Java class definition that provides methods for splitting HBase regions.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/RollingStatCalculator.class;;;public class org.apache.hadoop.hbase.util.RollingStatCalculator {\n  public org.apache.hadoop.hbase.util.RollingStatCalculator(int);\n  public void insertDataValue(long);\n  public double getMean();\n  public double getDeviation();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/RowBloomContext.class;;;public class org.apache.hadoop.hbase.util.RowBloomContext extends org.apache.hadoop.hbase.util.BloomContext {\n  public org.apache.hadoop.hbase.util.RowBloomContext(org.apache.hadoop.hbase.util.BloomFilterWriter, org.apache.hadoop.hbase.CellComparator);\n  public void addLastBloomKey(org.apache.hadoop.hbase.io.hfile.HFile$Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/RowColBloomContext.class;;;public class org.apache.hadoop.hbase.util.RowColBloomContext extends org.apache.hadoop.hbase.util.BloomContext {\n  public org.apache.hadoop.hbase.util.RowColBloomContext(org.apache.hadoop.hbase.util.BloomFilterWriter, org.apache.hadoop.hbase.CellComparator);\n  public void addLastBloomKey(org.apache.hadoop.hbase.io.hfile.HFile$Writer) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/RowPrefixFixedLengthBloomContext.class;;;public class org.apache.hadoop.hbase.util.RowPrefixFixedLengthBloomContext extends org.apache.hadoop.hbase.util.RowBloomContext {\n  public org.apache.hadoop.hbase.util.RowPrefixFixedLengthBloomContext(org.apache.hadoop.hbase.util.BloomFilterWriter, org.apache.hadoop.hbase.CellComparator, int);\n  public void writeBloom(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/ServerCommandLine$1.class;;;final class org.apache.hadoop.hbase.util.ServerCommandLine$1 extends java.util.HashSet<java.lang.String> {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ServerCommandLine.class;;;public abstract class org.apache.hadoop.hbase.util.ServerCommandLine extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public org.apache.hadoop.hbase.util.ServerCommandLine();\n  public static void logJVMInfo();\n  public static void logProcessInfo(org.apache.hadoop.conf.Configuration);\n  public void doMain(java.lang.String[]);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ServerRegionReplicaUtil.class;;;public class org.apache.hadoop.hbase.util.ServerRegionReplicaUtil extends org.apache.hadoop.hbase.client.RegionReplicaUtil {\n  public static final java.lang.String REGION_REPLICA_REPLICATION_CONF_KEY;\n  public static final java.lang.String REGION_REPLICA_REPLICATION_PEER;\n  public static final java.lang.String REGION_REPLICA_REPLICATION_CATALOG_CONF_KEY;\n  public static final java.lang.String REGION_REPLICA_STORE_FILE_REFRESH;\n  public static final java.lang.String REGION_REPLICA_STORE_FILE_REFRESH_MEMSTORE_MULTIPLIER;\n  public org.apache.hadoop.hbase.util.ServerRegionReplicaUtil();\n  public static org.apache.hadoop.hbase.client.RegionInfo getRegionInfoForFs(org.apache.hadoop.hbase.client.RegionInfo);\n  public static boolean isReadOnly(org.apache.hadoop.hbase.regionserver.HRegion);\n  public static boolean shouldReplayRecoveredEdits(org.apache.hadoop.hbase.regionserver.HRegion);\n  public static org.apache.hadoop.hbase.regionserver.StoreFileInfo getStoreFileInfo(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo, java.lang.String, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static boolean isRegionReplicaReplicationEnabled(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName);\n  public static boolean isMetaRegionReplicaReplicationEnabled(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName);\n  public static boolean isRegionReplicaWaitForPrimaryFlushEnabled(org.apache.hadoop.conf.Configuration);\n  public static boolean isRegionReplicaStoreFileRefreshEnabled(org.apache.hadoop.conf.Configuration);\n  public static double getRegionReplicaStoreFileRefreshMultiplier(org.apache.hadoop.conf.Configuration);\n}\n;;;No, this class is a utility class for working with server region replicas in HBase, and is not a message definition to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ShutdownHookManager$1.class;;;class org.apache.hadoop.hbase.util.ShutdownHookManager$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ShutdownHookManager$ShutdownHookManagerV1.class;;;class org.apache.hadoop.hbase.util.ShutdownHookManager$ShutdownHookManagerV1 extends org.apache.hadoop.hbase.util.ShutdownHookManager {\n  public void addShutdownHook(java.lang.Thread, int);\n  public boolean removeShutdownHook(java.lang.Runnable);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ShutdownHookManager$ShutdownHookManagerV2.class;;;class org.apache.hadoop.hbase.util.ShutdownHookManager$ShutdownHookManagerV2 extends org.apache.hadoop.hbase.util.ShutdownHookManager {\n  public void addShutdownHook(java.lang.Thread, int);\n  public boolean removeShutdownHook(java.lang.Runnable);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/ShutdownHookManager.class;;;public abstract class org.apache.hadoop.hbase.util.ShutdownHookManager {\n  public org.apache.hadoop.hbase.util.ShutdownHookManager();\n  public abstract void addShutdownHook(java.lang.Thread, int);\n  public abstract boolean removeShutdownHook(java.lang.Runnable);\n  public static void affixShutdownHook(java.lang.Thread, int);\n  public static boolean deleteShutdownHook(java.lang.Runnable);\n}\n;;;No. This class is not a message definition that might be put on a message queue. It is a utility class for managing shutdown hooks in Hadoop-based applications.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/SortedList.class;;;public class org.apache.hadoop.hbase.util.SortedList<E> implements java.util.List<E>, java.util.RandomAccess {\n  public org.apache.hadoop.hbase.util.SortedList(java.util.Comparator<? super E>);\n  public org.apache.hadoop.hbase.util.SortedList(java.util.Collection<? extends E>, java.util.Comparator<? super E>);\n  public java.util.List<E> get();\n  public int size();\n  public boolean isEmpty();\n  public boolean contains(java.lang.Object);\n  public java.util.Iterator<E> iterator();\n  public java.lang.Object[] toArray();\n  public <T> T[] toArray(T[]);\n  public synchronized boolean add(E);\n  public synchronized boolean remove(java.lang.Object);\n  public boolean containsAll(java.util.Collection<?>);\n  public synchronized boolean addAll(java.util.Collection<? extends E>);\n  public synchronized boolean addAll(int, java.util.Collection<? extends E>);\n  public synchronized boolean removeAll(java.util.Collection<?>);\n  public synchronized boolean retainAll(java.util.Collection<?>);\n  public synchronized void clear();\n  public synchronized E get(int);\n  public synchronized E set(int, E);\n  public synchronized void add(int, E);\n  public synchronized E remove(int);\n  public int indexOf(java.lang.Object);\n  public int lastIndexOf(java.lang.Object);\n  public java.util.ListIterator<E> listIterator();\n  public java.util.ListIterator<E> listIterator(int);\n  public java.util.List<E> subList(int, int);\n}\n;;;No. This is not a message definition, but rather a class definition for a utility class in the Hadoop library that implements the List and RandomAccess interfaces.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/StealJobQueue$1.class;;;class org.apache.hadoop.hbase.util.StealJobQueue$1 extends java.util.concurrent.PriorityBlockingQueue<T> {\n  public boolean offer(T);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/StealJobQueue.class;;;public class org.apache.hadoop.hbase.util.StealJobQueue<T> extends java.util.concurrent.PriorityBlockingQueue<T> {\n  public org.apache.hadoop.hbase.util.StealJobQueue(java.util.Comparator<? super T>);\n  public org.apache.hadoop.hbase.util.StealJobQueue(int, int, java.util.Comparator<? super T>);\n  public java.util.concurrent.BlockingQueue<T> getStealFromQueue();\n  public boolean offer(T);\n  public T take() throws java.lang.InterruptedException;\n  public T poll(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n}\n;;;No, this is not a message definition. It is a class definition for a priority blocking queue with additional methods. It could potentially be used within a messaging system, but it is not a message definition itself.;;;N;;;No, it is not a task definition. It is a class that extends the `PriorityBlockingQueue` class and provides some additional methods. It may be used to implement a message queue, but it is not itself a message or task definition.;;;N
org/apache/hadoop/hbase/util/TableDescriptorChecker.class;;;public final class org.apache.hadoop.hbase.util.TableDescriptorChecker {\n  public static final java.lang.String TABLE_SANITY_CHECKS;\n  public static final boolean DEFAULT_TABLE_SANITY_CHECKS;\n  public static final java.lang.String MASTER_CHECK_COMPRESSION;\n  public static final boolean DEFAULT_MASTER_CHECK_COMPRESSION;\n  public static final java.lang.String MASTER_CHECK_ENCRYPTION;\n  public static final boolean DEFAULT_MASTER_CHECK_ENCRYPTION;\n  public static void sanityCheck(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public static void checkCompression(org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public static void checkEncryption(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public static void checkClassLoading(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/YammerHistogramUtils.class;;;public final class org.apache.hadoop.hbase.util.YammerHistogramUtils {\n  public static com.codahale.metrics.Histogram newHistogram(com.codahale.metrics.Reservoir);\n  public static java.lang.String getShortHistogramReport(com.codahale.metrics.Histogram);\n  public static java.lang.String getHistogramReport(com.codahale.metrics.Histogram);\n  public static java.lang.String getPrettyHistogramReport(com.codahale.metrics.Histogram);\n}\n;;;No, this is not a message definition. It is a utility class with static methods for working with histograms using the Yammer metrics library.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ZKDataMigrator$1.class;;;class org.apache.hadoop.hbase.util.ZKDataMigrator$1 {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/ZKDataMigrator.class;;;public class org.apache.hadoop.hbase.util.ZKDataMigrator {\n  public static java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableState$State> queryForTableStates(org.apache.hadoop.hbase.zookeeper.ZKWatcher) throws org.apache.zookeeper.KeeperException, java.lang.InterruptedException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/compaction/ClusterCompactionQueues.class;;;class org.apache.hadoop.hbase.util.compaction.ClusterCompactionQueues {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/compaction/MajorCompactionRequest.class;;;class org.apache.hadoop.hbase.util.compaction.MajorCompactionRequest {\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/compaction/MajorCompactionTTLRequest.class;;;public class org.apache.hadoop.hbase.util.compaction.MajorCompactionTTLRequest extends org.apache.hadoop.hbase.util.compaction.MajorCompactionRequest {\n  public java.lang.String toString();\n}\n;;;No, it is not a complete message definition as it is missing the message fields and their data types.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/compaction/MajorCompactor$Compact.class;;;class org.apache.hadoop.hbase.util.compaction.MajorCompactor$Compact implements java.lang.Runnable {\n  public void run();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/compaction/MajorCompactor.class;;;public class org.apache.hadoop.hbase.util.compaction.MajorCompactor extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.util.Tool {\n  public org.apache.hadoop.hbase.util.compaction.MajorCompactor(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName, java.util.Set<java.lang.String>, int, long, long) throws java.io.IOException;\n  public void compactAllRegions() throws java.lang.Exception;\n  public void shutdown() throws java.lang.Exception;\n  public void setNumServers(int);\n  public void setNumRegions(int);\n  public void setSkipWait(boolean);\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/compaction/MajorCompactorTTL.class;;;public class org.apache.hadoop.hbase.util.compaction.MajorCompactorTTL extends org.apache.hadoop.hbase.util.compaction.MajorCompactor {\n  public org.apache.hadoop.hbase.util.compaction.MajorCompactorTTL(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.TableDescriptor, int, long) throws java.io.IOException;\n  public int compactRegionsTTLOnTable(org.apache.hadoop.conf.Configuration, java.lang.String, int, long, int, int, boolean, boolean) throws java.lang.Exception;\n  public int run(java.lang.String[]) throws java.lang.Exception;\n  public static void main(java.lang.String[]) throws java.lang.Exception;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker$MobRegionDirChecker.class;;;class org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker$MobRegionDirChecker extends org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker$RegionDirChecker {\n  public java.lang.Void call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker$RegionDirChecker.class;;;class org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker$RegionDirChecker implements java.util.concurrent.Callable<java.lang.Void> {\n  public java.lang.Void call() throws java.io.IOException;\n  public java.lang.Object call() throws java.lang.Exception;\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/util/hbck/HFileCorruptionChecker.class;;;public class org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker {\n  public org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker(org.apache.hadoop.conf.Configuration, java.util.concurrent.ExecutorService, boolean) throws java.io.IOException;\n  public void checkTables(java.util.Collection<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n  public java.util.Collection<org.apache.hadoop.fs.Path> getFailures();\n  public java.util.Collection<org.apache.hadoop.fs.Path> getCorrupted();\n  public int getHFilesChecked();\n  public java.util.Collection<org.apache.hadoop.fs.Path> getQuarantined();\n  public java.util.Collection<org.apache.hadoop.fs.Path> getMissing();\n  public java.util.Collection<org.apache.hadoop.fs.Path> getFailureMobFiles();\n  public java.util.Collection<org.apache.hadoop.fs.Path> getCorruptedMobFiles();\n  public int getMobFilesChecked();\n  public java.util.Collection<org.apache.hadoop.fs.Path> getQuarantinedMobFiles();\n  public java.util.Collection<org.apache.hadoop.fs.Path> getMissedMobFiles();\n  public void report(org.apache.hadoop.hbase.util.HbckErrorReporter);\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/hbck/ReplicationChecker.class;;;public class org.apache.hadoop.hbase.util.hbck.ReplicationChecker {\n  public org.apache.hadoop.hbase.util.hbck.ReplicationChecker(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.util.HbckErrorReporter);\n  public boolean hasUnDeletedQueues();\n  public void checkUnDeletedQueues() throws org.apache.hadoop.hbase.replication.ReplicationException;\n  public void fixUnDeletedQueues() throws org.apache.hadoop.hbase.replication.ReplicationException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandler.class;;;public interface org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandler {\n  public abstract org.apache.hadoop.hbase.util.HbckTableInfo getTableInfo();\n  public abstract void setTableInfo(org.apache.hadoop.hbase.util.HbckTableInfo);\n  public abstract void handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public abstract void handleRegionEndKeyNotEmpty(byte[]) throws java.io.IOException;\n  public abstract void handleDegenerateRegion(org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public abstract void handleDuplicateStartKeys(org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public abstract void handleSplit(org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public abstract void handleOverlapInRegionChain(org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public abstract void handleHoleInRegionChain(byte[], byte[]) throws java.io.IOException;\n  public abstract void handleOverlapGroup(java.util.Collection<org.apache.hadoop.hbase.util.HbckRegionInfo>) throws java.io.IOException;\n}\n;;;No, this is not a message definition that might be put on a message queue. It is an interface defining a set of methods that can be implemented by a class for handling errors related to HBase table integrity.;;;N;;;No.;;;N
org/apache/hadoop/hbase/util/hbck/TableIntegrityErrorHandlerImpl.class;;;public abstract class org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl implements org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandler {\n  public org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl();\n  public org.apache.hadoop.hbase.util.HbckTableInfo getTableInfo();\n  public void setTableInfo(org.apache.hadoop.hbase.util.HbckTableInfo);\n  public void handleRegionStartKeyNotEmpty(org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleRegionEndKeyNotEmpty(byte[]) throws java.io.IOException;\n  public void handleDegenerateRegion(org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleDuplicateStartKeys(org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleOverlapInRegionChain(org.apache.hadoop.hbase.util.HbckRegionInfo, org.apache.hadoop.hbase.util.HbckRegionInfo) throws java.io.IOException;\n  public void handleHoleInRegionChain(byte[], byte[]) throws java.io.IOException;\n  public void handleOverlapGroup(java.util.Collection<org.apache.hadoop.hbase.util.HbckRegionInfo>) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/AbstractFSWALProvider$1.class;;;final class org.apache.hadoop.hbase.wal.AbstractFSWALProvider$1 implements org.apache.hadoop.hbase.util.CancelableProgressable {\n  public boolean progress();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/wal/AbstractFSWALProvider$Reader.class;;;public interface org.apache.hadoop.hbase.wal.AbstractFSWALProvider$Reader extends org.apache.hadoop.hbase.wal.WAL$Reader {\n  public abstract void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FSDataInputStream) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/AbstractFSWALProvider$WALStartTimeComparator.class;;;public class org.apache.hadoop.hbase.wal.AbstractFSWALProvider$WALStartTimeComparator implements java.util.Comparator<org.apache.hadoop.fs.Path> {\n  public org.apache.hadoop.hbase.wal.AbstractFSWALProvider$WALStartTimeComparator();\n  public int compare(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path);\n  public static long getTS(org.apache.hadoop.fs.Path);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No, this class is not a message definition that might be put on a message queue.;;;N;;;No. It is a class definition that implements the Comparator interface for paths in the context of HBase Write-Ahead Logging (WAL) provider. It is not a task definition that can be put on a task queue.;;;N
org/apache/hadoop/hbase/wal/AbstractFSWALProvider.class;;;public abstract class org.apache.hadoop.hbase.wal.AbstractFSWALProvider<T extends org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL<?>> implements org.apache.hadoop.hbase.wal.WALProvider {\n  public static final java.lang.String SEPARATE_OLDLOGDIR;\n  public static final boolean DEFAULT_SEPARATE_OLDLOGDIR;\n  public static final java.lang.String WAL_FILE_NAME_DELIMITER;\n  public static final java.lang.String META_WAL_PROVIDER_ID;\n  public static final java.lang.String SPLITTING_EXT;\n  public org.apache.hadoop.hbase.wal.AbstractFSWALProvider();\n  public void init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.Abortable) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.wal.WAL> getWALs();\n  public T getWAL(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void shutdown() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getNumLogFiles();\n  public long getLogFileSize();\n  public static int getNumRolledLogFiles(org.apache.hadoop.hbase.wal.WAL);\n  public static long getLogFileSize(org.apache.hadoop.hbase.wal.WAL);\n  public static org.apache.hadoop.fs.Path getCurrentFileName(org.apache.hadoop.hbase.wal.WAL);\n  public static long extractFileNumFromWAL(org.apache.hadoop.hbase.wal.WAL);\n  public static boolean validateWALFilename(java.lang.String);\n  public static long getTimestamp(java.lang.String);\n  public static java.lang.String getWALDirectoryName(java.lang.String);\n  public static java.lang.String getWALArchiveDirectoryName(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public static org.apache.hadoop.hbase.ServerName getServerNameFromWALDirectoryName(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.ServerName getServerNameFromWALDirectoryName(org.apache.hadoop.fs.Path);\n  public static boolean isMetaFile(org.apache.hadoop.fs.Path);\n  public static boolean isMetaFile(java.lang.String);\n  public static boolean isArchivedLogFile(org.apache.hadoop.fs.Path);\n  public static org.apache.hadoop.fs.Path findArchivedLog(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WAL$Reader openReader(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public void addWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n  public static java.lang.String getWALPrefixFromWALName(java.lang.String);\n  public org.apache.hadoop.hbase.wal.WAL getWAL(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink$RecoveredEditsWriter.class;;;final class org.apache.hadoop.hbase.wal.AbstractRecoveredEditsOutputSink$RecoveredEditsWriter {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/AbstractRecoveredEditsOutputSink.class;;;abstract class org.apache.hadoop.hbase.wal.AbstractRecoveredEditsOutputSink extends org.apache.hadoop.hbase.wal.OutputSink {\n  public org.apache.hadoop.hbase.wal.AbstractRecoveredEditsOutputSink(org.apache.hadoop.hbase.wal.WALSplitter, org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.EntryBuffers, int);\n  public boolean keepRegionEvent(org.apache.hadoop.hbase.wal.WAL$Entry);\n}\n;;;No.;;;N;;;No, it is not a task definition, but rather a class definition for a component in the Apache HBase system.;;;N
org/apache/hadoop/hbase/wal/AbstractWALRoller$1.class;;;class org.apache.hadoop.hbase.wal.AbstractWALRoller$1 implements org.apache.hadoop.hbase.regionserver.wal.WALActionsListener {\n  public void logRollRequested(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener$RollRequestReason);\n  public void postLogArchive(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/AbstractWALRoller$RollController.class;;;public class org.apache.hadoop.hbase.wal.AbstractWALRoller$RollController {\n  public void requestRoll();\n  public java.util.Map<byte[], java.util.List<byte[]>> rollWal(long) throws java.io.IOException;\n  public boolean isRollRequested();\n  public boolean needsPeriodicRoll(long);\n  public boolean needsRoll(long);\n}\n;;;No.;;;N;;;No (it is a controller for managing WAL rolling in HBase).;;;N
org/apache/hadoop/hbase/wal/AbstractWALRoller.class;;;public abstract class org.apache.hadoop.hbase.wal.AbstractWALRoller<T extends org.apache.hadoop.hbase.Abortable> extends java.lang.Thread implements java.io.Closeable {\n  public static final java.lang.String WAL_ROLL_WAIT_TIMEOUT;\n  public static final long DEFAULT_WAL_ROLL_WAIT_TIMEOUT;\n  public void addWAL(org.apache.hadoop.hbase.wal.WAL);\n  public void requestRollAll();\n  public void run();\n  public boolean walRollFinished();\n  public void waitUntilWalRollFinished() throws java.lang.InterruptedException;\n  public void close();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/AsyncFSWALProvider$AsyncWriter.class;;;public interface org.apache.hadoop.hbase.wal.AsyncFSWALProvider$AsyncWriter extends org.apache.hadoop.hbase.wal.WALProvider$AsyncWriter {\n  public abstract void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, boolean, long, org.apache.hadoop.hbase.io.asyncfs.monitor.StreamSlowMonitor) throws java.io.IOException, org.apache.hadoop.hbase.util.CommonFSUtils$StreamLacksCapabilityException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/AsyncFSWALProvider.class;;;public class org.apache.hadoop.hbase.wal.AsyncFSWALProvider extends org.apache.hadoop.hbase.wal.AbstractFSWALProvider<org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL> {\n  public static final java.lang.String WRITER_IMPL;\n  public org.apache.hadoop.hbase.wal.AsyncFSWALProvider();\n  public static org.apache.hadoop.hbase.wal.AsyncFSWALProvider$AsyncWriter createAsyncWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, org.apache.hbase.thirdparty.io.netty.channel.EventLoopGroup, java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.Channel>) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.AsyncFSWALProvider$AsyncWriter createAsyncWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, long, org.apache.hbase.thirdparty.io.netty.channel.EventLoopGroup, java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.Channel>, org.apache.hadoop.hbase.io.asyncfs.monitor.StreamSlowMonitor) throws java.io.IOException;\n  public static boolean load();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/BoundedEntryBuffers.class;;;public class org.apache.hadoop.hbase.wal.BoundedEntryBuffers extends org.apache.hadoop.hbase.wal.EntryBuffers {\n  public org.apache.hadoop.hbase.wal.BoundedEntryBuffers(org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, long);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/BoundedGroupingStrategy.class;;;public class org.apache.hadoop.hbase.wal.BoundedGroupingStrategy implements org.apache.hadoop.hbase.wal.RegionGroupingProvider$RegionGroupingStrategy {\n  public org.apache.hadoop.hbase.wal.BoundedGroupingStrategy();\n  public java.lang.String group(byte[], byte[]);\n  public void init(org.apache.hadoop.conf.Configuration, java.lang.String);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/BoundedRecoveredEditsOutputSink.class;;;class org.apache.hadoop.hbase.wal.BoundedRecoveredEditsOutputSink extends org.apache.hadoop.hbase.wal.AbstractRecoveredEditsOutputSink {\n  public org.apache.hadoop.hbase.wal.BoundedRecoveredEditsOutputSink(org.apache.hadoop.hbase.wal.WALSplitter, org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.EntryBuffers, int);\n  public void append(org.apache.hadoop.hbase.wal.EntryBuffers$RegionEntryBuffer) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.Path> close() throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.Long> getOutputCounts();\n  public int getNumberOfRecoveredRegions();\n}\n;;;No. This class is not a message definition but rather a class definition for a specific module or component in the Apache HBase library. It may be used in the implementation of message handling, but it is not a standalone message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/BoundedRecoveredHFilesOutputSink.class;;;public class org.apache.hadoop.hbase.wal.BoundedRecoveredHFilesOutputSink extends org.apache.hadoop.hbase.wal.OutputSink {\n  public org.apache.hadoop.hbase.wal.BoundedRecoveredHFilesOutputSink(org.apache.hadoop.hbase.wal.WALSplitter, org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.EntryBuffers, int);\n  public java.util.List<org.apache.hadoop.fs.Path> close() throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.Long> getOutputCounts();\n  public int getNumberOfRecoveredRegions();\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/wal/DisabledWALProvider$DisabledWAL.class;;;class org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL implements org.apache.hadoop.hbase.wal.WAL {\n  public org.apache.hadoop.hbase.wal.DisabledWALProvider$DisabledWAL(org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, java.util.List<org.apache.hadoop.hbase.regionserver.wal.WALActionsListener>);\n  public void registerWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n  public boolean unregisterWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n  public java.util.Map<byte[], java.util.List<byte[]>> rollWriter();\n  public java.util.Map<byte[], java.util.List<byte[]>> rollWriter(boolean);\n  public void shutdown();\n  public void close();\n  public long appendData(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKeyImpl, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public long appendMarker(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKeyImpl, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public void updateStore(byte[], byte[], java.lang.Long, boolean);\n  public void sync();\n  public void sync(long);\n  public java.lang.Long startCacheFlush(byte[], java.util.Map<byte[], java.lang.Long>);\n  public java.lang.Long startCacheFlush(byte[], java.util.Set<byte[]>);\n  public void completeCacheFlush(byte[], long);\n  public void abortCacheFlush(byte[]);\n  public org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost getCoprocessorHost();\n  public long getEarliestMemStoreSeqNum(byte[]);\n  public long getEarliestMemStoreSeqNum(byte[], byte[]);\n  public java.lang.String toString();\n  public java.util.OptionalLong getLogFileSizeIfBeingWritten(org.apache.hadoop.fs.Path);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/DisabledWALProvider.class;;;class org.apache.hadoop.hbase.wal.DisabledWALProvider implements org.apache.hadoop.hbase.wal.WALProvider {\n  public void init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.Abortable) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.wal.WAL> getWALs();\n  public org.apache.hadoop.hbase.wal.WAL getWAL(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void shutdown() throws java.io.IOException;\n  public long getNumLogFiles();\n  public long getLogFileSize();\n  public void addWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/EntryBuffers$RegionEntryBuffer.class;;;class org.apache.hadoop.hbase.wal.EntryBuffers$RegionEntryBuffer implements org.apache.hadoop.hbase.io.HeapSize {\n  public long heapSize();\n  public byte[] getEncodedRegionName();\n  public org.apache.hadoop.hbase.TableName getTableName();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/EntryBuffers.class;;;class org.apache.hadoop.hbase.wal.EntryBuffers {\n  public org.apache.hadoop.hbase.wal.EntryBuffers(org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, long);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a constructor for a class and not a piece of work that needs to be executed asynchronously.;;;N
org/apache/hadoop/hbase/wal/FSHLogProvider$Writer.class;;;public interface org.apache.hadoop.hbase.wal.FSHLogProvider$Writer extends org.apache.hadoop.hbase.wal.WALProvider$Writer {\n  public abstract void init(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration, boolean, long, org.apache.hadoop.hbase.io.asyncfs.monitor.StreamSlowMonitor) throws java.io.IOException, org.apache.hadoop.hbase.util.CommonFSUtils$StreamLacksCapabilityException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/FSHLogProvider.class;;;public class org.apache.hadoop.hbase.wal.FSHLogProvider extends org.apache.hadoop.hbase.wal.AbstractFSWALProvider<org.apache.hadoop.hbase.regionserver.wal.FSHLog> {\n  public org.apache.hadoop.hbase.wal.FSHLogProvider();\n  public static org.apache.hadoop.hbase.wal.FSHLogProvider$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.FSHLogProvider$Writer createWriter(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, boolean, long) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class definition for a provider of a WAL (Write Ahead Log) implementation used in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/NamespaceGroupingStrategy.class;;;public class org.apache.hadoop.hbase.wal.NamespaceGroupingStrategy implements org.apache.hadoop.hbase.wal.RegionGroupingProvider$RegionGroupingStrategy {\n  public org.apache.hadoop.hbase.wal.NamespaceGroupingStrategy();\n  public java.lang.String group(byte[], byte[]);\n  public void init(org.apache.hadoop.conf.Configuration, java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/NettyAsyncFSWALConfigHelper.class;;;public final class org.apache.hadoop.hbase.wal.NettyAsyncFSWALConfigHelper {\n  public static void setEventLoopConfig(org.apache.hadoop.conf.Configuration, org.apache.hbase.thirdparty.io.netty.channel.EventLoopGroup, java.lang.Class<? extends org.apache.hbase.thirdparty.io.netty.channel.Channel>);\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a helper class for configuring a NettyAsyncFSWAL (write-ahead log) in Apache HBase.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a helper class with a static method for configuring NettyAsyncFSWAL.;;;N
org/apache/hadoop/hbase/wal/OutputSink$WriterThread.class;;;public class org.apache.hadoop.hbase.wal.OutputSink$WriterThread extends java.lang.Thread {\n  public void run();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/OutputSink.class;;;abstract class org.apache.hadoop.hbase.wal.OutputSink {\n  public org.apache.hadoop.hbase.wal.OutputSink(org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.EntryBuffers, int);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/RecoveredEditsOutputSink.class;;;class org.apache.hadoop.hbase.wal.RecoveredEditsOutputSink extends org.apache.hadoop.hbase.wal.AbstractRecoveredEditsOutputSink {\n  public org.apache.hadoop.hbase.wal.RecoveredEditsOutputSink(org.apache.hadoop.hbase.wal.WALSplitter, org.apache.hadoop.hbase.wal.WALSplitter$PipelineController, org.apache.hadoop.hbase.wal.EntryBuffers, int);\n  public void append(org.apache.hadoop.hbase.wal.EntryBuffers$RegionEntryBuffer) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.fs.Path> close() throws java.io.IOException;\n  public java.util.Map<java.lang.String, java.lang.Long> getOutputCounts();\n  public int getNumberOfRecoveredRegions();\n}\n;;;No. This is a class definition for a Java programming language class, which describes the implementation and behavior of an object. It is not a message definition that contains data or instructions meant to be transferred between systems via a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/RegionGroupingProvider$IdentityGroupingStrategy.class;;;class org.apache.hadoop.hbase.wal.RegionGroupingProvider$IdentityGroupingStrategy implements org.apache.hadoop.hbase.wal.RegionGroupingProvider$RegionGroupingStrategy {\n  public void init(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public java.lang.String group(byte[], byte[]);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/RegionGroupingProvider$RegionGroupingStrategy.class;;;public interface org.apache.hadoop.hbase.wal.RegionGroupingProvider$RegionGroupingStrategy {\n  public static final java.lang.String GROUP_NAME_DELIMITER;\n  public abstract java.lang.String group(byte[], byte[]);\n  public abstract void init(org.apache.hadoop.conf.Configuration, java.lang.String);\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is an interface for defining a region grouping strategy in Apache HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/RegionGroupingProvider$Strategies.class;;;final class org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies extends java.lang.Enum<org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies> {\n  public static final org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies defaultStrategy;\n  public static final org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies identity;\n  public static final org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies bounded;\n  public static final org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies namespace;\n  public static org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies[] values();\n  public static org.apache.hadoop.hbase.wal.RegionGroupingProvider$Strategies valueOf(java.lang.String);\n}\n;;;No. This is not a message definition, it is a Java class definition. It defines a set of static fields and methods for the RegionGroupingProvider$Strategies enum. It does not define any data or message structure to be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/RegionGroupingProvider.class;;;public class org.apache.hadoop.hbase.wal.RegionGroupingProvider implements org.apache.hadoop.hbase.wal.WALProvider {\n  public static final java.lang.String REGION_GROUPING_STRATEGY;\n  public static final java.lang.String DEFAULT_REGION_GROUPING_STRATEGY;\n  public static final java.lang.String DELEGATE_PROVIDER;\n  public static final java.lang.String DEFAULT_DELEGATE_PROVIDER;\n  public org.apache.hadoop.hbase.wal.RegionGroupingProvider();\n  public void init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.Abortable) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.wal.WAL> getWALs();\n  public org.apache.hadoop.hbase.wal.WAL getWAL(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public void shutdown() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getNumLogFiles();\n  public long getLogFileSize();\n  public void addWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/SyncReplicationWALProvider$1.class;;;class org.apache.hadoop.hbase.wal.SyncReplicationWALProvider$1 {\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/SyncReplicationWALProvider$DefaultSyncReplicationPeerInfoProvider.class;;;class org.apache.hadoop.hbase.wal.SyncReplicationWALProvider$DefaultSyncReplicationPeerInfoProvider implements org.apache.hadoop.hbase.replication.regionserver.SyncReplicationPeerInfoProvider {\n  public java.util.Optional<org.apache.hadoop.hbase.util.Pair<java.lang.String, java.lang.String>> getPeerIdAndRemoteWALDir(org.apache.hadoop.hbase.TableName);\n  public boolean checkState(org.apache.hadoop.hbase.TableName, java.util.function.BiPredicate<org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState>);\n}\n;;;No. It is a class definition implementing an interface, but it is not a message definition suitable for a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/wal/SyncReplicationWALProvider.class;;;public class org.apache.hadoop.hbase.wal.SyncReplicationWALProvider implements org.apache.hadoop.hbase.wal.WALProvider,org.apache.hadoop.hbase.replication.regionserver.PeerActionListener {\n  public static final java.lang.String DUAL_WAL_IMPL;\n  public void setPeerInfoProvider(org.apache.hadoop.hbase.replication.regionserver.SyncReplicationPeerInfoProvider);\n  public void init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.Abortable) throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WAL getWAL(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.wal.WAL> getWALs();\n  public void shutdown() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public long getNumLogFiles();\n  public long getLogFileSize();\n  public void addWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n  public void peerSyncReplicationStateChange(java.lang.String, org.apache.hadoop.hbase.replication.SyncReplicationState, org.apache.hadoop.hbase.replication.SyncReplicationState, int);\n  public static java.util.Optional<java.lang.String> getSyncReplicationPeerIdFromWALName(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WAL$Entry.class;;;public class org.apache.hadoop.hbase.wal.WAL$Entry {\n  public org.apache.hadoop.hbase.wal.WAL$Entry();\n  public org.apache.hadoop.hbase.wal.WAL$Entry(org.apache.hadoop.hbase.wal.WALKeyImpl, org.apache.hadoop.hbase.wal.WALEdit);\n  public org.apache.hadoop.hbase.wal.WALEdit getEdit();\n  public org.apache.hadoop.hbase.wal.WALKeyImpl getKey();\n  public java.lang.String toString();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WAL$Reader.class;;;public interface org.apache.hadoop.hbase.wal.WAL$Reader extends java.io.Closeable {\n  public abstract org.apache.hadoop.hbase.wal.WAL$Entry next() throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.wal.WAL$Entry next(org.apache.hadoop.hbase.wal.WAL$Entry) throws java.io.IOException;\n  public abstract void seek(long) throws java.io.IOException;\n  public abstract long getPosition() throws java.io.IOException;\n  public abstract void reset() throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WAL.class;;;public interface org.apache.hadoop.hbase.wal.WAL extends java.io.Closeable,org.apache.hadoop.hbase.replication.regionserver.WALFileLengthProvider {\n  public abstract void registerWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n  public abstract boolean unregisterWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n  public abstract java.util.Map<byte[], java.util.List<byte[]>> rollWriter() throws org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException, java.io.IOException;\n  public abstract java.util.Map<byte[], java.util.List<byte[]>> rollWriter(boolean) throws java.io.IOException;\n  public abstract void shutdown() throws java.io.IOException;\n  public abstract void close() throws java.io.IOException;\n  public abstract long appendData(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKeyImpl, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public abstract long appendMarker(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.wal.WALKeyImpl, org.apache.hadoop.hbase.wal.WALEdit) throws java.io.IOException;\n  public abstract void updateStore(byte[], byte[], java.lang.Long, boolean);\n  public abstract void sync() throws java.io.IOException;\n  public abstract void sync(long) throws java.io.IOException;\n  public default void sync(boolean) throws java.io.IOException;\n  public default void sync(long, boolean) throws java.io.IOException;\n  public abstract java.lang.Long startCacheFlush(byte[], java.util.Set<byte[]>);\n  public abstract java.lang.Long startCacheFlush(byte[], java.util.Map<byte[], java.lang.Long>);\n  public abstract void completeCacheFlush(byte[], long);\n  public abstract void abortCacheFlush(byte[]);\n  public abstract org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost getCoprocessorHost();\n  public abstract long getEarliestMemStoreSeqNum(byte[]);\n  public abstract long getEarliestMemStoreSeqNum(byte[], byte[]);\n  public abstract java.lang.String toString();\n}\n;;;Yes, this class might be put on a message queue as a message definition.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WALEdit.class;;;public class org.apache.hadoop.hbase.wal.WALEdit implements org.apache.hadoop.hbase.io.HeapSize {\n  public static final byte[] METAFAMILY;\n  public static final byte[] METAROW;\n  public static final byte[] COMPACTION;\n  public static final byte[] FLUSH;\n  public static final byte[] REGION_EVENT;\n  public static final byte[] BULK_LOAD;\n  public org.apache.hadoop.hbase.wal.WALEdit();\n  public org.apache.hadoop.hbase.wal.WALEdit(boolean);\n  public org.apache.hadoop.hbase.wal.WALEdit(int);\n  public org.apache.hadoop.hbase.wal.WALEdit(int, boolean);\n  public org.apache.hadoop.hbase.wal.WALEdit(org.apache.hadoop.hbase.wal.WALEdit);\n  public java.util.Set<byte[]> getFamilies();\n  public static boolean isMetaEditFamily(byte[]);\n  public static boolean isMetaEditFamily(org.apache.hadoop.hbase.Cell);\n  public boolean isMetaEdit();\n  public boolean isReplay();\n  public org.apache.hadoop.hbase.wal.WALEdit add(org.apache.hadoop.hbase.Cell, byte[]);\n  public org.apache.hadoop.hbase.wal.WALEdit add(org.apache.hadoop.hbase.Cell);\n  public org.apache.hadoop.hbase.wal.WALEdit add(java.util.List<org.apache.hadoop.hbase.Cell>);\n  public boolean isEmpty();\n  public int size();\n  public java.util.ArrayList<org.apache.hadoop.hbase.Cell> getCells();\n  public void setCells(java.util.ArrayList<org.apache.hadoop.hbase.Cell>);\n  public int readFromCells(org.apache.hadoop.hbase.codec.Codec$Decoder, int) throws java.io.IOException;\n  public long heapSize();\n  public long estimatedSerializedSizeOf();\n  public java.lang.String toString();\n  public static org.apache.hadoop.hbase.wal.WALEdit createFlushWALEdit(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$FlushDescriptor);\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$FlushDescriptor getFlushDescriptor(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WALEdit createRegionEventWALEdit(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$RegionEventDescriptor);\n  public static org.apache.hadoop.hbase.wal.WALEdit createRegionEventWALEdit(byte[], org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$RegionEventDescriptor);\n  public static byte[] createRegionEventDescriptorQualifier(org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$RegionEventDescriptor$EventType);\n  public boolean isRegionCloseMarker();\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$RegionEventDescriptor getRegionEventDescriptor(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WALEdit createCompaction(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$CompactionDescriptor);\n  public static byte[] getRowForRegion(org.apache.hadoop.hbase.client.RegionInfo);\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$CompactionDescriptor getCompaction(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public static boolean isCompactionMarker(org.apache.hadoop.hbase.Cell);\n  public static org.apache.hadoop.hbase.wal.WALEdit createBulkLoadEvent(org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$BulkLoadDescriptor);\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$BulkLoadDescriptor getBulkLoadDescriptor(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n  public void add(java.util.Map<byte[], java.util.List<org.apache.hadoop.hbase.Cell>>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WALFactory$Providers.class;;;final class org.apache.hadoop.hbase.wal.WALFactory$Providers extends java.lang.Enum<org.apache.hadoop.hbase.wal.WALFactory$Providers> {\n  public static final org.apache.hadoop.hbase.wal.WALFactory$Providers defaultProvider;\n  public static final org.apache.hadoop.hbase.wal.WALFactory$Providers filesystem;\n  public static final org.apache.hadoop.hbase.wal.WALFactory$Providers multiwal;\n  public static final org.apache.hadoop.hbase.wal.WALFactory$Providers asyncfs;\n  public static org.apache.hadoop.hbase.wal.WALFactory$Providers[] values();\n  public static org.apache.hadoop.hbase.wal.WALFactory$Providers valueOf(java.lang.String);\n}\n;;;No, this class is not a message definition. It defines a set of options, but it is not describing the content or format of a message that could be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WALFactory.class;;;public class org.apache.hadoop.hbase.wal.WALFactory {\n  public static final java.lang.String WAL_PROVIDER;\n  public static final java.lang.String META_WAL_PROVIDER;\n  public static final java.lang.String WAL_ENABLED;\n  public java.lang.Class<? extends org.apache.hadoop.hbase.wal.WALProvider> getProviderClass(java.lang.String, java.lang.String);\n  public org.apache.hadoop.hbase.wal.WALFactory(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WALFactory(org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.Abortable, boolean) throws java.io.IOException;\n  public void close() throws java.io.IOException;\n  public void shutdown() throws java.io.IOException;\n  public java.util.List<org.apache.hadoop.hbase.wal.WAL> getWALs();\n  public org.apache.hadoop.hbase.wal.WALProvider getMetaProvider() throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WAL getWAL(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WAL$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WAL$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.CancelableProgressable) throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WAL$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.util.CancelableProgressable, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WALProvider$Writer createWALWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.hbase.wal.WALProvider$Writer createRecoveredEditsWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WALFactory getInstance(org.apache.hadoop.conf.Configuration);\n  public static org.apache.hadoop.hbase.wal.WAL$Reader createReader(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WAL$Reader createReaderIgnoreCustomClass(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.wal.WALProvider$Writer createWALWriter(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public final org.apache.hadoop.hbase.wal.WALProvider getWALProvider();\n  public final org.apache.hadoop.hbase.wal.WALProvider getMetaWALProvider();\n  public org.apache.hadoop.hbase.io.asyncfs.monitor.ExcludeDatanodeManager getExcludeDatanodeManager();\n}\n;;;No, this class is not a message definition that might be put on a message queue. It appears to be a factory class for managing data in the Hadoop Distributed File System.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WALKey.class;;;public interface org.apache.hadoop.hbase.wal.WALKey extends org.apache.hadoop.hbase.regionserver.SequenceId, java.lang.Comparable<org.apache.hadoop.hbase.wal.WALKey> {\n  public static final java.util.List<java.util.UUID> EMPTY_UUIDS;\n  public default long estimatedSerializedSizeOf();\n  public abstract byte[] getEncodedRegionName();\n  public abstract org.apache.hadoop.hbase.TableName getTableName();\n  public abstract long getWriteTime();\n  public default long getNonceGroup();\n  public default long getNonce();\n  public abstract java.util.UUID getOriginatingClusterId();\n  public abstract long getOrigLogSeqNum();\n  public abstract void addExtendedAttribute(java.lang.String, byte[]);\n  public default byte[] getExtendedAttribute(java.lang.String);\n  public default java.util.Map<java.lang.String, byte[]> getExtendedAttributes();\n  public default java.util.Map<java.lang.String, java.lang.Object> toStringMap();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WALKeyImpl.class;;;public class org.apache.hadoop.hbase.wal.WALKeyImpl implements org.apache.hadoop.hbase.wal.WALKey {\n  public static final org.apache.hadoop.hbase.wal.WALKeyImpl EMPTY_WALKEYIMPL;\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl getMvcc();\n  public org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry getWriteEntry();\n  public void setWriteEntry(org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl$WriteEntry);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl();\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(java.util.NavigableMap<byte[], java.lang.Integer>);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.UUID);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.UUID, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, java.util.NavigableMap<byte[], java.lang.Integer>);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, java.util.NavigableMap<byte[], java.lang.Integer>);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, java.util.NavigableMap<byte[], java.lang.Integer>, java.util.Map<java.lang.String, byte[]>);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(org.apache.hadoop.hbase.wal.WALKeyImpl, java.util.Map<java.lang.String, byte[]>);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(org.apache.hadoop.hbase.wal.WALKey, java.util.List<java.util.UUID>, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, java.util.NavigableMap<byte[], java.lang.Integer>, java.util.Map<java.lang.String, byte[]>);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.List<java.util.UUID>, long, long, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, java.util.NavigableMap<byte[], java.lang.Integer>);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, long, java.util.List<java.util.UUID>, long, long, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, java.util.List<java.util.UUID>, long, long, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, java.util.List<java.util.UUID>, long, long, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, java.util.NavigableMap<byte[], java.lang.Integer>);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, long, long, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl);\n  public org.apache.hadoop.hbase.wal.WALKeyImpl(byte[], org.apache.hadoop.hbase.TableName, long, java.util.List<java.util.UUID>, long, long, org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl, java.util.NavigableMap<byte[], java.lang.Integer>, java.util.Map<java.lang.String, byte[]>);\n  public byte[] getEncodedRegionName();\n  public org.apache.hadoop.hbase.TableName getTableName();\n  public void setOrigLogSeqNum(long);\n  public long getOrigLogSeqNum();\n  public long getSequenceId();\n  public long getWriteTime();\n  public java.util.NavigableMap<byte[], java.lang.Integer> getReplicationScopes();\n  public long getNonceGroup();\n  public long getNonce();\n  public void clearReplicationScope();\n  public void addClusterId(java.util.UUID);\n  public java.util.List<java.util.UUID> getClusterIds();\n  public java.util.UUID getOriginatingClusterId();\n  public void addExtendedAttribute(java.lang.String, byte[]);\n  public byte[] getExtendedAttribute(java.lang.String);\n  public java.util.Map<java.lang.String, byte[]> getExtendedAttributes();\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public int compareTo(org.apache.hadoop.hbase.wal.WALKey);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$WALKey$Builder getBuilder(org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringCompressor) throws java.io.IOException;\n  public void readFieldsFromPb(org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos$WALKey, org.apache.hadoop.hbase.regionserver.wal.WALCellCodec$ByteStringUncompressor) throws java.io.IOException;\n  public long estimatedSerializedSizeOf();\n  public int compareTo(java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WALPrettyPrinter.class;;;public class org.apache.hadoop.hbase.wal.WALPrettyPrinter {\n  public org.apache.hadoop.hbase.wal.WALPrettyPrinter();\n  public void enableValues();\n  public void disableValues();\n  public void enableJSON();\n  public void disableJSON();\n  public void setSequenceFilter(long);\n  public void setTableFilter(java.lang.String);\n  public void setRegionFilter(java.lang.String);\n  public void setRowFilter(java.lang.String);\n  public void setRowPrefixFilter(java.lang.String);\n  public void setOutputOnlyRowKey();\n  public void setPosition(long);\n  public void beginPersistentOutput();\n  public void endPersistentOutput();\n  public void processFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void printCell(java.io.PrintStream, java.util.Map<java.lang.String, java.lang.Object>, boolean, boolean);\n  public static java.util.Map<java.lang.String, java.lang.Object> toStringMap(org.apache.hadoop.hbase.Cell, boolean, java.lang.String, java.lang.String, boolean);\n  public static java.util.Map<java.lang.String, java.lang.Object> toStringMap(org.apache.hadoop.hbase.Cell);\n  public static void main(java.lang.String[]) throws java.io.IOException;\n  public static void run(java.lang.String[]) throws java.io.IOException;\n}\n;;;No. The class org.apache.hadoop.hbase.wal.WALPrettyPrinter contains methods for pretty-printing WAL (Write-Ahead Log) files. While it may be used to interact with a message queue indirectly (e.g., as part of a larger HBase or Hadoop ecosystem), it does not define a message itself.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WALProvider$AsyncWriter.class;;;public interface org.apache.hadoop.hbase.wal.WALProvider$AsyncWriter extends org.apache.hadoop.hbase.wal.WALProvider$WriterBase {\n  public abstract java.util.concurrent.CompletableFuture<java.lang.Long> sync(boolean);\n  public abstract void append(org.apache.hadoop.hbase.wal.WAL$Entry);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WALProvider$Writer.class;;;public interface org.apache.hadoop.hbase.wal.WALProvider$Writer extends org.apache.hadoop.hbase.wal.WALProvider$WriterBase {\n  public abstract void sync(boolean) throws java.io.IOException;\n  public abstract void append(org.apache.hadoop.hbase.wal.WAL$Entry) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WALProvider$WriterBase.class;;;public interface org.apache.hadoop.hbase.wal.WALProvider$WriterBase extends java.io.Closeable {\n  public abstract long getLength();\n  public abstract long getSyncedLength();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WALProvider.class;;;public interface org.apache.hadoop.hbase.wal.WALProvider {\n  public abstract void init(org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.conf.Configuration, java.lang.String, org.apache.hadoop.hbase.Abortable) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.wal.WAL getWAL(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public abstract java.util.List<org.apache.hadoop.hbase.wal.WAL> getWALs();\n  public abstract void shutdown() throws java.io.IOException;\n  public abstract void close() throws java.io.IOException;\n  public abstract long getNumLogFiles();\n  public abstract long getLogFileSize();\n  public abstract void addWALActionsListener(org.apache.hadoop.hbase.regionserver.wal.WALActionsListener);\n  public default org.apache.hadoop.hbase.replication.regionserver.WALFileLengthProvider getWALFileLengthProvider();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WALSplitUtil$1.class;;;final class org.apache.hadoop.hbase.wal.WALSplitUtil$1 implements org.apache.hadoop.fs.PathFilter {\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WALSplitUtil$MutationReplay.class;;;public class org.apache.hadoop.hbase.wal.WALSplitUtil$MutationReplay implements java.lang.Comparable<org.apache.hadoop.hbase.wal.WALSplitUtil$MutationReplay> {\n  public final org.apache.hadoop.hbase.client.Mutation mutation;\n  public final long nonceGroup;\n  public final long nonce;\n  public org.apache.hadoop.hbase.wal.WALSplitUtil$MutationReplay(org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$MutationType, org.apache.hadoop.hbase.client.Mutation, long, long);\n  public int compareTo(org.apache.hadoop.hbase.wal.WALSplitUtil$MutationReplay);\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$MutationProto$MutationType getType();\n  public int compareTo(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WALSplitUtil.class;;;public final class org.apache.hadoop.hbase.wal.WALSplitUtil {\n  public static void finishSplitLogFile(java.lang.String, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static void moveWAL(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path getRegionDirRecoveredEditsDir(org.apache.hadoop.fs.Path);\n  public static boolean hasRecoveredEdits(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static long getMaxRegionSequenceId(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.util.ConcurrentMapUtils$IOExceptionSupplier<org.apache.hadoop.fs.FileSystem>, org.apache.hadoop.hbase.util.ConcurrentMapUtils$IOExceptionSupplier<org.apache.hadoop.fs.FileSystem>) throws java.io.IOException;\n  public static java.util.NavigableSet<org.apache.hadoop.fs.Path> getSplitEditFilesSorted(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static org.apache.hadoop.fs.Path moveAsideBadEditsFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static boolean isSequenceIdFile(org.apache.hadoop.fs.Path);\n  public static long getMaxRegionSequenceId(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void writeRegionSequenceIdFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, long) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.wal.WALSplitUtil$MutationReplay> getMutationsFromWALEntry(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WALEntry, org.apache.hadoop.hbase.CellScanner, org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.wal.WALKey, org.apache.hadoop.hbase.wal.WALEdit>, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n  public static org.apache.hadoop.fs.FileStatus[] getRecoveredHFiles(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, java.lang.String) throws java.io.IOException;\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a class with various static utility methods related to the HBase Write-Ahead Log (WAL) split process.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WALSplitter$1.class;;;class org.apache.hadoop.hbase.wal.WALSplitter$1 {\n}\n;;;No.;;;N;;;No;;;N
org/apache/hadoop/hbase/wal/WALSplitter$CorruptedLogFileException.class;;;class org.apache.hadoop.hbase.wal.WALSplitter$CorruptedLogFileException extends java.lang.Exception {\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WALSplitter$PipelineController.class;;;public class org.apache.hadoop.hbase.wal.WALSplitter$PipelineController {\n  public org.apache.hadoop.hbase.wal.WALSplitter$PipelineController();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/wal/WALSplitter$SplitWALResult.class;;;final class org.apache.hadoop.hbase.wal.WALSplitter$SplitWALResult {\n  public boolean isFinished();\n  public boolean isCorrupt();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/wal/WALSplitter.class;;;public class org.apache.hadoop.hbase.wal.WALSplitter {\n  public static final java.lang.String SPLIT_SKIP_ERRORS_KEY;\n  public static final boolean SPLIT_SKIP_ERRORS_DEFAULT;\n  public static final java.lang.String WAL_SPLIT_TO_HFILE;\n  public static final boolean DEFAULT_WAL_SPLIT_TO_HFILE;\n  public static final java.lang.String SPLIT_WRITER_CREATION_BOUNDED;\n  public static final java.lang.String SPLIT_WAL_BUFFER_SIZE;\n  public static final java.lang.String SPLIT_WAL_WRITER_THREADS;\n  public static boolean splitLogFile(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileStatus, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.util.CancelableProgressable, org.apache.hadoop.hbase.regionserver.LastSequenceId, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination, org.apache.hadoop.hbase.wal.WALFactory, org.apache.hadoop.hbase.regionserver.RegionServerServices) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.fs.Path> split(org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.wal.WALFactory) throws java.io.IOException;\n}\n;;;No. This class contains static methods and constants related to splitting WAL files in HBase, but it does not define a message format.;;;N;;;No.;;;N
