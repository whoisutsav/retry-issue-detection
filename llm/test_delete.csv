org/apache/hadoop/hbase/CoordinatedStateManager.class;;;public interface org.apache.hadoop.hbase.CoordinatedStateManager {\n  public abstract org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination getSplitLogWorkerCoordination();\n  public abstract org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination getSplitLogManagerCoordination();\n}\n;;;No.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/ExecutorStatusChore.class;;;public class org.apache.hadoop.hbase.ExecutorStatusChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public static final java.lang.String WAKE_FREQ;\n  public static final int DEFAULT_WAKE_FREQ;\n  public org.apache.hadoop.hbase.ExecutorStatusChore(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.hbase.executor.ExecutorService, org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource);\n  protected void chore();\n  public org.apache.hadoop.hbase.util.Pair<java.lang.Long, java.lang.Long> getExecutorStatus(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/HBaseRpcServicesBase.class;;;public abstract class org.apache.hadoop.hbase.HBaseRpcServicesBase<S extends org.apache.hadoop.hbase.HBaseServerBase<?>> implements org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$ClientMetaService$BlockingInterface, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$AdminService$BlockingInterface, org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler, org.apache.hadoop.hbase.ipc.PriorityFunction, org.apache.hadoop.hbase.conf.ConfigurationObserver {\n  public static final java.lang.String CLIENT_BOOTSTRAP_NODE_LIMIT;\n  public static final int DEFAULT_CLIENT_BOOTSTRAP_NODE_LIMIT;\n  protected final S server;\n  protected final org.apache.hadoop.hbase.ipc.RpcServer rpcServer;\n  protected final org.apache.hadoop.hbase.ipc.PriorityFunction priority;\n  protected org.apache.hadoop.hbase.HBaseRpcServicesBase(S, java.lang.String) throws java.io.IOException;\n  protected abstract boolean defaultReservoirEnabled();\n  protected abstract org.apache.hadoop.hbase.util.DNS$ServerType getDNSServerType();\n  protected abstract java.lang.String getHostname(org.apache.hadoop.conf.Configuration, java.lang.String);\n  protected abstract java.lang.String getPortConfigName();\n  protected abstract int getDefaultPort();\n  protected abstract org.apache.hadoop.hbase.ipc.PriorityFunction createPriority();\n  protected abstract java.lang.Class<?> getRpcSchedulerFactoryClass(org.apache.hadoop.conf.Configuration);\n  protected abstract java.util.List<org.apache.hadoop.hbase.ipc.RpcServer$BlockingServiceAndInterface> getServices();\n  protected final void internalStart(org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n  protected final void requirePermission(java.lang.String, org.apache.hadoop.hbase.security.access.Permission$Action) throws java.io.IOException;\n  public org.apache.hadoop.hbase.security.access.AccessChecker getAccessChecker();\n  public org.apache.hadoop.hbase.security.access.ZKPermissionWatcher getZkPermissionWatcher();\n  protected final void internalStop();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public S getServer();\n  public java.net.InetSocketAddress getSocketAddress();\n  public org.apache.hadoop.hbase.ipc.RpcServerInterface getRpcServer();\n  public org.apache.hadoop.hbase.ipc.RpcScheduler getRpcScheduler();\n  public int getPriority(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hadoop.hbase.security.User);\n  public long getDeadline(org.apache.hadoop.hbase.shaded.protobuf.generated.RPCProtos$RequestHeader, org.apache.hbase.thirdparty.com.google.protobuf.Message);\n  public boolean checkOOME(java.lang.Throwable);\n  public void onConfigurationChange(org.apache.hadoop.conf.Configuration);\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetClusterIdResponse getClusterId(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetClusterIdRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetActiveMasterResponse getActiveMaster(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetActiveMasterRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetMastersResponse getMasters(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetMastersRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetMetaRegionLocationsResponse getMetaRegionLocations(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetMetaRegionLocationsRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public final org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetBootstrapNodesResponse getBootstrapNodes(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.RegistryProtos$GetBootstrapNodesRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateConfigurationResponse updateConfiguration(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateConfigurationRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearSlowLogResponses clearSlowLogsResponses(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearSlowLogResponseRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n  public org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$LogEntry getLogEntries(org.apache.hbase.thirdparty.com.google.protobuf.RpcController, org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$LogRequest) throws org.apache.hbase.thirdparty.com.google.protobuf.ServiceException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/HBaseServerBase.class;;;public abstract class org.apache.hadoop.hbase.HBaseServerBase<R extends org.apache.hadoop.hbase.HBaseRpcServicesBase<?>> extends java.lang.Thread implements org.apache.hadoop.hbase.Server, org.apache.hadoop.hbase.conf.ConfigurationObserver, org.apache.hadoop.hbase.client.ConnectionRegistryEndpoint {\n  protected final org.apache.hadoop.conf.Configuration conf;\n  protected final java.util.concurrent.atomic.AtomicBoolean abortRequested;\n  protected volatile boolean stopped;\n  protected final long startcode;\n  protected final org.apache.hadoop.hbase.security.UserProvider userProvider;\n  protected final org.apache.hadoop.hbase.zookeeper.ZKWatcher zooKeeper;\n  protected org.apache.hadoop.hbase.ServerName serverName;\n  protected final R rpcServices;\n  protected final java.lang.String useThisHostnameInstead;\n  protected final org.apache.hadoop.hbase.namequeues.NamedQueueRecorder namedQueueRecorder;\n  protected final org.apache.hadoop.hbase.conf.ConfigurationManager configurationManager;\n  protected final org.apache.hadoop.hbase.ChoreService choreService;\n  protected final org.apache.hadoop.hbase.executor.ExecutorService executorService;\n  protected final org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker clusterStatusTracker;\n  protected final org.apache.hadoop.hbase.CoordinatedStateManager csm;\n  protected org.apache.hadoop.hbase.http.InfoServer infoServer;\n  protected org.apache.hadoop.hbase.fs.HFileSystem dataFs;\n  protected org.apache.hadoop.hbase.fs.HFileSystem walFs;\n  protected org.apache.hadoop.fs.Path dataRootDir;\n  protected org.apache.hadoop.fs.Path walRootDir;\n  protected final int msgInterval;\n  protected final org.apache.hadoop.hbase.util.Sleeper sleeper;\n  protected org.apache.hadoop.hbase.TableDescriptors tableDescriptors;\n  protected org.apache.hadoop.hbase.client.AsyncClusterConnection asyncClusterConnection;\n  protected final org.apache.hadoop.hbase.MetaRegionLocationCache metaRegionLocationCache;\n  protected final org.apache.hadoop.hbase.util.NettyEventLoopGroupConfig eventLoopGroupConfig;\n  protected final synchronized void setupClusterConnection() throws java.io.IOException;\n  protected final void initializeFileSystem() throws java.io.IOException;\n  public org.apache.hadoop.hbase.HBaseServerBase(org.apache.hadoop.conf.Configuration, java.lang.String) throws java.io.IOException;\n  protected final boolean setAbortRequested();\n  public boolean isStopped();\n  public boolean isAborted();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.client.AsyncClusterConnection getAsyncClusterConnection();\n  public org.apache.hadoop.hbase.zookeeper.ZKWatcher getZooKeeper();\n  protected final void shutdownChore(org.apache.hadoop.hbase.ScheduledChore);\n  protected final void initializeMemStoreChunkCreator(org.apache.hadoop.hbase.regionserver.HeapMemoryManager);\n  protected abstract void stopChores();\n  protected final void stopChoreService();\n  protected final void stopExecutorService();\n  protected final void closeClusterConnection();\n  protected final void stopInfoServer();\n  protected final void closeZooKeeper();\n  protected final void closeTableDescriptors();\n  protected final void installShutdownHook();\n  public boolean isShutdownHookInstalled();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public org.apache.hadoop.hbase.ChoreService getChoreService();\n  public org.apache.hadoop.hbase.TableDescriptors getTableDescriptors();\n  public org.apache.hadoop.hbase.executor.ExecutorService getExecutorService();\n  public org.apache.hadoop.hbase.security.access.AccessChecker getAccessChecker();\n  public org.apache.hadoop.hbase.security.access.ZKPermissionWatcher getZKPermissionWatcher();\n  public org.apache.hadoop.hbase.CoordinatedStateManager getCoordinatedStateManager();\n  public org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.fs.Path getDataRootDir();\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n  public org.apache.hadoop.fs.Path getWALRootDir();\n  public org.apache.hadoop.fs.FileSystem getWALFileSystem();\n  public boolean isClusterUp();\n  public long getStartcode();\n  public org.apache.hadoop.hbase.http.InfoServer getInfoServer();\n  public int getMsgInterval();\n  public org.apache.hadoop.hbase.namequeues.NamedQueueRecorder getNamedQueueRecorder();\n  public org.apache.hadoop.hbase.ipc.RpcServerInterface getRpcServer();\n  public org.apache.hadoop.hbase.util.NettyEventLoopGroupConfig getEventLoopGroupConfig();\n  public R getRpcServices();\n  public org.apache.hadoop.hbase.MetaRegionLocationCache getMetaRegionLocationCache();\n  public void updateConfiguration();\n  public java.lang.String toString();\n  protected abstract boolean canCreateBaseZNode();\n  protected abstract java.lang.String getProcessName();\n  protected abstract R createRpcServices() throws java.io.IOException;\n  protected abstract java.lang.String getUseThisHostnameInstead(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  protected abstract void login(org.apache.hadoop.hbase.security.UserProvider, java.lang.String) throws java.io.IOException;\n  protected abstract org.apache.hadoop.hbase.namequeues.NamedQueueRecorder createNamedQueueRecord();\n  protected abstract void configureInfoServer(org.apache.hadoop.hbase.http.InfoServer);\n  protected abstract java.lang.Class<? extends javax.servlet.http.HttpServlet> getDumpServlet();\n  protected abstract boolean canUpdateTableDescriptor();\n  protected abstract boolean cacheTableDescriptor();\n  protected abstract boolean clusterMode();\n}\n;;;No, this is not a message definition that might be put on a message queue. It is a class defining various methods and variables related to the HBase server.;;;N;;;No.;;;N
org/apache/hadoop/hbase/HealthCheckChore.class;;;public class org.apache.hadoop.hbase.HealthCheckChore extends org.apache.hadoop.hbase.ScheduledChore {\n  public org.apache.hadoop.hbase.HealthCheckChore(int, org.apache.hadoop.hbase.Stoppable, org.apache.hadoop.conf.Configuration);\n  protected void chore();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/HealthChecker$1.class;;;class org.apache.hadoop.hbase.HealthChecker$1 {\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition belonging to the package org.apache.hadoop.hbase and it is unclear what functionality it provides without further information.;;;N
org/apache/hadoop/hbase/HealthChecker$HealthCheckerExitStatus.class;;;final class org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus extends java.lang.Enum<org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus> {\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus SUCCESS;\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus TIMED_OUT;\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus FAILED_WITH_EXIT_CODE;\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus FAILED_WITH_EXCEPTION;\n  public static final org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus FAILED;\n  public static org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus[] values();\n  public static org.apache.hadoop.hbase.HealthChecker$HealthCheckerExitStatus valueOf(java.lang.String);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a Java class that defines a set of constants and methods for the HealthCheckerExitStatus enumeration type in the Apache Hadoop HBase library.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition for an enum type in the Apache HBase library.;;;N
org/apache/hadoop/hbase/HealthChecker.class;;;class org.apache.hadoop.hbase.HealthChecker {\n  public void init(java.lang.String, long);\n  public org.apache.hadoop.hbase.HealthReport checkHealth();\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/HealthReport.class;;;class org.apache.hadoop.hbase.HealthReport {\n  public java.lang.String toString();\n  public int hashCode();\n  public boolean equals(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/JMXListener.class;;;public class org.apache.hadoop.hbase.JMXListener implements org.apache.hadoop.hbase.coprocessor.MasterCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionServerCoprocessor {\n  public static final java.lang.String RMI_REGISTRY_PORT_CONF_KEY;\n  public static final java.lang.String RMI_CONNECTOR_PORT_CONF_KEY;\n  public static final int defMasterRMIRegistryPort;\n  public static final int defRegionserverRMIRegistryPort;\n  public org.apache.hadoop.hbase.JMXListener();\n  public static javax.management.remote.JMXServiceURL buildJMXServiceURL(int, int) throws java.io.IOException;\n  public void startConnectorServer(int, int) throws java.io.IOException;\n  public void stopConnectorServer() throws java.io.IOException;\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n  public void stop(org.apache.hadoop.hbase.CoprocessorEnvironment) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/LocalHBaseCluster$1.class;;;class org.apache.hadoop.hbase.LocalHBaseCluster$1 implements java.security.PrivilegedExceptionAction<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread> {\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/LocalHBaseCluster$2.class;;;class org.apache.hadoop.hbase.LocalHBaseCluster$2 implements java.security.PrivilegedExceptionAction<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread> {\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread run() throws java.lang.Exception;\n  public java.lang.Object run() throws java.lang.Exception;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/LocalHBaseCluster.class;;;public class org.apache.hadoop.hbase.LocalHBaseCluster {\n  public static final java.lang.String LOCAL;\n  public static final java.lang.String LOCAL_COLON;\n  public static final java.lang.String ASSIGN_RANDOM_PORTS;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int) throws java.io.IOException;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int, java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.LocalHBaseCluster(org.apache.hadoop.conf.Configuration, int, int, int, java.lang.Class<? extends org.apache.hadoop.hbase.master.HMaster>, java.lang.Class<? extends org.apache.hadoop.hbase.regionserver.HRegionServer>) throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread addRegionServer() throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread addRegionServer(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread addRegionServer(org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread addMaster() throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread addMaster(org.apache.hadoop.conf.Configuration, int) throws java.io.IOException;\n  public org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread addMaster(org.apache.hadoop.conf.Configuration, int, org.apache.hadoop.hbase.security.User) throws java.io.IOException, java.lang.InterruptedException;\n  public org.apache.hadoop.hbase.regionserver.HRegionServer getRegionServer(int);\n  public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread> getRegionServers();\n  public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread> getLiveRegionServers();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public java.lang.String waitOnRegionServer(int);\n  public java.lang.String waitOnRegionServer(org.apache.hadoop.hbase.util.JVMClusterUtil$RegionServerThread);\n  public org.apache.hadoop.hbase.master.HMaster getMaster(int);\n  public org.apache.hadoop.hbase.master.HMaster getActiveMaster();\n  public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread> getMasters();\n  public java.util.List<org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread> getLiveMasters();\n  public java.lang.String waitOnMaster(int);\n  public java.lang.String waitOnMaster(org.apache.hadoop.hbase.util.JVMClusterUtil$MasterThread);\n  public void join();\n  public void startup() throws java.io.IOException;\n  public void shutdown();\n  public static boolean isLocal(org.apache.hadoop.conf.Configuration);\n  public static void main(java.lang.String[]) throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/MetaRegionLocationCache$ZNodeOpType.class;;;final class org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType extends java.lang.Enum<org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType> {\n  public static final org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType INIT;\n  public static final org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType CREATED;\n  public static final org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType CHANGED;\n  public static final org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType DELETED;\n  public static org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType[] values();\n  public static org.apache.hadoop.hbase.MetaRegionLocationCache$ZNodeOpType valueOf(java.lang.String);\n}\n;;;No. This is an enum class definition in Java and is unlikely to be put on a message queue as a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/MetaRegionLocationCache.class;;;public class org.apache.hadoop.hbase.MetaRegionLocationCache extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public org.apache.hadoop.hbase.MetaRegionLocationCache(org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n  public java.util.List<org.apache.hadoop.hbase.HRegionLocation> getMetaRegionLocations();\n  public void nodeCreated(java.lang.String);\n  public void nodeDeleted(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n}\n;;;No. While this class has methods and extends another class, it does not define a message format or structure that can be sent on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/MetaTableAccessor.class;;;public final class org.apache.hadoop.hbase.MetaTableAccessor {\n  public static void fullScanRegions(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.Result> fullScanRegions(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public static void fullScanTables(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Table getMetaHTable(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName> getRegion(org.apache.hadoop.hbase.client.Connection, byte[]) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.HRegionLocation getRegionLocation(org.apache.hadoop.hbase.client.Connection, byte[]) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.HRegionLocation getRegionLocation(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Result getCatalogFamilyRow(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Result getRegionResult(org.apache.hadoop.hbase.client.Connection, byte[]) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Result scanByRegionEncodedName(org.apache.hadoop.hbase.client.Connection, java.lang.String) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getAllRegions(org.apache.hadoop.hbase.client.Connection, boolean) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getTableRegions(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.client.RegionInfo> getTableRegions(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Scan getScanForTableName(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.TableName);\n  public static java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName>> getTableRegionsAndLocations(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static java.util.List<org.apache.hadoop.hbase.util.Pair<org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName>> getTableRegionsAndLocations(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, boolean) throws java.io.IOException;\n  public static void fullScanMetaAndPrint(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public static void scanMetaForTableRegions(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static void scanMeta(org.apache.hadoop.hbase.client.Connection, byte[], byte[], org.apache.hadoop.hbase.ClientMetaTableAccessor$QueryType, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static void scanMeta(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor, org.apache.hadoop.hbase.TableName, byte[], int) throws java.io.IOException;\n  public static void scanMeta(org.apache.hadoop.hbase.client.Connection, byte[], byte[], org.apache.hadoop.hbase.ClientMetaTableAccessor$QueryType, int, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static void scanMeta(org.apache.hadoop.hbase.client.Connection, byte[], byte[], org.apache.hadoop.hbase.ClientMetaTableAccessor$QueryType, org.apache.hadoop.hbase.filter.Filter, int, org.apache.hadoop.hbase.ClientMetaTableAccessor$Visitor) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.ServerName getTargetServerName(org.apache.hadoop.hbase.client.Result, int);\n  public static org.apache.hadoop.hbase.util.PairOfSameType<org.apache.hadoop.hbase.client.RegionInfo> getDaughterRegions(org.apache.hadoop.hbase.client.Result);\n  public static org.apache.hadoop.hbase.client.TableState getTableState(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static java.util.Map<org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableState> getTableStates(org.apache.hadoop.hbase.client.Connection) throws java.io.IOException;\n  public static void updateTableState(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName, org.apache.hadoop.hbase.client.TableState$State) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put makePutFromRegionInfo(org.apache.hadoop.hbase.client.RegionInfo, long) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Delete makeDeleteFromRegionInfo(org.apache.hadoop.hbase.client.RegionInfo, long);\n  public static org.apache.hadoop.hbase.client.Put addDaughtersToPut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static void putsToMetaTable(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.Put>) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put addRegionStateToPut(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.master.RegionState$State) throws java.io.IOException;\n  public static void updateRegionState(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.master.RegionState$State) throws java.io.IOException;\n  public static void addSplitsToParent(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static void addRegionsToMeta(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, int) throws java.io.IOException;\n  public static void addRegionsToMeta(org.apache.hadoop.hbase.client.Connection, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, int, long) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put makePutFromTableState(org.apache.hadoop.hbase.client.TableState, long);\n  public static void deleteTableState(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public static void updateRegionLocation(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.ServerName, long, long) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put addRegionInfo(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put addLocation(org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.ServerName, long, int) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.Put addEmptyLocation(org.apache.hadoop.hbase.client.Put, int) throws java.io.IOException;\n}\n;;;Yes, this class defines a set of operations that can be performed on a message queue to access and manipulate metadata related to HBase tables and regions.;;;Y;;;;;;N
org/apache/hadoop/hbase/RegionStateListener.class;;;public interface org.apache.hadoop.hbase.RegionStateListener {\n  public abstract void onRegionSplit(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public abstract void onRegionSplitReverted(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public abstract void onRegionMerged(org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n}\n;;;No.;;;N;;;No. The class provides interface methods for an event listener and does not encapsulate a task that needs to be executed asynchronously.;;;N
org/apache/hadoop/hbase/Server.class;;;public interface org.apache.hadoop.hbase.Server extends org.apache.hadoop.hbase.Abortable,org.apache.hadoop.hbase.Stoppable {\n  public abstract org.apache.hadoop.conf.Configuration getConfiguration();\n  public abstract org.apache.hadoop.hbase.zookeeper.ZKWatcher getZooKeeper();\n  public default org.apache.hadoop.hbase.client.Connection getConnection();\n  public abstract org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public default org.apache.hadoop.hbase.client.AsyncConnection getAsyncConnection();\n  public abstract org.apache.hadoop.hbase.client.AsyncClusterConnection getAsyncClusterConnection();\n  public abstract org.apache.hadoop.hbase.ServerName getServerName();\n  public abstract org.apache.hadoop.hbase.CoordinatedStateManager getCoordinatedStateManager();\n  public abstract org.apache.hadoop.hbase.ChoreService getChoreService();\n  public default org.apache.hadoop.fs.FileSystem getFileSystem();\n  public default boolean isStopping();\n}\n;;;No. This is an interface definition for a class, but it does not define a specific message or data structure that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/SplitLogCounters.class;;;public class org.apache.hadoop.hbase.SplitLogCounters {\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_batch_start;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_batch_success;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_batch_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_success;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_log_split_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_create_queued;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_create_result;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_already_exists;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_create_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_create_retry;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_queued;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_result;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_nonode;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_get_data_retry;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_delete_queued;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_delete_result;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_node_delete_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_failed;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_null_data;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_orphan_task_acquired;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_wait_for_zk_delete;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_unacquired_orphan_done;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_threshold_reached;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_missing_state_in_delete;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_heartbeat;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_rescan;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_rescan_deleted;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_task_deleted;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_unassigned;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_dead_server_task;\n  public static final java.util.concurrent.atomic.LongAdder tot_mgr_resubmit_force;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_failed_to_grab_task_no_data;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_failed_to_grab_task_exception;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_failed_to_grab_task_owned;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_failed_to_grab_task_lost_race;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_acquired;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_resigned;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_done;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_err;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_heartbeat;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_acquired_rescan;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_get_data_queued;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_get_data_result;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_get_data_retry;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_preempt_task;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_heartbeat_failed;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_final_transition_failed;\n  public static final java.util.concurrent.atomic.LongAdder tot_wkr_task_grabing;\n  public static void resetCounters() throws java.lang.Exception;\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a class that provides methods and static fields related to counting various events and actions within the Hadoop HBase framework. It does not define a specific message format or protocol for communication between different components.;;;N;;;No.;;;N
org/apache/hadoop/hbase/SplitLogTask$Done.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Done extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Done(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask$Err.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Err extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Err(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask$Owned.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Owned extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Owned(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask$Resigned.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Resigned extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Resigned(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask$Unassigned.class;;;public class org.apache.hadoop.hbase.SplitLogTask$Unassigned extends org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.SplitLogTask$Unassigned(org.apache.hadoop.hbase.ServerName);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SplitLogTask.class;;;public class org.apache.hadoop.hbase.SplitLogTask {\n  public org.apache.hadoop.hbase.ServerName getServerName();\n  public boolean isUnassigned(org.apache.hadoop.hbase.ServerName);\n  public boolean isUnassigned();\n  public boolean isOwned(org.apache.hadoop.hbase.ServerName);\n  public boolean isOwned();\n  public boolean isResigned(org.apache.hadoop.hbase.ServerName);\n  public boolean isResigned();\n  public boolean isDone(org.apache.hadoop.hbase.ServerName);\n  public boolean isDone();\n  public boolean isErr(org.apache.hadoop.hbase.ServerName);\n  public boolean isErr();\n  public java.lang.String toString();\n  public boolean equals(java.lang.Object);\n  public int hashCode();\n  public static org.apache.hadoop.hbase.SplitLogTask parseFrom(byte[]) throws org.apache.hadoop.hbase.exceptions.DeserializationException;\n  public byte[] toByteArray();\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/SslRMIClientSocketFactorySecure.class;;;public class org.apache.hadoop.hbase.SslRMIClientSocketFactorySecure extends javax.rmi.ssl.SslRMIClientSocketFactory {\n  public org.apache.hadoop.hbase.SslRMIClientSocketFactorySecure();\n  public java.net.Socket createSocket(java.lang.String, int) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/SslRMIServerSocketFactorySecure$1.class;;;class org.apache.hadoop.hbase.SslRMIServerSocketFactorySecure$1 extends java.net.ServerSocket {\n  public java.net.Socket accept() throws java.io.IOException;\n}\n;;;No. This is a class definition and not a message definition.;;;N;;;No.;;;N
org/apache/hadoop/hbase/SslRMIServerSocketFactorySecure.class;;;public class org.apache.hadoop.hbase.SslRMIServerSocketFactorySecure extends javax.rmi.ssl.SslRMIServerSocketFactory {\n  public org.apache.hadoop.hbase.SslRMIServerSocketFactorySecure();\n  public java.net.ServerSocket createServerSocket(int) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/TableDescriptors.class;;;public interface org.apache.hadoop.hbase.TableDescriptors extends java.io.Closeable {\n  public default boolean exists(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public default void close() throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.TableDescriptor get(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public abstract java.util.Map<java.lang.String, org.apache.hadoop.hbase.client.TableDescriptor> getByNamespace(java.lang.String) throws java.io.IOException;\n  public abstract java.util.Map<java.lang.String, org.apache.hadoop.hbase.client.TableDescriptor> getAll() throws java.io.IOException;\n  public default void update(org.apache.hadoop.hbase.client.TableDescriptor) throws java.io.IOException;\n  public abstract void update(org.apache.hadoop.hbase.client.TableDescriptor, boolean) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.client.TableDescriptor remove(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n}\n;;;No, this is an interface that defines methods for working with table descriptors in HBase. It is not a message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/YouAreDeadException.class;;;public class org.apache.hadoop.hbase.YouAreDeadException extends java.io.IOException {\n  public org.apache.hadoop.hbase.YouAreDeadException(java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/ZNodeClearer$1.class;;;final class org.apache.hadoop.hbase.ZNodeClearer$1 implements org.apache.hadoop.hbase.Abortable {\n  public void abort(java.lang.String, java.lang.Throwable);\n  public boolean isAborted();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/ZNodeClearer.class;;;public final class org.apache.hadoop.hbase.ZNodeClearer {\n  public static void writeMyEphemeralNodeOnDisk(java.lang.String);\n  public static java.lang.String readMyEphemeralNodeOnDisk() throws java.io.IOException;\n  public static java.lang.String getMyEphemeralNodeFileName();\n  public static void deleteMyEphemeralNodeOnDisk();\n  public static java.lang.String parseMasterServerName(java.lang.String);\n  public static boolean clear(org.apache.hadoop.conf.Configuration);\n}\n;;;No. This class contains static methods for handling ephemeral nodes and managing Hadoop configurations, but it does not define a message or operation to be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/backup/FailedArchiveException.class;;;public class org.apache.hadoop.hbase.backup.FailedArchiveException extends java.io.IOException {\n  public org.apache.hadoop.hbase.backup.FailedArchiveException(java.lang.String, java.util.Collection<org.apache.hadoop.fs.Path>);\n  public java.util.Collection<org.apache.hadoop.fs.Path> getFailedFiles();\n  public java.lang.String getMessage();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$1.class;;;final class org.apache.hadoop.hbase.backup.HFileArchiver$1 implements java.util.function.Function<org.apache.hadoop.hbase.backup.HFileArchiver$File, org.apache.hadoop.fs.Path> {\n  public org.apache.hadoop.fs.Path apply(org.apache.hadoop.hbase.backup.HFileArchiver$File);\n  public java.lang.Object apply(java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$2.class;;;final class org.apache.hadoop.hbase.backup.HFileArchiver$2 implements org.apache.hadoop.fs.PathFilter {\n  public boolean accept(org.apache.hadoop.fs.Path);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is a Java class that implements the PathFilter interface for HBase backup archiving.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$3.class;;;final class org.apache.hadoop.hbase.backup.HFileArchiver$3 implements java.util.concurrent.ThreadFactory {\n  public java.lang.Thread newThread(java.lang.Runnable);\n}\n;;;No.;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/backup/HFileArchiver$File.class;;;abstract class org.apache.hadoop.hbase.backup.HFileArchiver$File {\n  protected final org.apache.hadoop.fs.FileSystem fs;\n  public org.apache.hadoop.hbase.backup.HFileArchiver$File(org.apache.hadoop.fs.FileSystem);\n  public boolean moveAndClose(org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public org.apache.hadoop.fs.FileSystem getFileSystem();\n  public java.lang.String toString();\n}\n;;;No.;;;N;;;No. The class defines a file in Hadoop Backup and provides methods to move and close the file. It is not a task definition to be put on a task queue.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$FileConverter.class;;;abstract class org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter<T> implements java.util.function.Function<T, org.apache.hadoop.hbase.backup.HFileArchiver$File> {\n  protected final org.apache.hadoop.fs.FileSystem fs;\n  public org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter(org.apache.hadoop.fs.FileSystem);\n}\n;;;No, it is not a message definition that might be put on a message queue. It is an abstract class that defines a file converter in the Hadoop HBase backup system.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$FileStatusConverter.class;;;class org.apache.hadoop.hbase.backup.HFileArchiver$FileStatusConverter extends org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter<org.apache.hadoop.fs.FileStatus> {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$FileStatusConverter(org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.hbase.backup.HFileArchiver$File apply(org.apache.hadoop.fs.FileStatus);\n  public java.lang.Object apply(java.lang.Object);\n}\n;;;No. This class is not a message definition that might be put on a message queue.;;;N;;;No, it is not a task definition that might be put on a task queue as it is a class definition for a file converter in Hadoop.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$FileablePath.class;;;class org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath extends org.apache.hadoop.hbase.backup.HFileArchiver$File {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$FileablePath(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path);\n  public void delete() throws java.io.IOException;\n  public java.lang.String getName();\n  public java.util.Collection<org.apache.hadoop.hbase.backup.HFileArchiver$File> getChildren() throws java.io.IOException;\n  public boolean isFile() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n;;;No;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$FileableStoreFile.class;;;class org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile extends org.apache.hadoop.hbase.backup.HFileArchiver$File {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$FileableStoreFile(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public void delete() throws java.io.IOException;\n  public java.lang.String getName();\n  public boolean isFile();\n  public java.util.Collection<org.apache.hadoop.hbase.backup.HFileArchiver$File> getChildren() throws java.io.IOException;\n  public void close() throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/backup/HFileArchiver$StoreToFile.class;;;class org.apache.hadoop.hbase.backup.HFileArchiver$StoreToFile extends org.apache.hadoop.hbase.backup.HFileArchiver$FileConverter<org.apache.hadoop.hbase.regionserver.HStoreFile> {\n  public org.apache.hadoop.hbase.backup.HFileArchiver$StoreToFile(org.apache.hadoop.fs.FileSystem);\n  public org.apache.hadoop.hbase.backup.HFileArchiver$File apply(org.apache.hadoop.hbase.regionserver.HStoreFile);\n  public java.lang.Object apply(java.lang.Object);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/backup/HFileArchiver.class;;;public class org.apache.hadoop.hbase.backup.HFileArchiver {\n  public static boolean exists(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static void archiveRegion(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo) throws java.io.IOException;\n  public static boolean archiveRegion(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path) throws java.io.IOException;\n  public static void archiveRegions(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.util.List<org.apache.hadoop.fs.Path>) throws java.io.IOException;\n  public static void archiveFamily(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, byte[]) throws java.io.IOException;\n  public static void archiveFamilyByFamilyDir(org.apache.hadoop.fs.FileSystem, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, byte[]) throws java.io.IOException;\n  public static void archiveStoreFiles(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public static void archiveRecoveredEdits(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo, byte[], java.util.Collection<org.apache.hadoop.hbase.regionserver.HStoreFile>) throws java.io.IOException;\n  public static void archiveStoreFile(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.fs.Path, byte[], org.apache.hadoop.fs.Path) throws java.io.IOException;\n}\n;;;No;;;N;;;No, this is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/backup/example/HFileArchiveManager.class;;;class org.apache.hadoop.hbase.backup.example.HFileArchiveManager {\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveManager(org.apache.hadoop.hbase.client.Connection, org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.ZooKeeperConnectionException, java.io.IOException;\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveManager enableHFileBackup(byte[]) throws org.apache.zookeeper.KeeperException;\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveManager disableHFileBackup(byte[]) throws org.apache.zookeeper.KeeperException;\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveManager disableHFileBackup() throws java.io.IOException;\n  public void stop();\n  public boolean isArchivingEnabled(byte[]) throws org.apache.zookeeper.KeeperException;\n}\n;;;No, this class is not a message definition that might be put on a message queue. It is a class defining methods for managing HFile backups in HBase.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/example/HFileArchiveTableMonitor.class;;;public class org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor {\n  public org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor();\n  public synchronized void setArchiveTables(java.util.List<java.lang.String>);\n  public synchronized void addTable(java.lang.String);\n  public synchronized void removeTable(java.lang.String);\n  public synchronized void clearArchive();\n  public synchronized boolean shouldArchiveTable(java.lang.String);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/example/LongTermArchivingHFileCleaner.class;;;public class org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner extends org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate {\n  public org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner();\n  public boolean isFileDeletable(org.apache.hadoop.fs.FileStatus);\n  public void setConf(org.apache.hadoop.conf.Configuration);\n  public void stop(java.lang.String);\n}\n;;;Yes;;;Y;;;;;;N
org/apache/hadoop/hbase/backup/example/TableHFileArchiveTracker.class;;;public final class org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker extends org.apache.hadoop.hbase.zookeeper.ZKListener {\n  public static final java.lang.String HFILE_ARCHIVE_ZNODE_PARENT;\n  public void start() throws org.apache.zookeeper.KeeperException;\n  public void nodeCreated(java.lang.String);\n  public void nodeChildrenChanged(java.lang.String);\n  public void nodeDeleted(java.lang.String);\n  public boolean keepHFiles(java.lang.String);\n  public final org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor getMonitor();\n  public static org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker create(org.apache.hadoop.conf.Configuration) throws org.apache.hadoop.hbase.ZooKeeperConnectionException, java.io.IOException;\n  public org.apache.hadoop.hbase.zookeeper.ZKWatcher getZooKeeperWatcher();\n  public void stop();\n}\n;;;No, this is not a message definition. It is a class definition that provides methods to start, stop, and monitor a process related to HBase backup.;;;N;;;No.;;;N
org/apache/hadoop/hbase/backup/example/ZKTableArchiveClient.class;;;public class org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient extends org.apache.hadoop.conf.Configured {\n  public org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.Connection);\n  public void enableHFileBackupAsync(byte[]) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public void disableHFileBackup(java.lang.String) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public void disableHFileBackup(byte[]) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public void disableHFileBackup() throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public boolean getArchivingEnabled(byte[]) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public boolean getArchivingEnabled(java.lang.String) throws java.io.IOException, org.apache.zookeeper.KeeperException;\n  public static java.lang.String getArchiveZNode(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/AsyncClusterConnection.class;;;public interface org.apache.hadoop.hbase.client.AsyncClusterConnection extends org.apache.hadoop.hbase.client.AsyncConnection {\n  public abstract org.apache.hadoop.hbase.client.AsyncRegionServerAdmin getRegionServerAdmin(org.apache.hadoop.hbase.ServerName);\n  public abstract org.apache.hadoop.hbase.client.NonceGenerator getNonceGenerator();\n  public abstract org.apache.hadoop.hbase.ipc.RpcClient getRpcClient();\n  public abstract java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionResponse> flush(byte[], boolean);\n  public abstract java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.RegionLocations> getRegionLocations(org.apache.hadoop.hbase.TableName, byte[], boolean);\n  public abstract java.util.concurrent.CompletableFuture<java.lang.String> prepareBulkLoad(org.apache.hadoop.hbase.TableName);\n  public abstract java.util.concurrent.CompletableFuture<java.lang.Boolean> bulkLoad(org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, byte[], boolean, org.apache.hadoop.security.token.Token<?>, java.lang.String, boolean, java.util.List<java.lang.String>, boolean);\n  public abstract java.util.concurrent.CompletableFuture<java.lang.Void> cleanupBulkLoad(org.apache.hadoop.hbase.TableName, java.lang.String);\n  public abstract java.util.concurrent.CompletableFuture<java.util.List<org.apache.hadoop.hbase.ServerName>> getLiveRegionServers(org.apache.hadoop.hbase.zookeeper.MasterAddressTracker, int);\n  public abstract java.util.concurrent.CompletableFuture<java.util.List<org.apache.hadoop.hbase.ServerName>> getAllBootstrapNodes(org.apache.hadoop.hbase.ServerName);\n  public abstract java.util.concurrent.CompletableFuture<java.lang.Void> replicate(org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>, int, long, long);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/AsyncClusterConnectionImpl.class;;;class org.apache.hadoop.hbase.client.AsyncClusterConnectionImpl extends org.apache.hadoop.hbase.client.AsyncConnectionImpl implements org.apache.hadoop.hbase.client.AsyncClusterConnection {\n  public org.apache.hadoop.hbase.client.AsyncClusterConnectionImpl(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.client.ConnectionRegistry, java.lang.String, java.net.SocketAddress, org.apache.hadoop.hbase.security.User);\n  public org.apache.hadoop.hbase.client.NonceGenerator getNonceGenerator();\n  public org.apache.hadoop.hbase.ipc.RpcClient getRpcClient();\n  public org.apache.hadoop.hbase.client.AsyncRegionServerAdmin getRegionServerAdmin(org.apache.hadoop.hbase.ServerName);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionResponse> flush(byte[], boolean);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.RegionLocations> getRegionLocations(org.apache.hadoop.hbase.TableName, byte[], boolean);\n  public java.util.concurrent.CompletableFuture<java.lang.String> prepareBulkLoad(org.apache.hadoop.hbase.TableName);\n  public java.util.concurrent.CompletableFuture<java.lang.Boolean> bulkLoad(org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.util.Pair<byte[], java.lang.String>>, byte[], boolean, org.apache.hadoop.security.token.Token<?>, java.lang.String, boolean, java.util.List<java.lang.String>, boolean);\n  public java.util.concurrent.CompletableFuture<java.lang.Void> cleanupBulkLoad(org.apache.hadoop.hbase.TableName, java.lang.String);\n  public java.util.concurrent.CompletableFuture<java.util.List<org.apache.hadoop.hbase.ServerName>> getLiveRegionServers(org.apache.hadoop.hbase.zookeeper.MasterAddressTracker, int);\n  public java.util.concurrent.CompletableFuture<java.util.List<org.apache.hadoop.hbase.ServerName>> getAllBootstrapNodes(org.apache.hadoop.hbase.ServerName);\n  public java.util.concurrent.CompletableFuture<java.lang.Void> replicate(org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>, int, long, long);\n}\n;;;Yes, the class org.apache.hadoop.hbase.client.AsyncClusterConnectionImpl may be put on a message queue as a message definition.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/AsyncRegionReplicationRetryingCaller.class;;;public class org.apache.hadoop.hbase.client.AsyncRegionReplicationRetryingCaller extends org.apache.hadoop.hbase.client.AsyncRpcRetryingCaller<java.lang.Void> {\n  public org.apache.hadoop.hbase.client.AsyncRegionReplicationRetryingCaller(org.apache.hbase.thirdparty.io.netty.util.HashedWheelTimer, org.apache.hadoop.hbase.client.AsyncClusterConnectionImpl, int, long, long, org.apache.hadoop.hbase.client.RegionInfo, java.util.List<org.apache.hadoop.hbase.wal.WAL$Entry>);\n  protected java.lang.Throwable preProcessError(java.lang.Throwable);\n  protected void doCall();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/AsyncRegionServerAdmin$1.class;;;class org.apache.hadoop.hbase.client.AsyncRegionServerAdmin$1 implements org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<RESP> {\n  public void run(RESP);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/AsyncRegionServerAdmin$RpcCall.class;;;interface org.apache.hadoop.hbase.client.AsyncRegionServerAdmin$RpcCall<RESP> {\n  public abstract void call(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$AdminService$Interface, org.apache.hadoop.hbase.ipc.HBaseRpcController, org.apache.hbase.thirdparty.com.google.protobuf.RpcCallback<RESP>);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/AsyncRegionServerAdmin.class;;;public class org.apache.hadoop.hbase.client.AsyncRegionServerAdmin {\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionInfoResponse> getRegionInfo(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionInfoRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetStoreFileResponse> getStoreFile(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetStoreFileRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetOnlineRegionResponse> getOnlineRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetOnlineRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$OpenRegionResponse> openRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$OpenRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WarmupRegionResponse> warmupRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$WarmupRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CloseRegionResponse> closeRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CloseRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionResponse> flushRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$FlushRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactionSwitchResponse> compactionSwitch(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactionSwitchRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactRegionResponse> compactRegion(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$CompactRegionRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse> replicateWALEntry(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner, int);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryResponse> replay(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ReplicateWALEntryRequest, org.apache.hadoop.hbase.CellScanner);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$RollWALWriterResponse> rollWALWriter(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$RollWALWriterRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetServerInfoResponse> getServerInfo(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetServerInfoRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$StopServerResponse> stopServer(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$StopServerRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateFavoredNodesResponse> updateFavoredNodes(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateFavoredNodesRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateConfigurationResponse> updateConfiguration(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$UpdateConfigurationRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionLoadResponse> getRegionLoad(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$GetRegionLoadRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearCompactionQueuesResponse> clearCompactionQueues(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearCompactionQueuesRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearRegionBlockCacheResponse> clearRegionBlockCache(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ClearRegionBlockCacheRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaSnapshotsResponse> getSpaceQuotaSnapshots(org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos$GetSpaceQuotaSnapshotsRequest);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ExecuteProceduresResponse> executeProcedures(org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos$ExecuteProceduresRequest);\n}\n;;;Yes, this class is a message definition that might be put on a message queue. Each method represents a specific operation that can be performed by an async region server admin and returns a CompletableFuture object, which is a way to represent an asynchronous operation that may complete in the future.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/ClientSideRegionScanner.class;;;public class org.apache.hadoop.hbase.client.ClientSideRegionScanner extends org.apache.hadoop.hbase.client.AbstractClientScanner {\n  public org.apache.hadoop.hbase.client.ClientSideRegionScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.FileSystem, org.apache.hadoop.fs.Path, org.apache.hadoop.hbase.client.TableDescriptor, org.apache.hadoop.hbase.client.RegionInfo, org.apache.hadoop.hbase.client.Scan, org.apache.hadoop.hbase.client.metrics.ScanMetrics) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result next() throws java.io.IOException;\n  public void close();\n  public boolean renewLease();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/ClusterConnectionFactory.class;;;public final class org.apache.hadoop.hbase.client.ClusterConnectionFactory {\n  public static final java.lang.String HBASE_SERVER_CLUSTER_CONNECTION_IMPL;\n  public static org.apache.hadoop.hbase.client.AsyncClusterConnection createAsyncClusterConnection(org.apache.hadoop.conf.Configuration, java.net.SocketAddress, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.AsyncClusterConnection createAsyncClusterConnection(org.apache.hadoop.hbase.client.ConnectionRegistryEndpoint, org.apache.hadoop.conf.Configuration, java.net.SocketAddress, org.apache.hadoop.hbase.security.User) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/ConnectionRegistryEndpoint.class;;;public interface org.apache.hadoop.hbase.client.ConnectionRegistryEndpoint {\n  public abstract java.lang.String getClusterId();\n  public abstract java.util.Optional<org.apache.hadoop.hbase.ServerName> getActiveMaster();\n  public abstract java.util.List<org.apache.hadoop.hbase.ServerName> getBackupMasters();\n  public abstract java.util.Iterator<org.apache.hadoop.hbase.ServerName> getBootstrapNodes();\n  public abstract java.util.List<org.apache.hadoop.hbase.HRegionLocation> getMetaLocations();\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a class defining methods for accessing certain information in a Hadoop HBase environment.;;;N
org/apache/hadoop/hbase/client/SharedAsyncConnection.class;;;public class org.apache.hadoop.hbase.client.SharedAsyncConnection implements org.apache.hadoop.hbase.client.AsyncConnection {\n  public org.apache.hadoop.hbase.client.SharedAsyncConnection(org.apache.hadoop.hbase.client.AsyncConnection);\n  public boolean isClosed();\n  public void close() throws java.io.IOException;\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.client.AsyncTableRegionLocator getRegionLocator(org.apache.hadoop.hbase.TableName);\n  public void clearRegionLocationCache();\n  public org.apache.hadoop.hbase.client.AsyncTableBuilder<org.apache.hadoop.hbase.client.AdvancedScanResultConsumer> getTableBuilder(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.client.AsyncTableBuilder<org.apache.hadoop.hbase.client.ScanResultConsumer> getTableBuilder(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService);\n  public org.apache.hadoop.hbase.client.AsyncAdminBuilder getAdminBuilder();\n  public org.apache.hadoop.hbase.client.AsyncAdminBuilder getAdminBuilder(java.util.concurrent.ExecutorService);\n  public org.apache.hadoop.hbase.client.AsyncBufferedMutatorBuilder getBufferedMutatorBuilder(org.apache.hadoop.hbase.TableName);\n  public org.apache.hadoop.hbase.client.AsyncBufferedMutatorBuilder getBufferedMutatorBuilder(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.client.Hbck> getHbck();\n  public org.apache.hadoop.hbase.client.Hbck getHbck(org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Connection toConnection();\n}\n;;;No. This class is not a message definition that might be put on a message queue. It is a Java class that implements the AsyncConnection interface in the HBase library.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/SharedConnection.class;;;public class org.apache.hadoop.hbase.client.SharedConnection implements org.apache.hadoop.hbase.client.Connection {\n  public org.apache.hadoop.hbase.client.SharedConnection(org.apache.hadoop.hbase.client.Connection);\n  public void close() throws java.io.IOException;\n  public boolean isClosed();\n  public void abort(java.lang.String, java.lang.Throwable);\n  public boolean isAborted();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n  public org.apache.hadoop.hbase.client.BufferedMutator getBufferedMutator(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.BufferedMutator getBufferedMutator(org.apache.hadoop.hbase.client.BufferedMutatorParams) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.RegionLocator getRegionLocator(org.apache.hadoop.hbase.TableName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Admin getAdmin() throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableBuilder getTableBuilder(org.apache.hadoop.hbase.TableName, java.util.concurrent.ExecutorService);\n  public void clearRegionLocationCache();\n  public org.apache.hadoop.hbase.client.Hbck getHbck() throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Hbck getHbck(org.apache.hadoop.hbase.ServerName) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.AsyncConnection toAsyncConnection();\n  public java.lang.String getClusterId();\n}\n;;;No;;;N;;;No. It is a class representing a connection to a Hadoop HBase database and its methods for performing operations on it. It is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/client/ShortCircuitConnectionRegistry.class;;;class org.apache.hadoop.hbase.client.ShortCircuitConnectionRegistry implements org.apache.hadoop.hbase.client.ConnectionRegistry {\n  public org.apache.hadoop.hbase.client.ShortCircuitConnectionRegistry(org.apache.hadoop.hbase.client.ConnectionRegistryEndpoint);\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.RegionLocations> getMetaRegionLocations();\n  public java.util.concurrent.CompletableFuture<java.lang.String> getClusterId();\n  public java.util.concurrent.CompletableFuture<org.apache.hadoop.hbase.ServerName> getActiveMaster();\n  public java.lang.String getConnectionString();\n  public void close();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/TableSnapshotScanner.class;;;public class org.apache.hadoop.hbase.client.TableSnapshotScanner extends org.apache.hadoop.hbase.client.AbstractClientScanner {\n  public org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.TableSnapshotScanner(org.apache.hadoop.conf.Configuration, org.apache.hadoop.fs.Path, org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.hbase.client.Scan, boolean) throws java.io.IOException;\n  public org.apache.hadoop.hbase.client.Result next() throws java.io.IOException;\n  public void close();\n  public boolean renewLease();\n}\n;;;No. This is not a message definition, but rather a class definition for a scanner used to read data from a HBase table snapshot.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/VersionInfoUtil$ServiceCallFunction.class;;;public interface org.apache.hadoop.hbase.client.VersionInfoUtil$ServiceCallFunction<T1, T2, R, E extends java.lang.Throwable> {\n  public abstract R apply(T1, T2) throws E;\n}\n;;;No.;;;N;;;No, it is not a task definition. It is an interface representing a function that takes two arguments of type T1 and T2, and returns a value of type R. It also throws an exception of type E.;;;N
org/apache/hadoop/hbase/client/VersionInfoUtil.class;;;public final class org.apache.hadoop.hbase.client.VersionInfoUtil {\n  public static boolean currentClientHasMinimumVersion(int, int);\n  public static boolean hasMinimumVersion(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$VersionInfo, int, int);\n  public static <T1, T2, R, E extends java.lang.Throwable> R callWithVersion(org.apache.hadoop.hbase.client.VersionInfoUtil$ServiceCallFunction<T1, T2, R, E>, T1, T2) throws E;\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$VersionInfo getCurrentClientVersionInfo();\n  public static java.lang.String versionNumberToString(int);\n  public static int getVersionNumber(org.apache.hadoop.hbase.shaded.protobuf.generated.HBaseProtos$VersionInfo);\n}\n;;;No, this is not a message definition. It is a utility class with static methods used for version checking and calling functions with specific versions.;;;N;;;No.;;;N
org/apache/hadoop/hbase/client/locking/EntityLock$LockHeartbeatWorker.class;;;public class org.apache.hadoop.hbase.client.locking.EntityLock$LockHeartbeatWorker extends java.lang.Thread {\n  public org.apache.hadoop.hbase.client.locking.EntityLock$LockHeartbeatWorker(org.apache.hadoop.hbase.client.locking.EntityLock, java.lang.String);\n  public void run();\n}\n;;;No;;;N;;;Yes.;;;Y
org/apache/hadoop/hbase/client/locking/EntityLock.class;;;public class org.apache.hadoop.hbase.client.locking.EntityLock {\n  public static final java.lang.String HEARTBEAT_TIME_BUFFER;\n  public java.lang.String toString();\n  public boolean isLocked();\n  public void requestLock() throws java.io.IOException;\n  public boolean await(long, java.util.concurrent.TimeUnit) throws java.lang.InterruptedException;\n  public void await() throws java.lang.InterruptedException;\n  public void unlock() throws java.io.IOException;\n}\n;;;Yes, it is a message definition that could be put on a message queue, as it provides methods for requesting, releasing, and checking the status of a lock on an entity.;;;Y;;;;;;N
org/apache/hadoop/hbase/client/locking/LockServiceClient.class;;;public class org.apache.hadoop.hbase.client.locking.LockServiceClient {\n  public org.apache.hadoop.hbase.client.locking.LockServiceClient(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockService$BlockingInterface, org.apache.hadoop.hbase.client.NonceGenerator);\n  public org.apache.hadoop.hbase.client.locking.EntityLock tableLock(org.apache.hadoop.hbase.TableName, boolean, java.lang.String, org.apache.hadoop.hbase.Abortable);\n  public org.apache.hadoop.hbase.client.locking.EntityLock namespaceLock(java.lang.String, java.lang.String, org.apache.hadoop.hbase.Abortable);\n  public org.apache.hadoop.hbase.client.locking.EntityLock regionLock(java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, java.lang.String, org.apache.hadoop.hbase.Abortable);\n  public static org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockRequest buildLockRequest(org.apache.hadoop.hbase.shaded.protobuf.generated.LockServiceProtos$LockType, java.lang.String, org.apache.hadoop.hbase.TableName, java.util.List<org.apache.hadoop.hbase.client.RegionInfo>, java.lang.String, long, long);\n}\n;;;No. While the LockServiceClient class has methods and parameters that are used for locking, it is not a specific definition of a message that can be sent on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/codec/MessageCodec$MessageDecoder.class;;;class org.apache.hadoop.hbase.codec.MessageCodec$MessageDecoder extends org.apache.hadoop.hbase.codec.BaseDecoder {\n  protected org.apache.hadoop.hbase.Cell parseCell() throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/codec/MessageCodec$MessageEncoder.class;;;class org.apache.hadoop.hbase.codec.MessageCodec$MessageEncoder extends org.apache.hadoop.hbase.codec.BaseEncoder {\n  public void write(org.apache.hadoop.hbase.Cell) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/codec/MessageCodec.class;;;public class org.apache.hadoop.hbase.codec.MessageCodec implements org.apache.hadoop.hbase.codec.Codec {\n  public org.apache.hadoop.hbase.codec.MessageCodec();\n  public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(java.io.InputStream);\n  public org.apache.hadoop.hbase.codec.Codec$Decoder getDecoder(org.apache.hadoop.hbase.nio.ByteBuff);\n  public org.apache.hadoop.hbase.codec.Codec$Encoder getEncoder(java.io.OutputStream);\n}\n;;;No. This is a class that is designed to encode and decode messages in the HBase system, but it is not a specific message definition that would be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/constraint/BaseConstraint.class;;;public abstract class org.apache.hadoop.hbase.constraint.BaseConstraint extends org.apache.hadoop.conf.Configured implements org.apache.hadoop.hbase.constraint.Constraint {\n  public org.apache.hadoop.hbase.constraint.BaseConstraint();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/constraint/Constraint.class;;;public interface org.apache.hadoop.hbase.constraint.Constraint extends org.apache.hadoop.conf.Configurable {\n  public abstract void check(org.apache.hadoop.hbase.client.Put) throws org.apache.hadoop.hbase.constraint.ConstraintException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/constraint/ConstraintException.class;;;public class org.apache.hadoop.hbase.constraint.ConstraintException extends org.apache.hadoop.hbase.DoNotRetryIOException {\n  public org.apache.hadoop.hbase.constraint.ConstraintException();\n  public org.apache.hadoop.hbase.constraint.ConstraintException(java.lang.String);\n  public org.apache.hadoop.hbase.constraint.ConstraintException(java.lang.String, java.lang.Throwable);\n}\n;;;No;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/constraint/ConstraintProcessor.class;;;public class org.apache.hadoop.hbase.constraint.ConstraintProcessor implements org.apache.hadoop.hbase.coprocessor.RegionCoprocessor,org.apache.hadoop.hbase.coprocessor.RegionObserver {\n  public java.util.Optional<org.apache.hadoop.hbase.coprocessor.RegionObserver> getRegionObserver();\n  public org.apache.hadoop.hbase.constraint.ConstraintProcessor();\n  public void start(org.apache.hadoop.hbase.CoprocessorEnvironment);\n  public void prePut(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hadoop.hbase.client.Put, org.apache.hadoop.hbase.wal.WALEdit, org.apache.hadoop.hbase.client.Durability) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/constraint/Constraints$1.class;;;final class org.apache.hadoop.hbase.constraint.Constraints$1 implements java.util.Comparator<org.apache.hadoop.hbase.constraint.Constraint> {\n  public int compare(org.apache.hadoop.hbase.constraint.Constraint, org.apache.hadoop.hbase.constraint.Constraint);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/constraint/Constraints.class;;;public final class org.apache.hadoop.hbase.constraint.Constraints {\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder enable(org.apache.hadoop.hbase.client.TableDescriptorBuilder) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder disable(org.apache.hadoop.hbase.client.TableDescriptorBuilder) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder remove(org.apache.hadoop.hbase.client.TableDescriptorBuilder) throws java.io.IOException;\n  public static boolean has(org.apache.hadoop.hbase.client.TableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>);\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder add(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>...) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder add(org.apache.hadoop.hbase.client.TableDescriptorBuilder, org.apache.hadoop.hbase.util.Pair<java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration>...) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder add(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder setConfiguration(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>, org.apache.hadoop.conf.Configuration) throws java.io.IOException, java.lang.IllegalArgumentException;\n  public static org.apache.hadoop.hbase.client.TableDescriptorBuilder remove(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>);\n  public static void enableConstraint(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>) throws java.io.IOException;\n  public static void disableConstraint(org.apache.hadoop.hbase.client.TableDescriptorBuilder, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>) throws java.io.IOException;\n  public static boolean enabled(org.apache.hadoop.hbase.client.TableDescriptor, java.lang.Class<? extends org.apache.hadoop.hbase.constraint.Constraint>) throws java.io.IOException;\n}\n;;;No, this class is not a message definition that might be put on a message queue.;;;N;;;No, it is not a task definition but rather a collection of static utility methods for managing constraints on HBase tables.;;;N
org/apache/hadoop/hbase/coordination/SplitLogManagerCoordination$SplitLogManagerDetails.class;;;public class org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails {\n  public org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails(java.util.concurrent.ConcurrentMap<java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task>, org.apache.hadoop.hbase.master.MasterServices, java.util.Set<java.lang.String>);\n  public org.apache.hadoop.hbase.master.MasterServices getMaster();\n  public java.util.concurrent.ConcurrentMap<java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task> getTasks();\n  public java.util.Set<java.lang.String> getFailedDeletions();\n  public org.apache.hadoop.hbase.ServerName getServerName();\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/SplitLogManagerCoordination.class;;;public interface org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination {\n  public abstract void setDetails(org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails);\n  public abstract org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails getDetails();\n  public abstract java.lang.String prepareTask(java.lang.String);\n  public abstract void checkTasks();\n  public abstract int remainingTasksInCoordination();\n  public abstract void checkTaskStillAvailable(java.lang.String);\n  public abstract boolean resubmitTask(java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task, org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective);\n  public abstract void submitTask(java.lang.String);\n  public abstract void deleteTask(java.lang.String);\n  public abstract void init() throws java.io.IOException;\n}\n;;;No;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/SplitLogWorkerCoordination$SplitTaskDetails.class;;;public interface org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails {\n  public abstract java.lang.String getWALFile();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/SplitLogWorkerCoordination.class;;;public interface org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination {\n  public abstract void init(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor, org.apache.hadoop.hbase.regionserver.SplitLogWorker);\n  public abstract void stopProcessingTasks();\n  public abstract boolean isStop();\n  public abstract void taskLoop() throws java.lang.InterruptedException;\n  public abstract void markCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem);\n  public abstract boolean isReady() throws java.lang.InterruptedException;\n  public abstract int getTaskReadySeq();\n  public abstract void registerListener();\n  public abstract void removeListener();\n  public abstract void endTask(org.apache.hadoop.hbase.SplitLogTask, java.util.concurrent.atomic.LongAdder, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails);\n}\n;;;No, this is not a message definition. It is an interface that defines methods for coordinating split log workers in the HBase framework.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$1.class;;;class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$1 implements org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status finish(org.apache.hadoop.hbase.ServerName, java.lang.String);\n}\n;;;No.;;;N;;;Yes, it is a task definition that might be put on a task queue.;;;Y
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$CreateAsyncCallback.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateAsyncCallback implements org.apache.zookeeper.AsyncCallback$StringCallback {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination);\n  public void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$CreateRescanAsyncCallback.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateRescanAsyncCallback implements org.apache.zookeeper.AsyncCallback$StringCallback {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$CreateRescanAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination);\n  public void processResult(int, java.lang.String, java.lang.Object, java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$DeleteAsyncCallback.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$DeleteAsyncCallback implements org.apache.zookeeper.AsyncCallback$VoidCallback {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$DeleteAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination);\n  public void processResult(int, java.lang.String, java.lang.Object);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$GetDataAsyncCallback.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$GetDataAsyncCallback implements org.apache.zookeeper.AsyncCallback$DataCallback {\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$GetDataAsyncCallback(org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination);\n  public void processResult(int, java.lang.String, java.lang.Object, byte[], org.apache.zookeeper.data.Stat);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$TaskFinisher$Status.class;;;public final class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status extends java.lang.Enum<org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status> {\n  public static final org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status DONE;\n  public static final org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status ERR;\n  public static org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status[] values();\n  public static org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status valueOf(java.lang.String);\n}\n;;;No. This is not a message definition, it is an enumeration that defines two possible values of a certain type. It could be used in a message definition, but it is not a message definition in itself.;;;N;;;No, it is not a task definition.;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination$TaskFinisher.class;;;public interface org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher {\n  public abstract org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination$TaskFinisher$Status finish(org.apache.hadoop.hbase.ServerName, java.lang.String);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/ZKSplitLogManagerCoordination.class;;;public class org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination extends org.apache.hadoop.hbase.zookeeper.ZKListener implements org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination {\n  public static final int DEFAULT_TIMEOUT;\n  public static final int DEFAULT_ZK_RETRIES;\n  public static final int DEFAULT_MAX_RESUBMIT;\n  public boolean ignoreZKDeleteForTesting;\n  public org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination(org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n  public void init() throws java.io.IOException;\n  public java.lang.String prepareTask(java.lang.String);\n  public int remainingTasksInCoordination();\n  public void deleteTask(java.lang.String);\n  public boolean resubmitTask(java.lang.String, org.apache.hadoop.hbase.master.SplitLogManager$Task, org.apache.hadoop.hbase.master.SplitLogManager$ResubmitDirective);\n  public void checkTasks();\n  public void submitTask(java.lang.String);\n  public void checkTaskStillAvailable(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public void setDetails(org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails);\n  public org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination$SplitLogManagerDetails getDetails();\n  public void setIgnoreDeleteForTesting(boolean);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coordination/ZkCoordinatedStateManager.class;;;public class org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager implements org.apache.hadoop.hbase.CoordinatedStateManager {\n  protected org.apache.hadoop.hbase.zookeeper.ZKWatcher watcher;\n  protected org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination splitLogWorkerCoordination;\n  protected org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination splitLogManagerCoordination;\n  public org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager(org.apache.hadoop.hbase.Server);\n  public org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination getSplitLogWorkerCoordination();\n  public org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination getSplitLogManagerCoordination();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination$1.class;;;class org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$1 implements org.apache.hadoop.hbase.util.CancelableProgressable {\n  public boolean progress();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination$GetDataAsyncCallback.class;;;class org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$GetDataAsyncCallback implements org.apache.zookeeper.AsyncCallback$DataCallback {\n  public void processResult(int, java.lang.String, java.lang.Object, byte[], org.apache.zookeeper.data.Stat);\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination$ZkSplitTaskDetails.class;;;public class org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails implements org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails {\n  public org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails();\n  public org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination$ZkSplitTaskDetails(java.lang.String, org.apache.commons.lang3.mutable.MutableInt);\n  public java.lang.String getTaskNode();\n  public void setTaskNode(java.lang.String);\n  public org.apache.commons.lang3.mutable.MutableInt getCurTaskZKVersion();\n  public void setCurTaskZKVersion(org.apache.commons.lang3.mutable.MutableInt);\n  public java.lang.String getWALFile();\n}\n;;;No.;;;N;;;Yes;;;Y
org/apache/hadoop/hbase/coordination/ZkSplitLogWorkerCoordination.class;;;public class org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination extends org.apache.hadoop.hbase.zookeeper.ZKListener implements org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination {\n  protected final java.util.concurrent.atomic.AtomicInteger tasksInProgress;\n  public org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination(org.apache.hadoop.hbase.ServerName, org.apache.hadoop.hbase.zookeeper.ZKWatcher);\n  public void nodeChildrenChanged(java.lang.String);\n  public void nodeDataChanged(java.lang.String);\n  public void init(org.apache.hadoop.hbase.regionserver.RegionServerServices, org.apache.hadoop.conf.Configuration, org.apache.hadoop.hbase.regionserver.SplitLogWorker$TaskExecutor, org.apache.hadoop.hbase.regionserver.SplitLogWorker);\n  public void getDataSetWatchAsync();\n  protected static int attemptToOwnTask(boolean, org.apache.hadoop.hbase.zookeeper.ZKWatcher, org.apache.hadoop.hbase.ServerName, java.lang.String, int);\n  public void taskLoop() throws java.lang.InterruptedException;\n  public void markCorrupted(org.apache.hadoop.fs.Path, java.lang.String, org.apache.hadoop.fs.FileSystem);\n  public boolean isReady() throws java.lang.InterruptedException;\n  public int getTaskReadySeq();\n  public void registerListener();\n  public void removeListener();\n  public void stopProcessingTasks();\n  public boolean isStop();\n  public void endTask(org.apache.hadoop.hbase.SplitLogTask, java.util.concurrent.atomic.LongAdder, org.apache.hadoop.hbase.coordination.SplitLogWorkerCoordination$SplitTaskDetails);\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/BaseEnvironment.class;;;public class org.apache.hadoop.hbase.coprocessor.BaseEnvironment<C extends org.apache.hadoop.hbase.Coprocessor> implements org.apache.hadoop.hbase.CoprocessorEnvironment<C> {\n  public C impl;\n  protected int priority;\n  public org.apache.hadoop.hbase.coprocessor.BaseEnvironment(C, int, int, org.apache.hadoop.conf.Configuration);\n  public void startup() throws java.io.IOException;\n  public void shutdown();\n  public C getInstance();\n  public java.lang.ClassLoader getClassLoader();\n  public int getPriority();\n  public int getLoadSequence();\n  public int getVersion();\n  public java.lang.String getHBaseVersion();\n  public org.apache.hadoop.conf.Configuration getConfiguration();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/BulkLoadObserver.class;;;public interface org.apache.hadoop.hbase.coprocessor.BulkLoadObserver {\n  public default void prePrepareBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException;\n  public default void preCleanupBulkLoad(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>) throws java.io.IOException;\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$1.class;;;final class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$1 implements java.util.Comparator<java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>> {\n  public int compare(java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>, java.lang.Class<? extends org.apache.hadoop.hbase.Coprocessor>);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue. It is a class definition that implements the `java.util.Comparator` interface.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$EnvironmentPriorityComparator.class;;;class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$EnvironmentPriorityComparator implements java.util.Comparator<org.apache.hadoop.hbase.CoprocessorEnvironment> {\n  public int compare(org.apache.hadoop.hbase.CoprocessorEnvironment, org.apache.hadoop.hbase.CoprocessorEnvironment);\n  public int compare(java.lang.Object, java.lang.Object);\n}\n;;;No, this is not a message definition. It is a class definition for a comparator used in a Hadoop/HBase coprocessor.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$ObserverGetter.class;;;public interface org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O> extends java.util.function.Function<C, java.util.Optional<O>> {\n}\n;;;No.;;;N;;;No, it is not a task definition. It is an interface definition for a function that gets an observer for a coprocessor host in HBase.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$ObserverOperation.class;;;abstract class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperation<O> extends org.apache.hadoop.hbase.coprocessor.ObserverContextImpl<E> {\n  protected void postEnvCall();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$ObserverOperationWithResult.class;;;public abstract class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithResult<O, R> extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<C, E>.ObserverOperation<O> {\n  protected abstract R call(O) throws java.io.IOException;\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, R);\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, R, boolean);\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, R, org.apache.hadoop.hbase.security.User);\n  protected R getResult();\n}\n;;;No.;;;N;;;It is not a task definition that might be put on a task queue.;;;?
org/apache/hadoop/hbase/coprocessor/CoprocessorHost$ObserverOperationWithoutResult.class;;;public abstract class org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult<O> extends org.apache.hadoop.hbase.coprocessor.CoprocessorHost<C, E>.ObserverOperation<O> {\n  protected abstract void call(O) throws java.io.IOException;\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>);\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, org.apache.hadoop.hbase.security.User);\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverOperationWithoutResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost$ObserverGetter<C, O>, org.apache.hadoop.hbase.security.User, boolean);\n}\n;;;No.;;;N;;;No, it is not a task definition that might be put on a task queue.;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorHost.class;;;public abstract class org.apache.hadoop.hbase.coprocessor.CoprocessorHost<C extends org.apache.hadoop.hbase.Coprocessor, E extends org.apache.hadoop.hbase.CoprocessorEnvironment<C>> {\n  public static final java.lang.String REGION_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String REGIONSERVER_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String USER_REGION_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String MASTER_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String WAL_COPROCESSOR_CONF_KEY;\n  public static final java.lang.String ABORT_ON_ERROR_KEY;\n  public static final boolean DEFAULT_ABORT_ON_ERROR;\n  public static final java.lang.String COPROCESSORS_ENABLED_CONF_KEY;\n  public static final boolean DEFAULT_COPROCESSORS_ENABLED;\n  public static final java.lang.String USER_COPROCESSORS_ENABLED_CONF_KEY;\n  public static final boolean DEFAULT_USER_COPROCESSORS_ENABLED;\n  public static final java.lang.String SKIP_LOAD_DUPLICATE_TABLE_COPROCESSOR;\n  public static final boolean DEFAULT_SKIP_LOAD_DUPLICATE_TABLE_COPROCESSOR;\n  protected org.apache.hadoop.hbase.Abortable abortable;\n  protected final org.apache.hadoop.hbase.util.SortedList<E> coprocEnvironments;\n  protected org.apache.hadoop.conf.Configuration conf;\n  protected java.lang.String pathPrefix;\n  protected java.util.concurrent.atomic.AtomicInteger loadSequence;\n  public org.apache.hadoop.hbase.coprocessor.CoprocessorHost(org.apache.hadoop.hbase.Abortable);\n  public static java.util.Set<java.lang.String> getLoadedCoprocessors();\n  public java.util.Set<java.lang.String> getCoprocessors();\n  protected void loadSystemCoprocessors(org.apache.hadoop.conf.Configuration, java.lang.String);\n  public E load(org.apache.hadoop.fs.Path, java.lang.String, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public E load(org.apache.hadoop.fs.Path, java.lang.String, int, org.apache.hadoop.conf.Configuration, java.lang.String[]) throws java.io.IOException;\n  public void load(java.lang.Class<? extends C>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public E checkAndLoadInstance(java.lang.Class<?>, int, org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract E createEnvironment(C, int, int, org.apache.hadoop.conf.Configuration);\n  public abstract C checkAndGetInstance(java.lang.Class<?>) throws java.lang.InstantiationException, java.lang.IllegalAccessException;\n  public void shutdown(E);\n  public C findCoprocessor(java.lang.String);\n  public <T extends C> T findCoprocessor(java.lang.Class<T>);\n  public <T extends C> java.util.List<T> findCoprocessors(java.lang.Class<T>);\n  public E findCoprocessorEnvironment(java.lang.String);\n  protected void abortServer(E, java.lang.Throwable);\n  protected void abortServer(java.lang.String, java.lang.Throwable);\n  protected void handleCoprocessorThrowable(E, java.lang.Throwable) throws java.io.IOException;\n  protected <O, R> R execOperationWithResult(org.apache.hadoop.hbase.coprocessor.CoprocessorHost<C, E>.ObserverOperationWithResult<O, R>) throws java.io.IOException;\n  protected <O> boolean execOperation(org.apache.hadoop.hbase.coprocessor.CoprocessorHost<C, E>.ObserverOperation<O>) throws java.io.IOException;\n  protected <O> boolean execShutdown(org.apache.hadoop.hbase.coprocessor.CoprocessorHost<C, E>.ObserverOperation<O>) throws java.io.IOException;\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/CoprocessorService.class;;;public interface org.apache.hadoop.hbase.coprocessor.CoprocessorService {\n  public abstract org.apache.hbase.thirdparty.com.google.protobuf.Service getService();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/CoreCoprocessor.class;;;public interface org.apache.hadoop.hbase.coprocessor.CoreCoprocessor extends java.lang.annotation.Annotation {\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/EndpointObserver.class;;;public interface org.apache.hadoop.hbase.coprocessor.EndpointObserver {\n  public default org.apache.hbase.thirdparty.com.google.protobuf.Message preEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hbase.thirdparty.com.google.protobuf.Service, java.lang.String, org.apache.hbase.thirdparty.com.google.protobuf.Message) throws java.io.IOException;\n  public default void postEndpointInvocation(org.apache.hadoop.hbase.coprocessor.ObserverContext<org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment>, org.apache.hbase.thirdparty.com.google.protobuf.Service, java.lang.String, org.apache.hbase.thirdparty.com.google.protobuf.Message, org.apache.hbase.thirdparty.com.google.protobuf.Message$Builder) throws java.io.IOException;\n}\n;;;No.;;;N;;;No, it is not a task definition but an interface for defining endpoint observer methods in Apache HBase's coprocessor.;;;N
org/apache/hadoop/hbase/coprocessor/HasMasterServices.class;;;public interface org.apache.hadoop.hbase.coprocessor.HasMasterServices {\n  public abstract org.apache.hadoop.hbase.master.MasterServices getMasterServices();\n}\n;;;No.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/HasRegionServerServices.class;;;public interface org.apache.hadoop.hbase.coprocessor.HasRegionServerServices {\n  public abstract org.apache.hadoop.hbase.regionserver.RegionServerServices getRegionServerServices();\n}\n;;;Yes.;;;Y;;;;;;N
org/apache/hadoop/hbase/coprocessor/MasterCoprocessor.class;;;public interface org.apache.hadoop.hbase.coprocessor.MasterCoprocessor extends org.apache.hadoop.hbase.Coprocessor {\n  public default java.util.Optional<org.apache.hadoop.hbase.coprocessor.MasterObserver> getMasterObserver();\n}\n;;;No. It is an interface definition that extends another interface and includes a method for getting a MasterObserver. It is not a complete message definition that can be put on a message queue.;;;N;;;No.;;;N
org/apache/hadoop/hbase/coprocessor/MasterCoprocessorEnvironment.class;;;public interface org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment extends org.apache.hadoop.hbase.CoprocessorEnvironment<org.apache.hadoop.hbase.coprocessor.MasterCoprocessor> {\n  public abstract org.apache.hadoop.hbase.ServerName getServerName();\n  public abstract org.apache.hadoop.hbase.client.Connection getConnection();\n  public abstract org.apache.hadoop.hbase.client.Connection createConnection(org.apache.hadoop.conf.Configuration) throws java.io.IOException;\n  public abstract org.apache.hadoop.hbase.metrics.MetricRegistry getMetricRegistryForMaster();\n}\n;;;No.;;;N;;;No, it is not a task definition. It is a definition of an interface for a master coprocessor environment in HBase.;;;N
